{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Model Training and Selection",
   "id": "8a5b4c6cfddc2b5e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This notebook furthur processes the data to match specific requirements of difrent models.Various models are explored and evaluated and feature engineering is also carried out on the clean dataset",
   "id": "86abbd716044f23d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T19:26:08.555397Z",
     "start_time": "2024-07-29T19:25:59.502939Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Import the neccesary libraries\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gr\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tabulate import tabulate"
   ],
   "id": "a710faa13ad150b4",
   "execution_count": 1,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T19:26:08.666821Z",
     "start_time": "2024-07-29T19:26:08.574766Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pd.read_csv(\"processed.csv\", parse_dates=['date'], index_col='date')\n",
    "df =df.asfreq('D')\n",
    "df =df[[\"quantity\"]]\n",
    "df.head()"
   ],
   "id": "b3dc5cb79c2516b8",
   "execution_count": 2,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T05:43:09.733509Z",
     "start_time": "2024-07-18T05:43:09.422273Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Merged max :\",  df.index.max())\n",
    "print(\"Merged min :\",  df.index.min())"
   ],
   "id": "f5c7b7b0b83baead",
   "execution_count": 3,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model 1 : SARIMAX",
   "id": "6154434b5d4533c0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T10:41:12.404027Z",
     "start_time": "2024-07-29T10:39:47.771186Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error\n",
    "\n",
    "## Model 1 : SARIMAX\n",
    "# Split data into train and test sets\n",
    "train_size = int(len(df) * 0.8)\n",
    "train, test = df.iloc[:train_size], df.iloc[train_size:]\n",
    "\n",
    "# Define and fit the SARIMAX model\n",
    "model = SARIMAX(train['quantity'], order=(1, 1, 1), seasonal_order=(1, 1, 1, 12))\n",
    "fit_model = model.fit(disp=False)\n",
    "\n",
    "# Forecast\n",
    "n_forecast = len(test)\n",
    "forecast = fit_model.get_forecast(steps=n_forecast)\n",
    "forecast_index = test.index\n",
    "forecast_values = forecast.predicted_mean\n",
    "\n",
    "# Calculate MAE and MAPE\n",
    "mae = mean_absolute_error(test['quantity'], forecast_values)\n",
    "mape = mean_absolute_percentage_error(test['quantity'], forecast_values)\n",
    "\n",
    "# Print the results\n",
    "print(f'MAE: {mae}')\n",
    "print(f'MAPE: {mape}')\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(train.index, train['quantity'], label='Train')\n",
    "plt.plot(test.index, test['quantity'], label='Test', color='green')\n",
    "plt.plot(forecast_index, forecast_values, label='Forecast', color='red')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Quantity')\n",
    "plt.title('SARIMAX Forecast')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "cd0826af2ad30e4b",
   "execution_count": 3,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model 2 and 3 : ARIMA & SES",
   "id": "86772cdbac9b200e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T20:02:08.873295Z",
     "start_time": "2024-07-22T20:02:04.859031Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.holtwinters import SimpleExpSmoothing\n",
    "\n",
    "\n",
    "# ARIMA Model\n",
    "def fit_arima(train_data, order):\n",
    "    model = ARIMA(train_data, order=order)\n",
    "    fit_model = model.fit()\n",
    "    return fit_model\n",
    "\n",
    "# Exponential Smoothing (Simple Exponential Smoothing)\n",
    "def fit_exponential_smoothing(train_data):\n",
    "    model = SimpleExpSmoothing(train_data)\n",
    "    fit_model = model.fit()\n",
    "    return fit_model\n",
    "\n",
    "# Forecasting and Evaluation Function\n",
    "def forecast_and_evaluate(model, test_data):\n",
    "    forecast_values = model.forecast(len(test_data))\n",
    "    mae = mean_absolute_error(test_data, forecast_values)\n",
    "    mape = mean_absolute_percentage_error(test_data, forecast_values)\n",
    "    return forecast_values, mae, mape\n",
    "\n",
    "# Fit ARIMA model\n",
    "arima_model = fit_arima(train['quantity'], order=(1, 1, 1))\n",
    "\n",
    "# Fit Exponential Smoothing model\n",
    "exp_smoothing_model = fit_exponential_smoothing(train['quantity'])\n",
    "\n",
    "# Forecast and evaluate ARIMA\n",
    "arima_forecast, arima_mae, arima_mape = forecast_and_evaluate(arima_model, test['quantity'])\n",
    "\n",
    "# Forecast and evaluate Exponential Smoothing\n",
    "exp_smoothing_forecast, exp_smoothing_mae, exp_smoothing_mape = forecast_and_evaluate(exp_smoothing_model, test['quantity'])\n",
    "\n",
    "# Print results\n",
    "print(f'ARIMA MAE: {arima_mae}, ARIMA MAPE: {arima_mape}')\n",
    "print(f'Exponential Smoothing MAE: {exp_smoothing_mae}, Exponential Smoothing MAPE: {exp_smoothing_mape}')\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(train.index, train['quantity'], label='Train')\n",
    "plt.plot(test.index, test['quantity'], label='Test', color='green')\n",
    "\n",
    "# Plot ARIMA forecast\n",
    "plt.plot(test.index, arima_forecast, label='ARIMA Forecast', color='blue')\n",
    "\n",
    "# Plot Exponential Smoothing forecast\n",
    "plt.plot(test.index, exp_smoothing_forecast, label='Exponential Smoothing Forecast', color='red')\n",
    "\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Quantity')\n",
    "plt.title('ARIMA vs Exponential Smoothing Forecast Comparison')\n",
    "plt.legend()\n",
    "plt.show();"
   ],
   "id": "bc9c067b75d6a89",
   "execution_count": 5,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T07:59:09.930919Z",
     "start_time": "2024-07-17T07:39:38.760708Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "\n",
    "# Fine-tuning SARIMAX model\n",
    "def fine_tune_sarimax(train_data):\n",
    "    best_mae = float('inf')\n",
    "    best_mape = float('inf')\n",
    "    best_order = None\n",
    "    best_seasonal_order = None\n",
    "    \n",
    "    # Iterate over possible parameter combinations\n",
    "    for p in range(3):\n",
    "        for d in range(2):\n",
    "            for q in range(3):\n",
    "                for P in range(2):\n",
    "                    for D in range(2):\n",
    "                        for Q in range(2):\n",
    "                            seasonal_order = (P, D, Q, 12)\n",
    "                            try:\n",
    "                                model = SARIMAX(train_data, order=(p, d, q), seasonal_order=seasonal_order)\n",
    "                                fit_model = model.fit(disp=False)\n",
    "                                forecast_values = fit_model.forecast(len(test))\n",
    "                                mae = mean_absolute_error(test['quantity'], forecast_values)\n",
    "                                mape = mean_absolute_percentage_error(test['quantity'], forecast_values)\n",
    "                                \n",
    "                                # Update best parameters if current model is better\n",
    "                                if mae < best_mae:\n",
    "                                    best_mae = mae\n",
    "                                    best_mape = mape\n",
    "                                    best_order = (p, d, q)\n",
    "                                    best_seasonal_order = seasonal_order\n",
    "                            except:\n",
    "                                continue\n",
    "    return best_order, best_seasonal_order, best_mae, best_mape\n",
    "\n",
    "# Fine-tuning ARIMA model\n",
    "def fine_tune_arima(train_data):\n",
    "    best_mae = float('inf')\n",
    "    best_mape = float('inf')\n",
    "    best_order = None\n",
    "    \n",
    "    # Iterate over possible parameter combinations\n",
    "    for p in range(3):\n",
    "        for d in range(2):\n",
    "            for q in range(3):\n",
    "                try:\n",
    "                    model = ARIMA(train_data, order=(p, d, q))\n",
    "                    fit_model = model.fit()\n",
    "                    forecast_values = fit_model.forecast(len(test))\n",
    "                    mae = mean_absolute_error(test['quantity'], forecast_values)\n",
    "                    mape = mean_absolute_percentage_error(test['quantity'], forecast_values)\n",
    "                    \n",
    "                    # Update best parameters if current model is better\n",
    "                    if mae < best_mae:\n",
    "                        best_mae = mae\n",
    "                        best_mape = mape\n",
    "                        best_order = (p, d, q)\n",
    "                except:\n",
    "                    continue\n",
    "    return best_order, best_mae, best_mape\n",
    "\n",
    "# Fine-tuning Exponential Smoothing model\n",
    "def fine_tune_exponential_smoothing(train_data):\n",
    "    best_mae = float('inf')\n",
    "    best_mape = float('inf')\n",
    "    best_alpha = None\n",
    "    \n",
    "    # Iterate over possible smoothing levels\n",
    "    for alpha in np.arange(0.1, 1.1, 0.1):\n",
    "        try:\n",
    "            model = SimpleExpSmoothing(train_data)\n",
    "            fit_model = model.fit(smoothing_level=alpha)\n",
    "            forecast_values = fit_model.forecast(len(test))\n",
    "            mae = mean_absolute_error(test['quantity'], forecast_values)\n",
    "            mape = mean_absolute_percentage_error(test['quantity'], forecast_values)\n",
    "            \n",
    "            # Update best parameters if current model is better\n",
    "            if mae < best_mae:\n",
    "                best_mae = mae\n",
    "                best_mape = mape\n",
    "                best_alpha = alpha\n",
    "        except:\n",
    "            continue\n",
    "    return best_alpha, best_mae, best_mape\n",
    "\n",
    "# Perform fine-tuning and get results for SARIMAX\n",
    "sarimax_order, sarimax_seasonal_order, sarimax_mae, sarimax_mape = fine_tune_sarimax(train['quantity'])\n",
    "\n",
    "# Perform fine-tuning and get results for ARIMA\n",
    "arima_order, arima_mae, arima_mape = fine_tune_arima(train['quantity'])\n",
    "\n",
    "# Perform fine-tuning and get results for Exponential Smoothing\n",
    "exp_smoothing_alpha, exp_smoothing_mae, exp_smoothing_mape = fine_tune_exponential_smoothing(train['quantity'])\n",
    "\n",
    "# Print results\n",
    "results = [\n",
    "    ['SARIMAX', sarimax_order, sarimax_seasonal_order, sarimax_mae, sarimax_mape],\n",
    "    ['ARIMA', arima_order, None, arima_mae, arima_mape],\n",
    "    ['Exponential Smoothing', exp_smoothing_alpha, None, exp_smoothing_mae, exp_smoothing_mape]\n",
    "]\n",
    "\n",
    "headers = ['Model', 'Order', 'Seasonal Order/Smoothing Level', 'MAE', 'MAPE']\n",
    "print(tabulate(results, headers=headers))"
   ],
   "id": "cdbde95d9e561d07",
   "execution_count": 15,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T20:03:12.833609Z",
     "start_time": "2024-07-22T20:03:12.804043Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = df.copy()\n",
    "data[\"unique_id\"]=1.0\n",
    "data[\"ds\"] = data.index\n",
    "data.rename(columns={\"quantity\":\"y\"},inplace=True)\n",
    "data.head()"
   ],
   "id": "403a8066e4fb63cb",
   "execution_count": 6,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T20:03:15.779999Z",
     "start_time": "2024-07-22T20:03:15.768391Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Using Nxitla libraries\n",
    "#Data Split\n",
    "# Calculate the index for the split\n",
    "split_index = int(0.8 * len(data))\n",
    "\n",
    "# Split the data\n",
    "Y_train_df = data.iloc[:split_index]\n",
    "Y_test_df = data.iloc[split_index:]   # Test data for January 2012\n",
    "\n",
    "horizon = len(Y_test_df)"
   ],
   "id": "91f132dbe13f05f2",
   "execution_count": 7,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model 4 : Prophet\n",
   "id": "bca47927f73e317e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T19:58:10.224923Z",
     "start_time": "2024-07-22T19:58:10.224923Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import itertools\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "import pandas as pd\n",
    "from prophet import Prophet\n",
    "from prophet.diagnostics import cross_validation, performance_metrics\n",
    "from prophet.plot import plot_cross_validation_metric\n",
    "import holidays\n",
    "\n",
    "# Add is_public_holiday column\n",
    "holiday = holidays.CountryHoliday('UK')\n",
    "data['is_public_holiday'] = data['ds'].apply(\n",
    "    lambda date: 1 if date in holiday else 0\n",
    ")\n",
    "\n",
    "# Define hyperparameters to tune\n",
    "param_grid = {  \n",
    "    'changepoint_prior_scale': [0.01, 0.1, 0.5],\n",
    "    'seasonality_prior_scale': [0.1, 1.0, 10.0],\n",
    "    'holidays_prior_scale': [0.1, 1.0, 10.0],\n",
    "    'seasonality_mode': ['additive', 'multiplicative']\n",
    "}\n",
    "\n",
    "# Generate all combinations of parameters\n",
    "all_params = [dict(zip(param_grid.keys(), v)) for v in itertools.product(*param_grid.values())]\n",
    "\n",
    "best_params = None\n",
    "best_mape = float('inf')\n",
    "\n",
    "# Grid search to find the best hyperparameters\n",
    "for params in all_params:\n",
    "    model = Prophet(**params)\n",
    "    model.add_regressor('is_public_holiday')\n",
    "    model.fit(data)\n",
    "\n",
    "    # Cross-validate the model\n",
    "    df_cv = cross_validation(model, initial='547 days', period='180 days', horizon='30 days')\n",
    "    df_p = performance_metrics(df_cv)\n",
    "    \n",
    "    # Calculate MAPE\n",
    "    mape = mean_absolute_percentage_error(df_cv['y'], df_cv['yhat'])\n",
    "    \n",
    "    if mape < best_mape:\n",
    "        best_mape = mape\n",
    "        best_params = params\n",
    "\n",
    "# Output the best parameters and MAPE\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "print(f\"Best MAPE: {round(best_mape, 2)}\")\n",
    "\n",
    "# Train the final model with the best parameters\n",
    "model = Prophet(**best_params)\n",
    "model.add_regressor('is_public_holiday')\n",
    "model.fit(data)\n",
    "\n",
    "# Forecast future values\n",
    "future = model.make_future_dataframe(periods=365)\n",
    "future['is_public_holiday'] = future['ds'].apply(\n",
    "    lambda date: 1 if date in holiday else 0\n",
    ")\n",
    "\n",
    "forecast = model.predict(future)\n",
    "\n",
    "# Visualize Results\n",
    "model.plot(forecast)\n",
    "model.plot_components(forecast)\n",
    "\n",
    "# Evaluate Accuracy\n",
    "df_cv = cross_validation(model, initial='547 days', period='180 days', horizon='30 days')\n",
    "df_p = performance_metrics(df_cv)\n",
    "print(df_p.head().round(2))\n",
    "\n",
    "fig = plot_cross_validation_metric(df_cv, metric='rmse')\n",
    "\n",
    "# Calculate MAPE using yhat and y\n",
    "mape = mean_absolute_percentage_error(df_cv['y'], df_cv['yhat'])\n",
    "print(f\"MAPE: {round(mape, 2)}\")\n"
   ],
   "id": "38c089ae3703faca",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "increase forecast horizon to 60 days",
   "id": "ab7d68e6da81a64a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T20:09:55.078660Z",
     "start_time": "2024-07-22T20:03:56.670132Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import itertools\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "import pandas as pd\n",
    "from prophet import Prophet\n",
    "from prophet.diagnostics import cross_validation, performance_metrics\n",
    "from prophet.plot import plot_cross_validation_metric\n",
    "import holidays\n",
    "\n",
    "# Add is_public_holiday column\n",
    "holiday = holidays.CountryHoliday('UK')\n",
    "data['is_public_holiday'] = data['ds'].apply(\n",
    "    lambda date: 1 if date in holiday else 0\n",
    ")\n",
    "\n",
    "# Define hyperparameters to tune\n",
    "param_grid = {  \n",
    "    'changepoint_prior_scale': [0.01, 0.1, 0.5],\n",
    "    'seasonality_prior_scale': [0.1, 1.0, 10.0],\n",
    "    'holidays_prior_scale': [0.1, 1.0, 10.0],\n",
    "    'seasonality_mode': ['additive', 'multiplicative']\n",
    "}\n",
    "\n",
    "# Generate all combinations of parameters\n",
    "all_params = [dict(zip(param_grid.keys(), v)) for v in itertools.product(*param_grid.values())]\n",
    "\n",
    "best_params = None\n",
    "best_mape = float('inf')\n",
    "\n",
    "# Grid search to find the best hyperparameters\n",
    "for params in all_params:\n",
    "    model = Prophet(**params)\n",
    "    model.add_regressor('is_public_holiday')\n",
    "    model.fit(data)\n",
    "\n",
    "    # Cross-validate the model\n",
    "    df_cv = cross_validation(model, initial='547 days', period='180 days', horizon='30 days')\n",
    "    df_p = performance_metrics(df_cv)\n",
    "    \n",
    "    # Calculate MAPE\n",
    "    mape = mean_absolute_percentage_error(df_cv['y'], df_cv['yhat'])\n",
    "    \n",
    "    if mape < best_mape:\n",
    "        best_mape = mape\n",
    "        best_params = params\n",
    "\n",
    "# Output the best parameters and MAPE\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "print(f\"Best MAPE: {round(best_mape, 2)}\")\n",
    "\n",
    "# Train the final model with the best parameters\n",
    "model = Prophet(**best_params)\n",
    "model.add_regressor('is_public_holiday')\n",
    "model.fit(data)\n",
    "\n",
    "# Forecast future values\n",
    "future = model.make_future_dataframe(periods=365)\n",
    "future['is_public_holiday'] = future['ds'].apply(\n",
    "    lambda date: 1 if date in holiday else 0\n",
    ")\n",
    "\n",
    "forecast = model.predict(future)\n",
    "\n",
    "# Visualize Results\n",
    "model.plot(forecast)\n",
    "model.plot_components(forecast)\n",
    "\n",
    "# Evaluate Accuracy\n",
    "df_cv = cross_validation(model, initial='547 days', period='180 days', horizon='60 days')\n",
    "df_p = performance_metrics(df_cv)\n",
    "print(df_p.head().round(2))\n",
    "\n",
    "fig = plot_cross_validation_metric(df_cv, metric='rmse')\n",
    "\n",
    "# Calculate MAPE using yhat and y\n",
    "mape = mean_absolute_percentage_error(df_cv['y'], df_cv['yhat'])\n",
    "print(f\"MAPE: {round(mape, 2)}\")\n"
   ],
   "id": "79fa483487d1f5c7",
   "execution_count": 8,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Using the best parameters\n",
    "\n",
    "# Add is_public_holiday column\n",
    "holiday = holidays.CountryHoliday('UK')\n",
    "data['is_public_holiday'] = data['ds'].apply(\n",
    "    lambda date: 1 if date in holiday else 0\n",
    ")\n",
    "\n",
    "# 2. Create and Fit Prophet Model with Best Parameters\n",
    "best_params = {'changepoint_prior_scale': 0.01, 'seasonality_prior_scale': 10.0, 'holidays_prior_scale': 0.1, 'seasonality_mode': 'additive'}\n",
    "\n",
    "model = Prophet(**best_params)\n",
    "model.add_regressor('is_public_holiday')\n",
    "model.fit(data)\n",
    "\n",
    "# 3. Forecast Future Values\n",
    "future = model.make_future_dataframe(periods=365)\n",
    "future['is_public_holiday'] = future['ds'].apply(\n",
    "    lambda date: 1 if date in holiday else 0\n",
    ")\n",
    "\n",
    "forecast = model.predict(future)\n",
    "\n",
    "# 4. Visualize Results\n",
    "model.plot(forecast)\n",
    "model.plot_components(forecast)\n",
    "\n",
    "# 5. Evaluate Accuracy\n",
    "df_cv = cross_validation(model, initial='547 days', period='180 days', horizon='90 days')\n",
    "df_p = performance_metrics(df_cv)\n",
    "print(df_p.head().round(2))\n",
    "\n",
    "fig = plot_cross_validation_metric(df_cv, metric='rmse')\n",
    "\n",
    "# Calculate MAPE using yhat and y\n",
    "mape = mean_absolute_percentage_error(df_cv['y'], df_cv['yhat'])\n",
    "print(f\"MAPE: {round(mape, 2)}\")\n",
    "\n",
    "# 6. Plot Actual vs Predicted Values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(data['ds'], data['y'], label='Actual')\n",
    "plt.plot(forecast['ds'], forecast['yhat'], label='Predicted')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Quantity Winsorized')\n",
    "plt.legend()\n",
    "plt.title('Actual vs Predicted Values')\n",
    "plt.show()\n"
   ],
   "id": "3cdbf55735a21719",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from ray import tune\n",
    "from neuralforecast import NeuralForecast\n",
    "from neuralforecast.auto import AutoNHITS, AutoLSTM\n",
    "from neuralforecast.losses.pytorch import MQLoss\n",
    "from datasetsforecast.losses import mape\n",
    "from datasetsforecast.evaluation import accuracy\n",
    "from statsforecast import StatsForecast\n",
    "\n",
    "\n",
    "# Step 3: Define model configurations\n",
    "config_nhits = {\n",
    "    \"input_size\": tune.choice([48, 48*2, 48*3]),\n",
    "    \"start_padding_enabled\": True,\n",
    "    \"n_blocks\": 5 * [1],\n",
    "    \"mlp_units\": 5 * [[64, 64]],\n",
    "    \"n_pool_kernel_size\": tune.choice([5 * [1], 5 * [2], 5 * [4], [8, 4, 2, 1, 1]]),\n",
    "    \"n_freq_downsample\": tune.choice([[8, 4, 2, 1, 1], [1, 1, 1, 1, 1]]),\n",
    "    \"learning_rate\": tune.loguniform(1e-4, 1e-2),\n",
    "    \"scaler_type\": tune.choice([None]),\n",
    "    \"max_steps\": tune.choice([1000]),\n",
    "    \"batch_size\": tune.choice([1, 4, 10]),\n",
    "    \"windows_batch_size\": tune.choice([128, 256, 512]),\n",
    "    \"random_seed\": tune.randint(1, 20),\n",
    "}\n",
    "\n",
    "config_lstm = {\n",
    "    \"input_size\": tune.choice([48, 48 * 2, 48 * 3]),\n",
    "    \"encoder_hidden_size\": tune.choice([64, 128]),\n",
    "    \"encoder_n_layers\": tune.choice([2, 4]),\n",
    "    \"learning_rate\": tune.loguniform(1e-4, 1e-2),\n",
    "    \"scaler_type\": tune.choice(['robust']),\n",
    "    \"max_steps\": tune.choice([500, 1000]),\n",
    "    \"batch_size\": tune.choice([1, 4]),\n",
    "    \"random_seed\": tune.randint(1, 20),\n",
    "}\n",
    "\n",
    "# Step 4: Train models\n",
    "nf = NeuralForecast(\n",
    "    models=[\n",
    "        AutoNHITS(h=30, config=config_nhits, loss=MQLoss(), num_samples=5),\n",
    "        AutoLSTM(h=30, config=config_lstm, loss=MQLoss(), num_samples=2),\n",
    "    ],\n",
    "    freq='D'\n",
    ")\n",
    "\n",
    "nf.fit(df=data)\n",
    "\n",
    "# Step 5: Predict future sales\n",
    "fcst_df = nf.predict()\n",
    "fcst_df.columns = fcst_df.columns.str.replace('-median', '')\n",
    "print(fcst_df.head())\n",
    "\n",
    "# Step 6: Cross-validation for model evaluation\n",
    "cv_df = nf.cross_validation(data, n_windows=2)\n",
    "cv_df.columns = cv_df.columns.str.replace('-median', '')\n",
    "\n",
    "# Evaluate using MAPE\n",
    "evaluation_df = accuracy(cv_df, [mape], agg_by=['unique_id'])\n",
    "evaluation_df['best_model'] = evaluation_df.drop(columns=['metric', 'unique_id']).idxmin(axis=1)\n",
    "print(evaluation_df)\n",
    "\n",
    "# Select the best model based on MAPE\n",
    "best_model_df = evaluation_df.query('metric == \"mape\"')\n",
    "best_model = best_model_df.groupby('unique_id')['best_model'].first().reset_index()\n",
    "print(best_model)\n",
    "\n",
    "# Function to get best model forecast\n",
    "def get_best_model_forecast(forecasts_df, evaluation_df, metric):\n",
    "    df = forecasts_df.set_index('ds', append=True).stack().to_frame().reset_index(level=2) # Wide to long \n",
    "    df.columns = ['model', 'best_model_forecast'] \n",
    "    df = df.join(evaluation_df.query('metric == @metric').set_index('unique_id')[['best_model']])\n",
    "    df = df.query('model.str.replace(\"-lo-90|-hi-90\", \"\", regex=True) == best_model').copy()\n",
    "    df.loc[:, 'model'] = [model.replace(bm, 'best_model') for model, bm in zip(df['model'], df['best_model'])]\n",
    "    df = df.drop(columns='best_model').set_index('model', append=True).unstack()\n",
    "    df.columns = df.columns.droplevel()\n",
    "    df = df.reset_index(level=1)\n",
    "    return df\n",
    "\n",
    "# Get the best model forecasts\n",
    "prod_forecasts_df = get_best_model_forecast(fcst_df, evaluation_df, metric='mape')\n",
    "print(prod_forecasts_df)\n",
    "\n",
    "# Plot the results (optional)\n",
    "StatsForecast.plot(data, prod_forecasts_df, engine='matplotlib')"
   ],
   "id": "f70d0c16c70dbcd6",
   "execution_count": 22,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model 5 ,6 & 7 :  MLForecast",
   "id": "7b814269e77d586c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T20:15:46.224672Z",
     "start_time": "2024-07-22T20:13:28.744696Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from mlforecast import MLForecast\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to create date features\n",
    "def create_date_features(data):\n",
    "    data[\"month\"] = data.index.month\n",
    "    data[\"day_of_month\"] = data.index.day\n",
    "    data[\"is_month_start\"] = data.index.is_month_start.astype(int)\n",
    "    data[\"is_month_end\"] = data.index.is_month_end.astype(int)\n",
    "    data[\"day_of_year\"] = data.index.dayofyear\n",
    "    data[\"week_of_year\"] = data.index.isocalendar().week\n",
    "    data[\"day_of_week\"] = data.index.dayofweek + 1\n",
    "    data[\"year\"] = data.index.year\n",
    "    data[\"is_weekend\"] = (data.index.weekday >= 5).astype(int)\n",
    "    data['is_spring'] = data['month'].isin([3, 4, 5]).astype(int)\n",
    "    data['is_summer'] = data['month'].isin([6, 7, 8]).astype(int)\n",
    "    data['is_fall'] = data['month'].isin([9, 10, 11]).astype(int)\n",
    "    data['is_winter'] = data['month'].isin([12, 1, 2]).astype(int)\n",
    "    data['sin_day'] = np.sin(2 * np.pi * data.index.dayofweek / 7)\n",
    "    data['cos_day'] = np.cos(2 * np.pi * data.index.dayofweek / 7)\n",
    "    return data\n",
    "\n",
    "ml_data = data.copy()\n",
    "ml_data  = create_date_features(ml_data )\n",
    "\n",
    "\n",
    "# Define Features and Models\n",
    "models = {\n",
    "    'RandomForest': RandomForestRegressor(n_estimators=100),\n",
    "    'XGBoost': XGBRegressor(objective='reg:squarederror', n_estimators=100),\n",
    "    'LightGBM': lgb.LGBMRegressor(n_estimators=100)\n",
    "}\n",
    "\n",
    "# Specify the features to be used in the model\n",
    "date_features = ['month', 'day_of_month', 'is_month_start', 'is_month_end',\n",
    "                 'day_of_year', 'week_of_year', 'day_of_week', 'year',\n",
    "                 'is_weekend', 'is_spring', 'is_summer', 'is_fall', 'is_winter',\n",
    "                 'sin_day', 'cos_day']\n",
    "\n",
    "# Define the Forecasting Pipeline\n",
    "forecast = MLForecast(\n",
    "    models=models,\n",
    "    freq='D', \n",
    "    lags=[7,14,30],  # Using 1, 2, 3 days lagged features\n",
    ")\n",
    "\n",
    "# Train the Models\n",
    "forecast.fit(ml_data , id_col='unique_id', time_col='ds', target_col='y')\n",
    "\n",
    "# Evaluate the Models\n",
    "# We'll use the last 30 days as the test set for evaluation\n",
    "train_df = ml_data [:-30]\n",
    "test_df = ml_data [-30:]\n",
    "forecast.fit(train_df, id_col='unique_id', time_col='ds', target_col='y')\n",
    "\n",
    "# Generate predictions for the test set\n",
    "predictions = forecast.predict(30)\n",
    "\n",
    "# Calculate Mean Absolute Percentage Error (MAPE) for each model\n",
    "mape_scores = {\n",
    "    model_name: mean_absolute_percentage_error(test_df['y'], predictions[model_name])\n",
    "    for model_name in models.keys()\n",
    "}\n",
    "\n",
    "# Print MAPE scores\n",
    "for model_name, mape_score in mape_scores.items():\n",
    "    print(f\"{model_name} MAPE: {mape_score}\")\n",
    "\n",
    "# Select the Best Model\n",
    "best_model_name = min(mape_scores, key=mape_scores.get)\n",
    "print(f\"The best model is {best_model_name} with a MAPE of {mape_scores[best_model_name]}\")\n",
    "\n",
    "# Retrain the best model on the entire dataset\n",
    "forecast.models = {best_model_name: models[best_model_name]}\n",
    "forecast.fit(ml_data , id_col='unique_id', time_col='ds', target_col='y')\n",
    "\n",
    "# Predict the next 30 days\n",
    "future_predictions = forecast.predict(30)\n",
    "\n",
    "# Plot actual vs predicted values\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(test_df['ds'], test_df['y'], label='Actual', marker='o')\n",
    "for model_name in models.keys():\n",
    "    plt.plot(test_df['ds'], predictions[model_name], label=f'Predicted - {model_name}', marker='x')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Actual vs Predicted Values')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ],
   "id": "7d76b0ad6f4213fb",
   "execution_count": 9,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## NeuralForecast",
   "id": "f282dfe670c9faf0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T20:18:28.884821Z",
     "start_time": "2024-07-22T20:15:46.236186Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from neuralforecast import NeuralForecast\n",
    "from neuralforecast.models import NHITS, NBEATS, LSTM\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have defined train_df, test_df, and ml_data somewhere in your code\n",
    "\n",
    "# Define the models and their respective parameters\n",
    "nhits_params = {\n",
    "    'h': 30,\n",
    "    'input_size': 30,\n",
    "    'max_steps': 50\n",
    "}\n",
    "\n",
    "nbeats_params = {\n",
    "    'h': 30,\n",
    "    'input_size': 30,\n",
    "    'max_steps': 50\n",
    "}\n",
    "\n",
    "lstm_params = {\n",
    "    'h': 30,\n",
    "    'input_size': 30,\n",
    "    'max_steps': 50\n",
    "}\n",
    "\n",
    "# Initialize the models\n",
    "nhits_model = NHITS(**nhits_params)\n",
    "nbeats_model = NBEATS(**nbeats_params)\n",
    "lstm_model = LSTM(**lstm_params)\n",
    "\n",
    "# Train each model on the training data and evaluate on test data\n",
    "models = [nhits_model, nbeats_model, lstm_model]\n",
    "model_names = ['NHITS', 'NBEATS', 'LSTM']\n",
    "mape_scores = []\n",
    "\n",
    "for model, name in zip(models, model_names):\n",
    "    nf = NeuralForecast(models=[model], freq='D')\n",
    "    nf.fit(train_df)\n",
    "    forecasts = nf.predict(futr_df=test_df)\n",
    "    print(forecasts.head())\n",
    "    mape = mean_absolute_percentage_error(test_df['y'], forecasts[name])\n",
    "    mape_scores.append((name, mape))\n",
    "\n",
    "# Select the best model based on MAPE\n",
    "best_model_name, best_mape = min(mape_scores, key=lambda x: x[1])\n",
    "best_model = None\n",
    "\n",
    "if best_model_name == 'NHITS':\n",
    "    best_model = NHITS(**nhits_params)\n",
    "elif best_model_name == 'NBEATS':\n",
    "    best_model = NBEATS(**nbeats_params)\n",
    "elif best_model_name == 'LSTM':\n",
    "    best_model = LSTM(**lstm_params)\n",
    "\n",
    "# Train the best model on the entire dataset\n",
    "nf_best_model = NeuralForecast(models=[best_model], freq='D')\n",
    "nf_best_model.fit(ml_data)\n",
    "\n",
    "# Predict the next 30 days with the best model\n",
    "final_forecasts = nf_best_model.predict()\n",
    "\n",
    "# Plot actual vs predicted values for the best model\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Plot the test data\n",
    "plt.plot(test_df['ds'], test_df['y'], label='Actual', marker='o')\n",
    "\n",
    "# Plot the forecasts\n",
    "forecast_dates = pd.date_range(test_df['ds'].max() + pd.to_timedelta('1 days'), periods=30, freq='D')\n",
    "plt.plot(\n",
    "    forecast_dates,\n",
    "    final_forecasts[best_model_name][:30],  # Ensure we take only 30 predictions\n",
    "    label=f'Predicted - {best_model_name}',\n",
    "    marker='x',\n",
    ")\n",
    "\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.title(f'Actual vs Predicted Values using {best_model_name}')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"The best model is {best_model_name} with a MAPE of {best_mape}\")\n"
   ],
   "id": "1a2288ab91ee438c",
   "execution_count": 10,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T20:19:40.748443Z",
     "start_time": "2024-07-22T20:18:28.884821Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from neuralforecast import NeuralForecast\n",
    "from neuralforecast.models import NHITS, NBEATS, LSTM\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have defined train_df, test_df, and ml_data somewhere in your code\n",
    "\n",
    "# Define the models and their respective parameters\n",
    "nhits_params = {\n",
    "    'h': 30,\n",
    "    'input_size': 30,\n",
    "    'max_steps': 50\n",
    "}\n",
    "\n",
    "nbeats_params = {\n",
    "    'h': 30,\n",
    "    'input_size': 30,\n",
    "    'max_steps': 50\n",
    "}\n",
    "\n",
    "lstm_params = {\n",
    "    'h': 30,\n",
    "    'input_size': 30,\n",
    "    'max_steps': 50\n",
    "}\n",
    "\n",
    "# Initialize the models\n",
    "nhits_model = NHITS(**nhits_params)\n",
    "nbeats_model = NBEATS(**nbeats_params)\n",
    "lstm_model = LSTM(**lstm_params)\n",
    "\n",
    "# Train each model on the training data and evaluate on test data\n",
    "models = [nhits_model, nbeats_model, lstm_model]\n",
    "model_names = ['NHITS', 'NBEATS', 'LSTM']\n",
    "mape_scores = []\n",
    "\n",
    "for model, name in zip(models, model_names):\n",
    "    nf = NeuralForecast(models=[model], freq='D')\n",
    "    nf.fit(train_df)\n",
    "    forecasts = nf.predict(futr_df=test_df)\n",
    "    print(forecasts.head())\n",
    "    mape = mean_absolute_percentage_error(test_df['y'], forecasts[name])\n",
    "    mape_scores.append((name, mape))\n",
    "\n",
    "# Select the best model based on MAPE\n",
    "best_model_name, best_mape = min(mape_scores, key=lambda x: x[1])\n",
    "best_model = None\n",
    "\n",
    "if best_model_name == 'NHITS':\n",
    "    best_model = NHITS(**nhits_params)\n",
    "elif best_model_name == 'NBEATS':\n",
    "    best_model = NBEATS(**nbeats_params)\n",
    "elif best_model_name == 'LSTM':\n",
    "    best_model = LSTM(**lstm_params)\n",
    "\n",
    "# Train the best model on the entire dataset\n",
    "nf_best_model = NeuralForecast(models=[best_model], freq='D')\n",
    "nf_best_model.fit(train_df)\n",
    "\n",
    "# Predict the entire historical period\n",
    "historical_forecasts = nf_best_model.predict(futr_df=test_df)\n",
    "historical_forecasts.index = test_df['ds']  # Set the index to the dates for plotting\n",
    "\n",
    "print(f\"The best model is {best_model_name} with a MAPE of {best_mape}\")\n",
    "\n",
    "# Plot actual vs predicted values for the historical period\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Plot the entire dataset\n",
    "plt.plot(ml_data['ds'], ml_data['y'], label='Actual', marker='o')\n",
    "\n",
    "# Plot the historical forecasts\n",
    "plt.plot(\n",
    "    historical_forecasts.index,\n",
    "    historical_forecasts[best_model_name],  # Historical forecasts\n",
    "    label=f'Predicted - {best_model_name}',\n",
    "    marker='x',\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.title(f'Actual vs Predicted Values using {best_model_name} for Historical Period')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"The best model is {best_model_name} with a MAPE of {best_mape}\")\n"
   ],
   "id": "deba0b4ad570df8e",
   "execution_count": 11,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Increase forecast horizon to 60",
   "id": "5759ea6f60b7ff75"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T20:23:12.963840Z",
     "start_time": "2024-07-22T20:19:40.749953Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from neuralforecast import NeuralForecast\n",
    "from neuralforecast.models import NHITS, NBEATS, LSTM\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have defined train_df, test_df, and ml_data somewhere in your code\n",
    "\n",
    "# Define the models and their respective parameters\n",
    "nhits_params = {\n",
    "    'h': 60,\n",
    "    'input_size': 30,\n",
    "    'max_steps': 50\n",
    "}\n",
    "\n",
    "nbeats_params = {\n",
    "    'h': 60,\n",
    "    'input_size': 30,\n",
    "    'max_steps': 50\n",
    "}\n",
    "\n",
    "lstm_params = {\n",
    "    'h': 60,\n",
    "    'input_size': 30,\n",
    "    'max_steps': 50\n",
    "}\n",
    "\n",
    "# Initialize the models\n",
    "nhits_model = NHITS(**nhits_params)\n",
    "nbeats_model = NBEATS(**nbeats_params)\n",
    "lstm_model = LSTM(**lstm_params)\n",
    "\n",
    "# Train each model on the training data and evaluate on test data\n",
    "models = [nhits_model, nbeats_model, lstm_model]\n",
    "model_names = ['NHITS', 'NBEATS', 'LSTM']\n",
    "mape_scores = []\n",
    "\n",
    "for model, name in zip(models, model_names):\n",
    "    nf = NeuralForecast(models=[model], freq='D')\n",
    "    nf.fit(train_df)\n",
    "    \n",
    "    # Create future dataframe for the test set to match expected combinations\n",
    "    future_test_df = nf.make_future_dataframe()\n",
    "    \n",
    "    # Make predictions for the test set\n",
    "    forecasts = nf.predict()\n",
    "    print(forecasts.head())\n",
    "    \n",
    "    # Align forecast with test_df to calculate MAPE\n",
    "    aligned_forecasts = forecasts[name].iloc[:len(test_df)]\n",
    "    \n",
    "    mape = mean_absolute_percentage_error(test_df['y'], aligned_forecasts)\n",
    "    mape_scores.append((name, mape))\n",
    "\n",
    "# Select the best model based on MAPE\n",
    "best_model_name, best_mape = min(mape_scores, key=lambda x: x[1])\n",
    "best_model = None\n",
    "\n",
    "if best_model_name == 'NHITS':\n",
    "    best_model = NHITS(**nhits_params)\n",
    "elif best_model_name == 'NBEATS':\n",
    "    best_model = NBEATS(**nbeats_params)\n",
    "elif best_model_name == 'LSTM':\n",
    "    best_model = LSTM(**lstm_params)\n",
    "  \n",
    "    \n",
    "print(f\"The best model is {best_model_name} with a MAPE of {best_mape}\")    \n",
    "\n",
    "# Train the best model on the entire dataset\n",
    "nf_best_model = NeuralForecast(models=[best_model], freq='D')\n",
    "nf_best_model.fit(ml_data)\n",
    "\n",
    "# Create a future dataframe for the next 60 days\n",
    "future_df = nf_best_model.make_future_dataframe()\n",
    "\n",
    "# Predict the next 60 days\n",
    "future_forecasts = nf_best_model.predict()\n",
    "\n",
    "# Combine the historical data with the forecast data for plotting\n",
    "combined_df = pd.concat([ml_data, future_df], ignore_index=True)\n",
    "combined_df['forecast'] = np.nan\n",
    "combined_df.loc[ml_data.shape[0]:, 'forecast'] = future_forecasts[best_model_name].values\n",
    "\n",
    "# Plot actual vs predicted values for the historical period and future forecast\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Plot the entire dataset\n",
    "plt.plot(combined_df['ds'], combined_df['y'], label='Actual', marker='o')\n",
    "\n",
    "# Plot the future forecasts\n",
    "plt.plot(\n",
    "    combined_df['ds'],\n",
    "    combined_df['forecast'],\n",
    "    label=f'Predicted - {best_model_name} (Next 60 Days)',\n",
    "    marker='x',\n",
    ")\n",
    "\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.title(f'Actual vs Predicted Values using {best_model_name}')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"The best model is {best_model_name} with a MAPE of {best_mape}\")\n"
   ],
   "id": "6013864d528234dc",
   "execution_count": 12,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T20:30:12.254953Z",
     "start_time": "2024-07-22T20:23:12.963840Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from neuralforecast import NeuralForecast\n",
    "from neuralforecast.models import NHITS, NBEATS, LSTM\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have defined train_df, test_df, and ml_data somewhere in your code\n",
    "\n",
    "# Define the parameter grids for manual fine-tuning\n",
    "nhits_param_grid = [\n",
    "    {'h': 60, 'input_size': 30, 'max_steps': 50, 'learning_rate': 0.001},\n",
    "    {'h': 60, 'input_size': 60, 'max_steps': 100, 'learning_rate': 0.01},\n",
    "    {'h': 60, 'input_size': 90, 'max_steps': 150, 'learning_rate': 0.1},\n",
    "]\n",
    "\n",
    "nbeats_param_grid = [\n",
    "    {'h': 60, 'input_size': 30, 'max_steps': 50, 'learning_rate': 0.001},\n",
    "    {'h': 60, 'input_size': 60, 'max_steps': 100, 'learning_rate': 0.01},\n",
    "    {'h': 60, 'input_size': 90, 'max_steps': 150, 'learning_rate': 0.1},\n",
    "]\n",
    "\n",
    "lstm_param_grid = [\n",
    "    {'h': 60, 'input_size': 30, 'max_steps': 50, 'learning_rate': 0.001, 'encoder_n_layers': 2, 'encoder_hidden_size': 200},\n",
    "    {'h': 60, 'input_size': 60, 'max_steps': 100, 'learning_rate': 0.01, 'encoder_n_layers': 2, 'encoder_hidden_size': 200},\n",
    "    {'h': 60, 'input_size': 90, 'max_steps': 150, 'learning_rate': 0.1, 'encoder_n_layers': 3, 'encoder_hidden_size': 300},\n",
    "]\n",
    "\n",
    "# Initialize the models\n",
    "# Initialize the models\n",
    "nhits_model = NHITS(h=60,input_size=30)\n",
    "nbeats_model = NBEATS(h=60,input_size=30)\n",
    "lstm_model = LSTM(h=60,input_size=30)\n",
    "\n",
    "# Train and evaluate each model with different hyperparameters\n",
    "models = [nhits_model, nbeats_model, lstm_model]\n",
    "param_grids = [nhits_param_grid, nbeats_param_grid, lstm_param_grid]\n",
    "model_names = ['NHITS', 'NBEATS', 'LSTM']\n",
    "mape_scores = []\n",
    "\n",
    "for model, param_grid, name in zip(models, param_grids, model_names):\n",
    "    for params in param_grid:\n",
    "        if name == 'NHITS':\n",
    "            model = NHITS(**params)\n",
    "        elif name == 'NBEATS':\n",
    "            model = NBEATS(**params)\n",
    "        elif name == 'LSTM':\n",
    "            model = LSTM(**params)\n",
    "            \n",
    "        nf = NeuralForecast(models=[model], freq='D')\n",
    "        nf.fit(train_df)\n",
    "        \n",
    "        # Make predictions for the test set\n",
    "        forecasts = nf.predict()\n",
    "        print(forecasts.head())\n",
    "        \n",
    "        # Align forecast with test_df to calculate MAPE\n",
    "        aligned_forecasts = forecasts[name].iloc[:len(test_df)]\n",
    "        \n",
    "        mape = mean_absolute_percentage_error(test_df['y'], aligned_forecasts)\n",
    "        mape_scores.append((name, mape, params))\n",
    "\n",
    "# Select the best model based on MAPE\n",
    "best_model_name, best_mape, best_params = min(mape_scores, key=lambda x: x[1])\n",
    "\n",
    "if best_model_name == 'NHITS':\n",
    "    best_model = NHITS(**best_params)\n",
    "elif best_model_name == 'NBEATS':\n",
    "    best_model = NBEATS(**best_params)\n",
    "elif best_model_name == 'LSTM':\n",
    "    best_model = LSTM(**best_params)\n",
    "    \n",
    "print(f\"The best model is {best_model_name} with a MAPE of {best_mape} and best parameters: {best_params}\")\n",
    "\n",
    "# Train the best model on the entire dataset\n",
    "nf_best_model = NeuralForecast(models=[best_model], freq='D')\n",
    "nf_best_model.fit(ml_data)\n",
    "\n",
    "# Create a future dataframe for the next 60 days\n",
    "future_df = nf_best_model.make_future_dataframe()\n",
    "\n",
    "# Predict the next 60 days\n",
    "future_forecasts = nf_best_model.predict()\n",
    "\n",
    "# Combine the historical data with the forecast data for plotting\n",
    "combined_df = pd.concat([ml_data, future_df], ignore_index=True)\n",
    "combined_df['forecast'] = np.nan\n",
    "combined_df.loc[ml_data.shape[0]:, 'forecast'] = future_forecasts[best_model_name].values\n",
    "\n",
    "# Plot actual vs predicted values for the historical period and future forecast\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Plot the entire dataset\n",
    "plt.plot(combined_df['ds'], combined_df['y'], label='Actual', marker='o')\n",
    "\n",
    "# Plot the future forecasts\n",
    "plt.plot(\n",
    "    combined_df['ds'],\n",
    "    combined_df['forecast'],\n",
    "    label=f'Predicted - {best_model_name} (Next 60 Days)',\n",
    "    marker='x',\n",
    ")\n",
    "\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.title(f'Actual vs Predicted Values using {best_model_name}')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"The best model is {best_model_name} with a MAPE of {best_mape} and best parameters: {best_params}\")\n"
   ],
   "id": "d5b23d1e97460820",
   "execution_count": 13,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## StatsForecast",
   "id": "5fb9c1c89a95a3d0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T20:33:34.598940Z",
     "start_time": "2024-07-22T20:30:12.258195Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsforecast.models import AutoARIMA, AutoETS, AutoCES, AutoTheta, SimpleExponentialSmoothingOptimized\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have defined train_df, test_df, and ml_data somewhere in your code\n",
    "\n",
    "# Extract the 'y' series from train and test dataframes\n",
    "train_series = train_df['y'].values\n",
    "test_series = test_df['y'].values\n",
    "\n",
    "# Define the models\n",
    "models = {\n",
    "    'AutoARIMA': AutoARIMA(season_length=12),\n",
    "    'AutoETS': AutoETS(model='ZZZ', season_length=12),\n",
    "    'AutoCES': AutoCES(model='Z', season_length=12),\n",
    "    'AutoTheta': AutoTheta(season_length=12),\n",
    "    'SESOpt': SimpleExponentialSmoothingOptimized()\n",
    "}\n",
    "\n",
    "# Initialize lists to store results\n",
    "mape_scores = []\n",
    "forecasts_dict = {}\n",
    "\n",
    "# Train and evaluate each model\n",
    "for name, model in models.items():\n",
    "    model = model.fit(y=train_series)\n",
    "    forecasts = model.predict(h=len(test_series))\n",
    "    forecasts_dict[name] = forecasts['mean']\n",
    "    \n",
    "    # Calculate MAPE\n",
    "    mape = mean_absolute_percentage_error(test_series, forecasts['mean'])\n",
    "    mape_scores.append((name, mape))\n",
    "\n",
    "# Select the best model based on MAPE\n",
    "best_model_name, best_mape = min(mape_scores, key=lambda x: x[1])\n",
    "best_model = models[best_model_name]\n",
    "\n",
    "print(f\"The best model is {best_model_name} with a MAPE of {best_mape}\")\n",
    "\n",
    "# Train the best model on the entire dataset\n",
    "best_model = best_model.fit(y=ml_data['y'].values)\n",
    "\n",
    "# Predict the next 60 days\n",
    "future_forecasts = best_model.predict(h=60)\n",
    "\n",
    "# Combine the historical data with the forecast data for plotting\n",
    "combined_df = ml_data.copy()\n",
    "future_df = pd.DataFrame({\n",
    "    'ds': pd.date_range(start=ml_data['ds'].max() + pd.Timedelta(days=1), periods=60, freq='D'),\n",
    "    'y': np.nan\n",
    "})\n",
    "combined_df = pd.concat([combined_df, future_df], ignore_index=True)\n",
    "combined_df['forecast'] = np.nan\n",
    "combined_df.loc[ml_data.shape[0]:, 'forecast'] = future_forecasts['mean']\n",
    "\n",
    "# Plot actual vs predicted values for the historical period and future forecast\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Plot the entire dataset\n",
    "plt.plot(combined_df['ds'], combined_df['y'], label='Actual', marker='o')\n",
    "\n",
    "# Plot the future forecasts\n",
    "plt.plot(\n",
    "    combined_df['ds'],\n",
    "    combined_df['forecast'],\n",
    "    label=f'Predicted - {best_model_name} (Next 60 Days)',\n",
    "    marker='x',\n",
    ")\n",
    "\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.title(f'Actual vs Predicted Values using {best_model_name}')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"The best model is {best_model_name} with a MAPE of {best_mape}\")\n"
   ],
   "id": "e6a5b9e6e95c520a",
   "execution_count": 14,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T20:33:48.766854Z",
     "start_time": "2024-07-22T20:33:34.598940Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsforecast.models import AutoARIMA, AutoETS, AutoCES, AutoTheta, SimpleExponentialSmoothingOptimized\n",
    "from sklearn.metrics import mean_absolute_percentage_error, mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have defined train_df, test_df, and ml_data somewhere in your code\n",
    "\n",
    "# Extract the 'y' series from train and test dataframes\n",
    "train_series = train_df['y'].values\n",
    "test_series = test_df['y'].values\n",
    "\n",
    "# Define the models\n",
    "models = {\n",
    "    'AutoARIMA': AutoARIMA(season_length=12),\n",
    "    'AutoETS': AutoETS(model='ZZZ', season_length=12),\n",
    "    'AutoCES': AutoCES(model='Z', season_length=12),\n",
    "    'AutoTheta': AutoTheta(season_length=12),\n",
    "    'SESOpt': SimpleExponentialSmoothingOptimized()\n",
    "}\n",
    "\n",
    "# Initialize lists to store results\n",
    "mape_scores = []\n",
    "mae_scores = []\n",
    "forecasts_dict = {}\n",
    "\n",
    "# Train and evaluate each model\n",
    "for name, model in models.items():\n",
    "    model = model.fit(y=train_series)\n",
    "    forecasts = model.predict(h=len(test_series))\n",
    "    forecasts_dict[name] = forecasts['mean']\n",
    "    \n",
    "    # Calculate MAPE and MAE\n",
    "    mape = mean_absolute_percentage_error(test_series, forecasts['mean'])\n",
    "    mae = mean_absolute_error(test_series, forecasts['mean'])\n",
    "    \n",
    "    mape_scores.append((name, mape))\n",
    "    mae_scores.append((name, mae))\n",
    "    \n",
    "    print(f\"{name} - MAE: {mae:.4f}, MAPE: {mape:.4%}\")\n",
    "\n",
    "# Select the best model based on MAPE\n",
    "best_model_name, best_mape = min(mape_scores, key=lambda x: x[1])\n",
    "best_model = models[best_model_name]\n",
    "\n",
    "print(f\"\\nThe best model is {best_model_name} with a MAPE of {best_mape:.4%}\")\n",
    "\n",
    "# Train the best model on the entire dataset\n",
    "best_model = best_model.fit(y=ml_data['y'].values)\n",
    "\n",
    "# Predict the next 60 days\n",
    "future_forecasts = best_model.predict(h=60)\n",
    "\n",
    "# Combine the historical data with the forecast data for plotting\n",
    "combined_df = ml_data.copy()\n",
    "future_df = pd.DataFrame({\n",
    "    'ds': pd.date_range(start=ml_data['ds'].max() + pd.Timedelta(days=1), periods=60, freq='D'),\n",
    "    'y': np.nan\n",
    "})\n",
    "combined_df = pd.concat([combined_df, future_df], ignore_index=True)\n",
    "combined_df['forecast'] = np.nan\n",
    "combined_df.loc[ml_data.shape[0]:, 'forecast'] = future_forecasts['mean']\n",
    "\n",
    "# Plot actual vs predicted values for the historical period and future forecast\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Plot the entire dataset\n",
    "plt.plot(combined_df['ds'], combined_df['y'], label='Actual', marker='o')\n",
    "\n",
    "# Plot the future forecasts\n",
    "plt.plot(\n",
    "    combined_df['ds'],\n",
    "    combined_df['forecast'],\n",
    "    label=f'Predicted - {best_model_name} (Next 60 Days)',\n",
    "    marker='x',\n",
    ")\n",
    "\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.title(f'Actual vs Predicted Values using {best_model_name}')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nThe best model is {best_model_name} with a MAPE of {best_mape:.4%}\")\n"
   ],
   "id": "59d819d8fe3cdb3e",
   "execution_count": 15,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T20:34:13.536634Z",
     "start_time": "2024-07-22T20:33:49.658514Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from mlforecast import MLForecast\n",
    "from mlforecast.lag_transforms import ExpandingMean, RollingMean\n",
    "from mlforecast.target_transforms import Differences\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have defined train_df, test_df, and ml_data somewhere in your code\n",
    "\n",
    "# Convert 'ds' column to datetime if necessary\n",
    "train_df['ds'] = pd.to_datetime(train_df['ds'])\n",
    "test_df['ds'] = pd.to_datetime(test_df['ds'])\n",
    "ml_data['ds'] = pd.to_datetime(ml_data['ds'])\n",
    "\n",
    "# Define the models\n",
    "models = {\n",
    "    'LightGBM': lgb.LGBMRegressor(verbosity=-1),\n",
    "    'XGBoost': xgb.XGBRegressor(verbosity=0),\n",
    "    'RandomForest': RandomForestRegressor(),\n",
    "    'LinearRegression': LinearRegression()\n",
    "}\n",
    "\n",
    "# Initialize lists to store results\n",
    "mape_scores = []\n",
    "mae_scores = []\n",
    "forecasts_dict = {}\n",
    "\n",
    "# Define MLForecast object\n",
    "def create_forecast_object(model):\n",
    "    return MLForecast(\n",
    "        models=[model],\n",
    "        freq='D',\n",
    "        lags=[7, 14],\n",
    "        lag_transforms={\n",
    "            1: [ExpandingMean()],\n",
    "            7: [RollingMean(window_size=28)]\n",
    "        },\n",
    "        date_features=['dayofweek'],\n",
    "        target_transforms=[Differences([1])]\n",
    "    )\n",
    "\n",
    "# Train and evaluate each model\n",
    "for name, model in models.items():\n",
    "    fcst = create_forecast_object(model)\n",
    "    fcst.fit(train_df)\n",
    "    \n",
    "    # Make predictions for the test set\n",
    "    predictions = fcst.predict(len(test_df))\n",
    "    forecasts_dict[name] = predictions\n",
    "    \n",
    "    # Align forecast with test_df to calculate MAE and MAPE\n",
    "    aligned_forecasts = predictions[name].values[:len(test_df)]\n",
    "    \n",
    "    mae = mean_absolute_error(test_df['y'], aligned_forecasts)\n",
    "    mape = mean_absolute_percentage_error(test_df['y'], aligned_forecasts)\n",
    "    \n",
    "    mae_scores.append((name, mae))\n",
    "    mape_scores.append((name, mape))\n",
    "    \n",
    "    print(f\"{name} - MAE: {mae:.4f}, MAPE: {mape:.4%}\")\n",
    "\n",
    "# Select the best model based on MAPE\n",
    "best_model_name, best_mape = min(mape_scores, key=lambda x: x[1])\n",
    "best_model = models[best_model_name]\n",
    "\n",
    "print(f\"\\nThe best model is {best_model_name} with a MAPE of {best_mape:.4%}\")\n",
    "\n",
    "# Train the best model on the entire dataset\n",
    "best_fcst = create_forecast_object(best_model)\n",
    "best_fcst.fit(ml_data)\n",
    "\n",
    "# Predict the next 60 days\n",
    "future_forecasts = best_fcst.predict(60)\n",
    "\n",
    "# Combine the historical data with the forecast data for plotting\n",
    "combined_df = ml_data.copy()\n",
    "future_df = pd.DataFrame({\n",
    "    'ds': pd.date_range(start=ml_data['ds'].max() + pd.Timedelta(days=1), periods=60, freq='D'),\n",
    "    'y': np.nan\n",
    "})\n",
    "combined_df = pd.concat([combined_df, future_df], ignore_index=True)\n",
    "combined_df['forecast'] = np.nan\n",
    "combined_df.loc[ml_data.shape[0]:, 'forecast'] = future_forecasts[best_model_name].values\n",
    "\n",
    "# Plot actual vs predicted values for the historical period and future forecast\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Plot the entire dataset\n",
    "plt.plot(combined_df['ds'], combined_df['y'], label='Actual', marker='o')\n",
    "\n",
    "# Plot the future forecasts\n",
    "plt.plot(\n",
    "    combined_df['ds'],\n",
    "    combined_df['forecast'],\n",
    "    label=f'Predicted - {best_model_name} (Next 60 Days)',\n",
    "    marker='x',\n",
    ")\n",
    "\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.title(f'Actual vs Predicted Values using {best_model_name}')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nThe best model is {best_model_name} with a MAPE of {best_mape:.4%}\")\n"
   ],
   "id": "3b9874b0998dd982",
   "execution_count": 16,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-20T19:04:33.433440Z",
     "start_time": "2024-07-20T19:02:09.478089Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install catboost",
   "id": "cf29835ada476e83",
   "execution_count": 37,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T20:36:41.508083Z",
     "start_time": "2024-07-22T20:36:25.446430Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from mlforecast import MLForecast\n",
    "from mlforecast.lag_transforms import ExpandingMean, RollingMean\n",
    "from mlforecast.target_transforms import Differences\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have defined train_df, test_df, and ml_data somewhere in your code\n",
    "\n",
    "# Convert 'ds' column to datetime if necessary\n",
    "train_df['ds'] = pd.to_datetime(train_df['ds'])\n",
    "test_df['ds'] = pd.to_datetime(test_df['ds'])\n",
    "ml_data['ds'] = pd.to_datetime(ml_data['ds'])\n",
    "\n",
    "# Define the models\n",
    "models = {\n",
    "    'LightGBM': lgb.LGBMRegressor(verbosity=-1),\n",
    "    'XGBoost': xgb.XGBRegressor(verbosity=0),\n",
    "    'RandomForest': RandomForestRegressor(),\n",
    "    'LinearRegression': LinearRegression()\n",
    "}\n",
    "\n",
    "# Initialize lists to store results\n",
    "mape_scores = []\n",
    "mae_scores = []\n",
    "forecasts_dict = {}\n",
    "\n",
    "# Define MLForecast object\n",
    "def create_forecast_object(model):\n",
    "    return MLForecast(\n",
    "        models=[model],\n",
    "        freq='D',\n",
    "        lags=[7, 14],\n",
    "        lag_transforms={\n",
    "            1: [ExpandingMean()],\n",
    "            7: [RollingMean(window_size=28)]\n",
    "        },\n",
    "        date_features=['dayofweek'],\n",
    "        target_transforms=[Differences([1])]\n",
    "    )\n",
    "\n",
    "# Train and evaluate each model\n",
    "for name, model in models.items():\n",
    "    fcst = create_forecast_object(model)\n",
    "    fcst.fit(train_df)\n",
    "    \n",
    "    # Make predictions for the test set\n",
    "    predictions = fcst.predict(len(test_df))\n",
    "    predictions.columns = ['unique_id', 'ds', name]\n",
    "    forecasts_dict[name] = predictions\n",
    "    \n",
    "    # Align forecast with test_df to calculate MAE and MAPE\n",
    "    aligned_forecasts = predictions[name].values[:len(test_df)]\n",
    "    \n",
    "    mae = mean_absolute_error(test_df['y'], aligned_forecasts)\n",
    "    mape = mean_absolute_percentage_error(test_df['y'], aligned_forecasts)\n",
    "    \n",
    "    mae_scores.append((name, mae))\n",
    "    mape_scores.append((name, mape))\n",
    "    \n",
    "    print(f\"{name} - MAE: {mae:.4f}, MAPE: {mape:.4%}\")\n",
    "\n",
    "# Select the best model based on MAPE\n",
    "best_model_name, best_mape = min(mape_scores, key=lambda x: x[1])\n",
    "best_model = models[best_model_name]\n",
    "\n",
    "print(f\"\\nThe best model is {best_model_name} with a MAPE of {best_mape:.4%}\")\n",
    "\n",
    "# Train the best model on the entire dataset\n",
    "best_fcst = create_forecast_object(best_model)\n",
    "best_fcst.fit(ml_data)\n",
    "\n",
    "# Predict the next 60 days\n",
    "future_forecasts = best_fcst.predict(60)\n",
    "future_forecasts.columns = ['unique_id', 'ds', best_model_name]\n",
    "\n",
    "# Combine the historical data with the forecast data for plotting\n",
    "combined_df = ml_data.copy()\n",
    "future_df = pd.DataFrame({\n",
    "    'ds': pd.date_range(start=ml_data['ds'].max() + pd.Timedelta(days=1), periods=60, freq='D'),\n",
    "    'y': np.nan\n",
    "})\n",
    "combined_df = pd.concat([combined_df, future_df], ignore_index=True)\n",
    "combined_df['forecast'] = np.nan\n",
    "combined_df.loc[ml_data.shape[0]:, 'forecast'] = future_forecasts[best_model_name].values\n",
    "\n",
    "# Plot actual vs predicted values for the historical period and future forecast\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Plot the entire dataset\n",
    "plt.plot(combined_df['ds'], combined_df['y'], label='Actual', marker='o')\n",
    "\n",
    "# Plot the future forecasts\n",
    "plt.plot(\n",
    "    combined_df['ds'],\n",
    "    combined_df['forecast'],\n",
    "    label=f'Predicted - {best_model_name} (Next 60 Days)',\n",
    "    marker='x',\n",
    ")\n",
    "\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.title(f'Actual vs Predicted Values using {best_model_name}')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nThe best model is {best_model_name} with a MAPE of {best_mape:.4%}\")\n"
   ],
   "id": "8cb2235a8d606ab0",
   "execution_count": 17,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Machiine Learning Methods",
   "id": "ac01db8001db7af5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-20T19:25:48.058386Z",
     "start_time": "2024-07-20T19:25:48.037705Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ml = ml_data.copy()\n",
    "t1 = train_df.copy()\n",
    "t2 = test_df.copy()"
   ],
   "id": "e140fec6ed3fa3bc",
   "execution_count": 41,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T20:37:34.428932Z",
     "start_time": "2024-07-22T20:37:12.631791Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from mlforecast import MLForecast\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to create date features\n",
    "def create_date_features(data):\n",
    "    data[\"month\"] = data.index.month\n",
    "    data[\"day_of_month\"] = data.index.day\n",
    "    data[\"is_month_start\"] = data.index.is_month_start.astype(int)\n",
    "    data[\"is_month_end\"] = data.index.is_month_end.astype(int)\n",
    "    data[\"day_of_year\"] = data.index.dayofyear\n",
    "    data[\"week_of_year\"] = data.index.isocalendar().week\n",
    "    data[\"day_of_week\"] = data.index.dayofweek + 1\n",
    "    data[\"year\"] = data.index.year\n",
    "    data[\"is_weekend\"] = (data.index.weekday >= 5).astype(int)\n",
    "    data['is_spring'] = data['month'].isin([3, 4, 5]).astype(int)\n",
    "    data['is_summer'] = data['month'].isin([6, 7, 8]).astype(int)\n",
    "    data['is_fall'] = data['month'].isin([9, 10, 11]).astype(int)\n",
    "    data['is_winter'] = data['month'].isin([12, 1, 2]).astype(int)\n",
    "    data['sin_day'] = np.sin(2 * np.pi * data.index.dayofweek / 7)\n",
    "    data['cos_day'] = np.cos(2 * np.pi * data.index.dayofweek / 7)\n",
    "    return data\n",
    "\n",
    "# Assuming 'data' is your initial DataFrame\n",
    "rml_data = data.copy()\n",
    "rml_data = create_date_features(rml_data)\n",
    "\n",
    "# Define Features and Models\n",
    "models = {\n",
    "    'RandomForest': RandomForestRegressor(n_estimators=100),\n",
    "    'XGBoost': XGBRegressor(objective='reg:squarederror', n_estimators=100),\n",
    "    'LightGBM': lgb.LGBMRegressor(n_estimators=100)\n",
    "}\n",
    "\n",
    "# Specify the features to be used in the model\n",
    "date_features = ['month', 'day_of_month', 'is_month_start', 'is_month_end',\n",
    "                 'day_of_year', 'week_of_year', 'day_of_week', 'year',\n",
    "                 'is_weekend', 'is_spring', 'is_summer', 'is_fall', 'is_winter',\n",
    "                 'sin_day', 'cos_day']\n",
    "\n",
    "# Define the Forecasting Pipeline\n",
    "forecast = MLForecast(\n",
    "    models=models,\n",
    "    freq='D',\n",
    "    lags=[7, 14, 30]  # Using 7, 14, 30 days lagged features\n",
    ")\n",
    "\n",
    "# Train the Models\n",
    "forecast.fit(rml_data, id_col='unique_id', time_col='ds', target_col='y')\n",
    "\n",
    "# Evaluate the Models\n",
    "# We'll use the last 30 days as the test set for evaluation\n",
    "train_df = rml_data[:-30]\n",
    "test_df = rml_data[-30:]\n",
    "forecast.fit(train_df, id_col='unique_id', time_col='ds', target_col='y')\n",
    "\n",
    "# Generate predictions for the test set\n",
    "predictions = forecast.predict(len(test_df))\n",
    "\n",
    "# Calculate Mean Absolute Percentage Error (MAPE) for each model\n",
    "mape_scores = {\n",
    "    model_name: mean_absolute_percentage_error(test_df['y'].values[:30], predictions[model_name].values[:30])\n",
    "    for model_name in models.keys()\n",
    "}\n",
    "\n",
    "# Print MAPE scores\n",
    "for model_name, mape_score in mape_scores.items():\n",
    "    print(f\"{model_name} MAPE: {mape_score}\")\n",
    "\n",
    "# Select the Best Model\n",
    "best_model_name = min(mape_scores, key=mape_scores.get)\n",
    "print(f\"The best model is {best_model_name} with a MAPE of {mape_scores[best_model_name]}\")\n",
    "\n",
    "# Retrain the best model on the entire dataset\n",
    "forecast.models = {best_model_name: models[best_model_name]}\n",
    "forecast.fit(rml_data, id_col='unique_id', time_col='ds', target_col='y')\n",
    "\n",
    "# Predict the next 30 days\n",
    "future_predictions = forecast.predict(30)\n",
    "\n",
    "# Plot actual vs predicted values\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(test_df['ds'], test_df['y'], label='Actual', marker='o')\n",
    "plt.plot(test_df['ds'], predictions[best_model_name], label=f'Predicted - {best_model_name}', marker='x')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Actual vs Predicted Values')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ],
   "id": "405401a8f8569ffa",
   "execution_count": 18,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T20:37:50.294649Z",
     "start_time": "2024-07-22T20:37:35.798062Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to create date features\n",
    "def create_date_features(data):\n",
    "    data[\"month\"] = data.index.month\n",
    "    data[\"day_of_month\"] = data.index.day\n",
    "    data[\"is_month_start\"] = data.index.is_month_start.astype(int)\n",
    "    data[\"is_month_end\"] = data.index.is_month_end.astype(int)\n",
    "    data[\"day_of_year\"] = data.index.dayofyear\n",
    "    data[\"week_of_year\"] = data.index.isocalendar().week\n",
    "    data[\"day_of_week\"] = data.index.dayofweek + 1\n",
    "    data[\"year\"] = data.index.year\n",
    "    data[\"is_weekend\"] = (data.index.weekday >= 5).astype(int)\n",
    "    data['is_spring'] = data['month'].isin([3, 4, 5]).astype(int)\n",
    "    data['is_summer'] = data['month'].isin([6, 7, 8]).astype(int)\n",
    "    data['is_fall'] = data['month'].isin([9, 10, 11]).astype(int)\n",
    "    data['is_winter'] = data['month'].isin([12, 1, 2]).astype(int)\n",
    "    data['sin_day'] = np.sin(2 * np.pi * data.index.dayofweek / 7)\n",
    "    data['cos_day'] = np.cos(2 * np.pi * data.index.dayofweek / 7)\n",
    "    return data\n",
    "\n",
    "\n",
    "# Create date features\n",
    "rml_data = create_date_features(data.copy())\n",
    "\n",
    "# Define Features and Models\n",
    "models = {\n",
    "    'RandomForest': RandomForestRegressor(n_estimators=100),\n",
    "    'XGBoost': XGBRegressor(objective='reg:squarederror', n_estimators=100),\n",
    "    'LightGBM': lgb.LGBMRegressor(n_estimators=100)\n",
    "}\n",
    "\n",
    "# Specify the features to be used in the model\n",
    "date_features = ['month', 'day_of_month', 'is_month_start', 'is_month_end',\n",
    "                 'day_of_year', 'week_of_year', 'day_of_week', 'year',\n",
    "                 'is_weekend', 'is_spring', 'is_summer', 'is_fall', 'is_winter',\n",
    "                 'sin_day', 'cos_day']\n",
    "\n",
    "# Prepare training data\n",
    "def create_lagged_features(data, lags):\n",
    "    for lag in lags:\n",
    "        data[f'lag_{lag}'] = data['y'].shift(lag)\n",
    "    return data\n",
    "\n",
    "lags = [7, 14, 30]\n",
    "rml_data = create_lagged_features(rml_data, lags).dropna()\n",
    "\n",
    "X = rml_data[date_features + [f'lag_{lag}' for lag in lags]]\n",
    "y = rml_data['y']\n",
    "\n",
    "# Split data into train and test sets\n",
    "train_X = X[:-30]\n",
    "train_y = y[:-30]\n",
    "test_X = X[-30:]\n",
    "test_y = y[-30:]\n",
    "\n",
    "# Train and predict using each model\n",
    "predictions = {}\n",
    "for model_name, model in models.items():\n",
    "    model.fit(train_X, train_y)\n",
    "    predictions[model_name] = model.predict(test_X)\n",
    "\n",
    "# Calculate Mean Absolute Percentage Error (MAPE) for each model\n",
    "mape_scores = {\n",
    "    model_name: mean_absolute_percentage_error(test_y, pred)\n",
    "    for model_name, pred in predictions.items()\n",
    "}\n",
    "\n",
    "# Print MAPE scores\n",
    "for model_name, mape_score in mape_scores.items():\n",
    "    print(f\"{model_name} MAPE: {mape_score}\")\n",
    "\n",
    "# Select the Best Model\n",
    "best_model_name = min(mape_scores, key=mape_scores.get)\n",
    "print(f\"The best model is {best_model_name} with a MAPE of {mape_scores[best_model_name]}\")\n",
    "\n",
    "# Retrain the best model on the entire dataset\n",
    "best_model = models[best_model_name]\n",
    "best_model.fit(X, y)\n",
    "\n",
    "# Predict the next 30 days\n",
    "future_X = X[-30:]  # Assuming that you want to predict the next 30 days using the same features\n",
    "future_predictions = best_model.predict(future_X)\n",
    "\n",
    "# Plot actual vs predicted values\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(test_y.index, test_y, label='Actual', marker='o')\n",
    "plt.plot(test_y.index, predictions[best_model_name], label=f'Predicted - {best_model_name}', marker='x')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Actual vs Predicted Values')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ],
   "id": "e257f5331a987aeb",
   "execution_count": 19,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T20:37:55.790704Z",
     "start_time": "2024-07-22T20:37:50.298191Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "import matplotlib.pyplot as plt\n",
    "import holidays\n",
    "\n",
    "# Function to create date features\n",
    "def create_date_features(data):\n",
    "    data[\"month\"] = data.index.month\n",
    "    data[\"day_of_month\"] = data.index.day\n",
    "    data[\"is_month_start\"] = data.index.is_month_start.astype(int)\n",
    "    data[\"is_month_end\"] = data.index.is_month_end.astype(int)\n",
    "    data[\"day_of_year\"] = data.index.dayofyear\n",
    "    data[\"week_of_year\"] = data.index.isocalendar().week\n",
    "    data[\"day_of_week\"] = data.index.dayofweek + 1\n",
    "    data[\"year\"] = data.index.year\n",
    "    data[\"is_weekend\"] = (data.index.weekday >= 5).astype(int)\n",
    "    data['is_spring'] = data['month'].isin([3, 4, 5]).astype(int)\n",
    "    data['is_summer'] = data['month'].isin([6, 7, 8]).astype(int)\n",
    "    data['is_fall'] = data['month'].isin([9, 10, 11]).astype(int)\n",
    "    data['is_winter'] = data['month'].isin([12, 1, 2]).astype(int)\n",
    "    data['sin_day'] = np.sin(2 * np.pi * data.index.dayofweek / 7)\n",
    "    data['cos_day'] = np.cos(2 * np.pi * data.index.dayofweek / 7)\n",
    "    return data\n",
    "\n",
    "\n",
    "# Add holiday feature\n",
    "uk_holidays = holidays.CountryHoliday('UK')\n",
    "\n",
    "\n",
    "# Create date features\n",
    "rml_data = create_date_features(data.copy())\n",
    "rml_data['is_public_holiday'] = rml_data.index.to_series().apply(lambda date: 1 if date in uk_holidays else 0)\n",
    "\n",
    "# Define Features and Models\n",
    "models = {\n",
    "    'RandomForest': RandomForestRegressor(n_estimators=100),\n",
    "    'XGBoost': XGBRegressor(objective='reg:squarederror', n_estimators=100),\n",
    "    'LightGBM': lgb.LGBMRegressor(n_estimators=100)\n",
    "}\n",
    "\n",
    "# Specify the features to be used in the model\n",
    "date_features = ['month', 'day_of_month', 'is_month_start', 'is_month_end',\n",
    "                 'day_of_year', 'week_of_year', 'day_of_week', 'year',\n",
    "                 'is_weekend', 'is_spring', 'is_summer', 'is_fall', 'is_winter',\n",
    "                 'sin_day', 'cos_day', 'is_public_holiday']\n",
    "\n",
    "# Prepare training data\n",
    "def create_lagged_features(data, lags):\n",
    "    for lag in lags:\n",
    "        data[f'lag_{lag}'] = data['y'].shift(lag)\n",
    "    return data\n",
    "\n",
    "lags = [7, 14, 30]\n",
    "rml_data = create_lagged_features(rml_data, lags).dropna()\n",
    "\n",
    "X = rml_data[date_features + [f'lag_{lag}' for lag in lags]]\n",
    "y = rml_data['y']\n",
    "\n",
    "# Split data into train and test sets\n",
    "train_X = X[:-30]\n",
    "train_y = y[:-30]\n",
    "test_X = X[-30:]\n",
    "test_y = y[-30:]\n",
    "\n",
    "# Train and predict using each model\n",
    "predictions = {}\n",
    "for model_name, model in models.items():\n",
    "    model.fit(train_X, train_y)\n",
    "    predictions[model_name] = model.predict(test_X)\n",
    "\n",
    "# Calculate Mean Absolute Percentage Error (MAPE) for each model\n",
    "mape_scores = {\n",
    "    model_name: mean_absolute_percentage_error(test_y, pred)\n",
    "    for model_name, pred in predictions.items()\n",
    "}\n",
    "\n",
    "# Print MAPE scores\n",
    "for model_name, mape_score in mape_scores.items():\n",
    "    print(f\"{model_name} MAPE: {mape_score}\")\n",
    "\n",
    "# Select the Best Model\n",
    "best_model_name = min(mape_scores, key=mape_scores.get)\n",
    "print(f\"The best model is {best_model_name} with a MAPE of {mape_scores[best_model_name]}\")\n",
    "\n",
    "# Retrain the best model on the entire dataset\n",
    "best_model = models[best_model_name]\n",
    "best_model.fit(X, y)\n",
    "\n",
    "# Predict the next 30 days\n",
    "future_X = X[-30:]  # Assuming that you want to predict the next 30 days using the same features\n",
    "future_predictions = best_model.predict(future_X)\n",
    "\n",
    "# Plot actual vs predicted values\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(test_y.index, test_y, label='Actual', marker='o')\n",
    "plt.plot(test_y.index, predictions[best_model_name], label=f'Predicted - {best_model_name}', marker='x')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Actual vs Predicted Values')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ],
   "id": "5c260ae345025a92",
   "execution_count": 20,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "",
   "id": "49af265b7ca9b4d",
   "outputs": []
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-07-22T20:37:55.793138Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "import matplotlib.pyplot as plt\n",
    "import holidays\n",
    "\n",
    "# Function to create date features\n",
    "def create_date_features(data):\n",
    "    data[\"month\"] = data.index.month\n",
    "    data[\"day_of_month\"] = data.index.day\n",
    "    data[\"is_month_start\"] = data.index.is_month_start.astype(int)\n",
    "    data[\"is_month_end\"] = data.index.is_month_end.astype(int)\n",
    "    data[\"day_of_year\"] = data.index.dayofyear\n",
    "    data[\"week_of_year\"] = data.index.isocalendar().week\n",
    "    data[\"day_of_week\"] = data.index.dayofweek + 1\n",
    "    data[\"year\"] = data.index.year\n",
    "    data[\"is_weekend\"] = (data.index.weekday >= 5).astype(int)\n",
    "    data['is_spring'] = data['month'].isin([3, 4, 5]).astype(int)\n",
    "    data['is_summer'] = data['month'].isin([6, 7, 8]).astype(int)\n",
    "    data['is_fall'] = data['month'].isin([9, 10, 11]).astype(int)\n",
    "    data['is_winter'] = data['month'].isin([12, 1, 2]).astype(int)\n",
    "    data['sin_day'] = np.sin(2 * np.pi * data.index.dayofweek / 7)\n",
    "    data['cos_day'] = np.cos(2 * np.pi * data.index.dayofweek / 7)\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "# Add holiday feature\n",
    "uk_holidays = holidays.CountryHoliday('UK')\n",
    "\n",
    "\n",
    "# Create date features\n",
    "rml_data = create_date_features(data.copy())\n",
    "rml_data['is_public_holiday'] = rml_data.index.to_series().apply(lambda date: 1 if date in uk_holidays else 0)\n",
    "\n",
    "# Define Features and Models\n",
    "date_features = ['month', 'day_of_month', 'is_month_start', 'is_month_end',\n",
    "                 'day_of_year', 'week_of_year', 'day_of_week', 'year',\n",
    "                 'is_weekend', 'is_spring', 'is_summer', 'is_fall', 'is_winter',\n",
    "                 'sin_day', 'cos_day', 'is_public_holiday']\n",
    "\n",
    "# Prepare training data\n",
    "def create_lagged_features(data, lags):\n",
    "    for lag in lags:\n",
    "        data[f'lag_{lag}'] = data['y'].shift(lag)\n",
    "    return data\n",
    "\n",
    "lags = [7, 14, 30]\n",
    "rml_data = create_lagged_features(rml_data, lags).dropna()\n",
    "\n",
    "X = rml_data[date_features + [f'lag_{lag}' for lag in lags]]\n",
    "y = rml_data['y']\n",
    "\n",
    "# Split data into train and test sets\n",
    "train_X = X[:-30]\n",
    "train_y = y[:-30]\n",
    "test_X = X[-30:]\n",
    "test_y = y[-30:]\n",
    "\n",
    "# Define parameter grids for each model\n",
    "param_grids = {\n",
    "    'RandomForest': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10]\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [3, 6, 9],\n",
    "        'learning_rate': [0.01, 0.1, 0.2]\n",
    "    },\n",
    "    'LightGBM': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'num_leaves': [31, 40, 50],\n",
    "        'learning_rate': [0.01, 0.1, 0.2]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Train and predict using each model with grid search\n",
    "predictions = {}\n",
    "best_params = {}\n",
    "\n",
    "for model_name, model in {'RandomForest': RandomForestRegressor(), \n",
    "                          'XGBoost': XGBRegressor(objective='reg:squarederror'), \n",
    "                          'LightGBM': lgb.LGBMRegressor()}.items():\n",
    "    grid_search = GridSearchCV(model, param_grids[model_name], cv=3, scoring='neg_mean_absolute_percentage_error')\n",
    "    grid_search.fit(train_X, train_y)\n",
    "    best_params[model_name] = grid_search.best_params_\n",
    "    predictions[model_name] = grid_search.predict(test_X)\n",
    "    print(f\"Best parameters for {model_name}: {best_params[model_name]}\")\n",
    "\n",
    "# Calculate Mean Absolute Percentage Error (MAPE) for each model\n",
    "mape_scores = {\n",
    "    model_name: mean_absolute_percentage_error(test_y, pred)\n",
    "    for model_name, pred in predictions.items()\n",
    "}\n",
    "\n",
    "# Print MAPE scores\n",
    "for model_name, mape_score in mape_scores.items():\n",
    "    print(f\"{model_name} MAPE: {mape_score}\")\n",
    "\n",
    "# Select the Best Model\n",
    "best_model_name = min(mape_scores, key=mape_scores.get)\n",
    "print(f\"The best model is {best_model_name} with a MAPE of {mape_scores[best_model_name]}\")\n",
    "\n",
    "# Retrain the best model on the entire dataset with best parameters\n",
    "best_model = {'RandomForest': RandomForestRegressor(**best_params['RandomForest']),\n",
    "              'XGBoost': XGBRegressor(objective='reg:squarederror', **best_params['XGBoost']),\n",
    "              'LightGBM': lgb.LGBMRegressor(**best_params['LightGBM'])}[best_model_name]\n",
    "best_model.fit(X, y)\n",
    "\n",
    "# Predict the next 30 days\n",
    "future_X = X[-30:]  # Assuming that you want to predict the next 30 days using the same features\n",
    "future_predictions = best_model.predict(future_X)\n",
    "\n",
    "# Plot actual vs predicted values\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(test_y.index, test_y, label='Actual', marker='o')\n",
    "plt.plot(test_y.index, predictions[best_model_name], label=f'Predicted - {best_model_name}', marker='x')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Actual vs Predicted Values')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ],
   "id": "fa01c27817f277f5",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error\n",
    "import matplotlib.pyplot as plt\n",
    "import holidays\n",
    "\n",
    "# Function to create date features\n",
    "def create_date_features(data):\n",
    "    data[\"month\"] = data.index.month\n",
    "    data[\"day_of_month\"] = data.index.day\n",
    "    data[\"is_month_start\"] = data.index.is_month_start.astype(int)\n",
    "    data[\"is_month_end\"] = data.index.is_month_end.astype(int)\n",
    "    data[\"day_of_year\"] = data.index.dayofyear\n",
    "    data[\"week_of_year\"] = data.index.isocalendar().week\n",
    "    data[\"day_of_week\"] = data.index.dayofweek + 1\n",
    "    data[\"year\"] = data.index.year\n",
    "    data[\"is_weekend\"] = (data.index.weekday >= 5).astype(int)\n",
    "    data['is_spring'] = data['month'].isin([3, 4, 5]).astype(int)\n",
    "    data['is_summer'] = data['month'].isin([6, 7, 8]).astype(int)\n",
    "    data['is_fall'] = data['month'].isin([9, 10, 11]).astype(int)\n",
    "    data['is_winter'] = data['month'].isin([12, 1, 2]).astype(int)\n",
    "    data['sin_day'] = np.sin(2 * np.pi * data.index.dayofweek / 7)\n",
    "    data['cos_day'] = np.cos(2 * np.pi * data.index.dayofweek / 7)\n",
    "    return data\n",
    "\n",
    "  \n",
    "# Add holiday feature\n",
    "uk_holidays = holidays.CountryHoliday('UK')\n",
    "\n",
    "# Create date features\n",
    "rml_data = create_date_features(data.copy())\n",
    "rml_data['is_public_holiday'] = rml_data.index.to_series().apply(lambda date: 1 if date in uk_holidays else 0)\n",
    "\n",
    "# Define Features and Models\n",
    "date_features = ['month', 'day_of_month', 'is_month_start', 'is_month_end',\n",
    "                 'day_of_year', 'week_of_year', 'day_of_week', 'year',\n",
    "                 'is_weekend', 'is_spring', 'is_summer', 'is_fall', 'is_winter',\n",
    "                 'sin_day', 'cos_day', 'is_public_holiday']\n",
    "\n",
    "# Prepare training data\n",
    "def create_lagged_features(data, lags):\n",
    "    for lag in lags:\n",
    "        data[f'lag_{lag}'] = data['y'].shift(lag)\n",
    "    return data\n",
    "\n",
    "lags = [7, 14, 30]\n",
    "rml_data = create_lagged_features(rml_data, lags).dropna()\n",
    "\n",
    "X = rml_data[date_features + [f'lag_{lag}' for lag in lags]]\n",
    "y = rml_data['y']\n",
    "\n",
    "# Split data into train and test sets\n",
    "train_X = X[:-30]\n",
    "train_y = y[:-30]\n",
    "test_X = X[-30:]\n",
    "test_y = y[-30:]\n",
    "\n",
    "# Define parameter grids for each model\n",
    "param_grids = {\n",
    "    'RandomForest': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10]\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [3, 6, 9],\n",
    "        'learning_rate': [0.01, 0.1, 0.2]\n",
    "    },\n",
    "    'LightGBM': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'num_leaves': [31, 40, 50],\n",
    "        'learning_rate': [0.01, 0.1, 0.2]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Train and predict using each model with grid search\n",
    "predictions = {}\n",
    "best_params = {}\n",
    "model_mae_mape = []\n",
    "\n",
    "for model_name, model in {'RandomForest': RandomForestRegressor(), \n",
    "                          'XGBoost': XGBRegressor(objective='reg:squarederror'), \n",
    "                          'LightGBM': lgb.LGBMRegressor()}.items():\n",
    "    grid_search = GridSearchCV(model, param_grids[model_name], cv=3, scoring='neg_mean_absolute_percentage_error')\n",
    "    grid_search.fit(train_X, train_y)\n",
    "    best_params[model_name] = grid_search.best_params_\n",
    "    predictions[model_name] = grid_search.predict(test_X)\n",
    "    print(f\"Best parameters for {model_name}: {best_params[model_name]}\")\n",
    "    \n",
    "    # Calculate MAE and MAPE\n",
    "    mae = mean_absolute_error(test_y, predictions[model_name])\n",
    "    mape = mean_absolute_percentage_error(test_y, predictions[model_name])\n",
    "    model_mae_mape.append({\n",
    "        'Model': model_name,\n",
    "        'MAE': mae,\n",
    "        'MAPE': mape\n",
    "    })\n",
    "\n",
    "# Convert the results to a DataFrame\n",
    "evaluation_df = pd.DataFrame(model_mae_mape)\n",
    "\n",
    "# Print the MAE and MAPE scores\n",
    "print(evaluation_df)\n",
    "\n",
    "# Select the Best Model\n",
    "best_model_name = min(mape_scores, key=mape_scores.get)\n",
    "print(f\"The best model is {best_model_name} with a MAPE of {mape_scores[best_model_name]}\")\n",
    "\n",
    "# Retrain the best model on the entire dataset with best parameters\n",
    "best_model = {'RandomForest': RandomForestRegressor(**best_params['RandomForest']),\n",
    "              'XGBoost': XGBRegressor(objective='reg:squarederror', **best_params['XGBoost']),\n",
    "              'LightGBM': lgb.LGBMRegressor(**best_params['LightGBM'])}[best_model_name]\n",
    "best_model.fit(X, y)\n",
    "\n",
    "# Predict the next 60 days\n",
    "future_dates = pd.date_range(start=data.index[-1] + pd.Timedelta(days=1), periods=60, freq='D')\n",
    "future_data = pd.DataFrame(index=future_dates)\n",
    "future_data = create_date_features(future_data)\n",
    "future_data['is_public_holiday'] = future_data.index.to_series().apply(lambda date: 1 if date in uk_holidays else 0)\n",
    "\n",
    "# Iterative prediction for the next 60 days\n",
    "for date in future_dates:\n",
    "    lagged_features = {}\n",
    "    for lag in lags:\n",
    "        lagged_date = date - pd.Timedelta(days=lag)\n",
    "        if lagged_date in future_data.index:\n",
    "            lagged_features[f'lag_{lag}'] = future_data.at[lagged_date, 'Predicted']\n",
    "        else:\n",
    "            lagged_features[f'lag_{lag}'] = data.at[lagged_date, 'y'] if lagged_date in data.index else np.nan\n",
    "    future_data = future_data.assign(**lagged_features)\n",
    "    current_features = future_data.loc[date, date_features + [f'lag_{lag}' for lag in lags]].to_frame().T\n",
    "    future_data.at[date, 'Predicted'] = best_model.predict(current_features)[0]\n",
    "\n",
    "# Combine actual and predicted values\n",
    "combined_df = pd.DataFrame({\n",
    "    'Date': future_dates,\n",
    "    'Predicted': future_data['Predicted']\n",
    "}).set_index('Date')\n",
    "\n",
    "# Plot actual vs predicted values\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(test_y.index, test_y, label='Actual', marker='o')\n",
    "plt.plot(test_y.index, predictions[best_model_name], label=f'Predicted - {best_model_name}', marker='x')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Actual vs Predicted Values')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(evaluation_df)\n"
   ],
   "id": "42bae40289f6cecb",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T19:28:34.485259Z",
     "start_time": "2024-07-29T19:28:34.468407Z"
    }
   },
   "cell_type": "code",
   "source": "df.head()",
   "id": "6b3849ee2f7fd5a5",
   "execution_count": 3,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T19:28:39.709846Z",
     "start_time": "2024-07-29T19:28:39.705019Z"
    }
   },
   "cell_type": "code",
   "source": "x = df.copy()",
   "id": "772211d5b2bad5d6",
   "execution_count": 4,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T14:46:31.218783Z",
     "start_time": "2024-07-29T14:46:31.212602Z"
    }
   },
   "cell_type": "code",
   "source": "df = x.copy()",
   "id": "5667012220e99bdc",
   "execution_count": 14,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T19:28:47.378165Z",
     "start_time": "2024-07-29T19:28:46.915844Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import holidays\n",
    "# Function to create date features\n",
    "def create_date_features(data):\n",
    "    data[\"month\"] = data.index.month\n",
    "    data[\"day_of_month\"] = data.index.day\n",
    "    data[\"is_month_start\"] = data.index.is_month_start.astype(int)\n",
    "    data[\"is_month_end\"] = data.index.is_month_end.astype(int)\n",
    "    data[\"day_of_year\"] = data.index.dayofyear\n",
    "    data[\"week_of_year\"] = data.index.isocalendar().week\n",
    "    data[\"day_of_week\"] = data.index.dayofweek + 1\n",
    "    data[\"year\"] = data.index.year\n",
    "    data[\"is_weekend\"] = data.index.weekday\n",
    "    data['is_spring'] = data['month'].isin([3, 4, 5]).astype(int)\n",
    "    data['is_summer'] = data['month'].isin([6, 7, 8]).astype(int)\n",
    "    data['is_fall'] = data['month'].isin([9, 10, 11]).astype(int)\n",
    "    data['is_winter'] = data['month'].isin([12, 1, 2]).astype(int)\n",
    "    data['sin_day'] = np.sin(2 * np.pi * data.index.dayofweek / 7)\n",
    "    data['cos_day'] = np.cos(2 * np.pi * data.index.dayofweek / 7)\n",
    "    return data"
   ],
   "id": "6ac5bc49c84f8a3f",
   "execution_count": 5,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T19:29:14.361935Z",
     "start_time": "2024-07-29T19:29:08.830910Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Add holidays\n",
    " # Determine public holidays\n",
    "holiday = holidays.UK()\n",
    "df['is_public_holiday'] = df.index.map(lambda x: 1 if x in holiday else 0)\n",
    "def create_lag_and_window_features(data, target_col):\n",
    "    \"\"\"\n",
    "    Create lag and window features for a given DataFrame and target column.\n",
    "    \n",
    "    Parameters:\n",
    "    data (pd.DataFrame): The input DataFrame.\n",
    "    target_col (str): The name of the target column to create features for.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with lag and window features.\n",
    "    \"\"\"\n",
    "    # Create lag features for the past week\n",
    "    for i in range(1, 8):\n",
    "        data[f'lag_{i}'] = data[target_col].shift(i)\n",
    "\n",
    "    # Rolling window statistics for 7 days\n",
    "    data['rolling_mean_7'] = data[target_col].rolling(window=7).mean()\n",
    "    data['rolling_sum_7'] = data[target_col].rolling(window=7).sum()\n",
    "    data['rolling_std_7'] = data[target_col].rolling(window=7).std()\n",
    "\n",
    "    # Rolling window statistics for 30 days\n",
    "    data['rolling_mean_30'] = data[target_col].rolling(window=30).mean()\n",
    "    data['rolling_sum_30'] = data[target_col].rolling(window=30).sum()\n",
    "    data['rolling_std_30'] = data[target_col].rolling(window=30).std()\n",
    "\n",
    "    # Expanding window statistics\n",
    "    data['expanding_sum'] = data[target_col].expanding().sum()\n",
    "\n",
    "    return data\n",
    "\n",
    "df = create_lag_and_window_features(df, 'quantity')\n",
    "df.head(5)"
   ],
   "id": "ccef881476c643a2",
   "execution_count": 6,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T19:29:35.184583Z",
     "start_time": "2024-07-29T19:29:33.347149Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Fill any remaining NaN values\n",
    "df = df.fillna(method='bfill')\n",
    "# Set winsorization threshold\n",
    "threshold = 60000\n",
    "\n",
    "# Winsorize the 'quantity' column\n",
    "df['quantity'] = df['quantity'].where(df['quantity'] <= threshold, threshold)\n",
    "df.head(5)"
   ],
   "id": "c6dd6421a92cf562",
   "execution_count": 7,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T14:19:16.090724Z",
     "start_time": "2024-07-29T14:18:44.423680Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from prophet import Prophet\n",
    "from prophet.diagnostics import cross_validation, performance_metrics\n",
    "from prophet.plot import plot_cross_validation_metric\n",
    "import pandas as pd\n",
    "import holidays\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "# 1. Prepare Data\n",
    "data = df.copy()\n",
    "data[\"ds\"] = df.index\n",
    "data[\"y\"] = df[\"quantity\"]\n",
    "data['ds'] = pd.to_datetime(data['ds'])\n",
    "\n",
    "# Add is_public_holiday column\n",
    "holiday = holidays.CountryHoliday('UK')\n",
    "data['is_public_holiday'] = data['ds'].apply(\n",
    "    lambda date: 1 if date in holiday else 0\n",
    ")\n",
    "\n",
    "# 2. Create and Fit Prophet Model with Best Parameters\n",
    "best_params = {'changepoint_prior_scale': 0.005, 'seasonality_mode': 'additive', 'seasonality_prior_scale': 0.1, 'holidays_prior_scale': 1.0, 'n_changepoints': 25}\n",
    "\n",
    "model = Prophet(**best_params)\n",
    "model.add_regressor('is_public_holiday')\n",
    "model.fit(data)\n",
    "\n",
    "# 3. Forecast Future Values\n",
    "future = model.make_future_dataframe(periods=365)\n",
    "future['is_public_holiday'] = future['ds'].apply(\n",
    "    lambda date: 1 if date in holiday else 0\n",
    ")\n",
    "\n",
    "forecast = model.predict(future)\n",
    "\n",
    "# 4. Visualize Results\n",
    "model.plot(forecast)\n",
    "model.plot_components(forecast)\n",
    "\n",
    "# 5. Evaluate Accuracy\n",
    "df_cv = cross_validation(model, initial='547 days', period='180 days', horizon='30 days')\n",
    "df_p = performance_metrics(df_cv)\n",
    "print(df_p.head().round(2))\n",
    "\n",
    "fig = plot_cross_validation_metric(df_cv, metric='mape')\n",
    "\n",
    "# Calculate MAPE using yhat and y\n",
    "mape = mean_absolute_percentage_error(df_cv['y'], df_cv['yhat'])\n",
    "print(f\"MAPE: {round(mape, 4)}\")\n",
    "\n",
    "# 6. Plot Actual vs Predicted Values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(data['ds'], data['y'], label='Actual')\n",
    "plt.plot(forecast['ds'], forecast['yhat'], label='Predicted')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Quantity Winsorized')\n",
    "plt.legend()\n",
    "plt.title('Actual vs Predicted Values')\n",
    "plt.show()\n"
   ],
   "id": "e739ed303b7d78c0",
   "execution_count": 12,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T19:47:30.854222Z",
     "start_time": "2024-07-29T19:47:30.782828Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import holidays\n",
    "# Function to create date features\n",
    "def create_date_features(data):\n",
    "    data[\"month\"] = data.index.month\n",
    "    data[\"day_of_month\"] = data.index.day\n",
    "    data[\"is_month_start\"] = data.index.is_month_start.astype(int)\n",
    "    data[\"is_month_end\"] = data.index.is_month_end.astype(int)\n",
    "    data[\"day_of_year\"] = data.index.dayofyear\n",
    "    data[\"week_of_year\"] = data.index.isocalendar().week\n",
    "    data[\"day_of_week\"] = data.index.dayofweek + 1\n",
    "    data[\"year\"] = data.index.year\n",
    "    data[\"is_weekend\"] = data.index.weekday\n",
    "    data['is_spring'] = data['month'].isin([3, 4, 5]).astype(int)\n",
    "    data['is_summer'] = data['month'].isin([6, 7, 8]).astype(int)\n",
    "    data['is_fall'] = data['month'].isin([9, 10, 11]).astype(int)\n",
    "    data['is_winter'] = data['month'].isin([12, 1, 2]).astype(int)\n",
    "    data['sin_day'] = np.sin(2 * np.pi * data.index.dayofweek / 7)\n",
    "    data['cos_day'] = np.cos(2 * np.pi * data.index.dayofweek / 7)\n",
    "    return data\n",
    "\n",
    "df = create_date_features(df)\n",
    " # Determine public holidays\n",
    "holiday = holidays.UK()\n",
    "df['is_public_holiday'] = df.index.map(lambda x: 1 if x in holiday else 0)\n",
    "def create_lag_and_window_features(data, target_col):\n",
    "    \"\"\"\n",
    "    Create lag and window features for a given DataFrame and target column.\n",
    "    \n",
    "    Parameters:\n",
    "    data (pd.DataFrame): The input DataFrame.\n",
    "    target_col (str): The name of the target column to create features for.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with lag and window features.\n",
    "    \"\"\"\n",
    "    # Create lag features for the past week\n",
    "    for i in range(1, 8):\n",
    "        data[f'lag_{i}'] = data[target_col].shift(i)\n",
    "\n",
    "    # Rolling window statistics for 7 days\n",
    "    data['rolling_mean_7'] = data[target_col].rolling(window=7).mean()\n",
    "    data['rolling_sum_7'] = data[target_col].rolling(window=7).sum()\n",
    "    data['rolling_std_7'] = data[target_col].rolling(window=7).std()\n",
    "\n",
    "    # Rolling window statistics for 30 days\n",
    "    data['rolling_mean_30'] = data[target_col].rolling(window=30).mean()\n",
    "    data['rolling_sum_30'] = data[target_col].rolling(window=30).sum()\n",
    "    data['rolling_std_30'] = data[target_col].rolling(window=30).std()\n",
    "\n",
    "    # Expanding window statistics\n",
    "    data['expanding_sum'] = data[target_col].expanding().sum()\n",
    "\n",
    "    return data\n",
    "\n",
    "df = create_lag_and_window_features(df, 'quantity')\n",
    "\n",
    "\n",
    "# Fill any remaining NaN values\n",
    "df = df.fillna(method='bfill')\n",
    "\n",
    "# Set winsorization threshold\n",
    "threshold = 60000\n",
    "\n",
    "# Winsorize the 'quantity' column\n",
    "df['quantity'] = df['quantity'].where(df['quantity'] <= threshold, threshold)\n",
    "df.head(5)\n",
    "\n"
   ],
   "id": "161eb793d2466cbc",
   "execution_count": 16,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T19:47:43.974248Z",
     "start_time": "2024-07-29T19:47:43.951600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import holidays\n",
    "\n",
    "def create_date_features(data):\n",
    "    data[\"month\"] = data['ds'].dt.month\n",
    "    data[\"day_of_month\"] = data['ds'].dt.day\n",
    "    data[\"is_month_start\"] = data['ds'].dt.is_month_start.astype(int)\n",
    "    data[\"is_month_end\"] = data['ds'].dt.is_month_end.astype(int)\n",
    "    data[\"day_of_year\"] = data['ds'].dt.dayofyear\n",
    "    data[\"week_of_year\"] = data['ds'].dt.isocalendar().week\n",
    "    data[\"day_of_week\"] = data['ds'].dt.dayofweek + 1\n",
    "    data[\"year\"] = data['ds'].dt.year\n",
    "    data[\"is_weekend\"] = (data['ds'].dt.weekday >= 5).astype(int)\n",
    "    data['is_spring'] = data['month'].isin([3, 4, 5]).astype(int)\n",
    "    data['is_summer'] = data['month'].isin([6, 7, 8]).astype(int)\n",
    "    data['is_fall'] = data['month'].isin([9, 10, 11]).astype(int)\n",
    "    data['is_winter'] = data['month'].isin([12, 1, 2]).astype(int)\n",
    "    data['sin_day'] = np.sin(2 * np.pi * data['ds'].dt.dayofweek / 7)\n",
    "    data['cos_day'] = np.cos(2 * np.pi * data['ds'].dt.dayofweek / 7)\n",
    "    return data\n",
    "\n",
    "def create_lag_and_window_features(data, target_col):\n",
    "    for i in range(1, 8):\n",
    "        data[f'lag_{i}'] = data[target_col].shift(i)\n",
    "\n",
    "    data['rolling_mean_7'] = data[target_col].rolling(window=7).mean()\n",
    "    data['rolling_sum_7'] = data[target_col].rolling(window=7).sum()\n",
    "    data['rolling_std_7'] = data[target_col].rolling(window=7).std()\n",
    "\n",
    "    data['rolling_mean_30'] = data[target_col].rolling(window=30).mean()\n",
    "    data['rolling_sum_30'] = data[target_col].rolling(window=30).sum()\n",
    "    data['rolling_std_30'] = data[target_col].rolling(window=30).std()\n",
    "\n",
    "    data['expanding_sum'] = data[target_col].expanding().sum()\n",
    "\n",
    "    return data\n",
    "\n",
    "def preprocess_data(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    # Ensure 'ds' column is datetime\n",
    "    df['ds'] = df.index\n",
    "    df['ds'] = pd.to_datetime(df['ds'])\n",
    "\n",
    "    # Create date features\n",
    "    df = create_date_features(df)\n",
    "\n",
    "    # Determine public holidays\n",
    "    holiday = holidays.UK()\n",
    "    df['is_public_holiday'] = df['ds'].apply(lambda x: 1 if x in holiday else 0)\n",
    "\n",
    "    # Create lag and window features\n",
    "    df = create_lag_and_window_features(df, 'quantity')\n",
    "\n",
    "    # Fill any remaining NaN values\n",
    "    df = df.fillna(method='bfill')\n",
    "\n",
    "    # Set winsorization threshold\n",
    "    threshold = 60000\n",
    "\n",
    "    # Winsorize the 'quantity' column\n",
    "    df['quantity'] = df['quantity'].where(df['quantity'] <= threshold, threshold)\n",
    "\n",
    "    return df"
   ],
   "id": "c3ae767d98a0bf09",
   "execution_count": 17,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T19:30:07.981119Z",
     "start_time": "2024-07-29T19:30:07.954099Z"
    }
   },
   "cell_type": "code",
   "source": "x.head()",
   "id": "dda87fc4f6ee0f75",
   "execution_count": 9,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T19:47:46.908450Z",
     "start_time": "2024-07-29T19:47:46.829413Z"
    }
   },
   "cell_type": "code",
   "source": [
    "b = preprocess_data(x)\n",
    "b.head()"
   ],
   "id": "61560d3aa21b6981",
   "execution_count": 18,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T19:32:12.072245Z",
     "start_time": "2024-07-29T19:31:24.190142Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "import holidays\n",
    "\n",
    "# Make a copy of the dataframe\n",
    "rml_data = b.copy()\n",
    "\n",
    "\n",
    "rml_data['y'] = rml_data['quantity']\n",
    "\n",
    "# Define features and target\n",
    "features = ['month', 'day_of_month', 'is_month_start', 'is_month_end',\n",
    "            'day_of_year', 'week_of_year', 'day_of_week', 'year',\n",
    "            'is_weekend', 'is_spring', 'is_summer', 'is_fall', 'is_winter',\n",
    "            'sin_day', 'cos_day', 'is_public_holiday', 'lag_1', 'lag_2', \n",
    "            'lag_3', 'lag_4', 'lag_5', 'lag_6', 'lag_7', 'rolling_mean_7', \n",
    "            'rolling_sum_7', 'rolling_std_7', 'rolling_mean_30', \n",
    "            'rolling_sum_30', 'rolling_std_30', 'expanding_sum']\n",
    "\n",
    "target = 'y'\n",
    "\n",
    "# Split the data into train and test sets\n",
    "train_df = rml_data[:-30]\n",
    "test_df = rml_data[-30:]\n",
    "\n",
    "X_train, y_train = train_df[features], train_df[target]\n",
    "X_test, y_test = test_df[features], test_df[target]\n",
    "\n",
    "# Define the models and hyperparameters for tuning\n",
    "models = {\n",
    "    'LightGBM': (lgb.LGBMRegressor(), {\n",
    "        'n_estimators': [100, 200],\n",
    "        'learning_rate': [0.01, 0.1],\n",
    "        'num_leaves': [31, 127]\n",
    "    }),\n",
    "    'XGBoost': (XGBRegressor(objective='reg:squarederror'), {\n",
    "        'n_estimators': [100, 200],\n",
    "        'learning_rate': [0.01, 0.1],\n",
    "        'max_depth': [3, 6]\n",
    "    }),\n",
    "    'RandomForest': (RandomForestRegressor(), {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_features': ['auto', 'sqrt'],\n",
    "        'max_depth': [10, 20]\n",
    "    })\n",
    "}\n",
    "\n",
    "# Initialize lists to store the scores\n",
    "mae_scores = {}\n",
    "mape_scores = {}\n",
    "\n",
    "# Train and evaluate each model\n",
    "for model_name, (model, params) in models.items():\n",
    "    grid_search = GridSearchCV(model, params, cv=5, scoring='neg_mean_absolute_percentage_error', n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    best_model = grid_search.best_estimator_\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    \n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "    \n",
    "    mae_scores[model_name] = mae\n",
    "    mape_scores[model_name] = mape\n",
    "    \n",
    "    print(f\"{model_name} - MAE: {mae:.4f}, MAPE: {mape:.4%}\")\n",
    "\n",
    "# Select the best model based on MAPE\n",
    "best_model_name = min(mape_scores, key=mape_scores.get)\n",
    "best_model = models[best_model_name][0]\n",
    "print(f\"\\nThe best model is {best_model_name} with a MAPE of {mape_scores[best_model_name]:.4%}\")\n",
    "\n",
    "# Retrain the best model on the entire dataset\n",
    "best_model.fit(rml_data[features], rml_data[target])\n",
    "\n",
    "# Predict the next 30 days\n",
    "future_dates = pd.date_range(start=rml_data['ds'].max() + pd.Timedelta(days=1), periods=30, freq='D')\n",
    "future_df = pd.DataFrame({\n",
    "    'ds': future_dates,\n",
    "    'month': future_dates.month,\n",
    "    'day_of_month': future_dates.day,\n",
    "    'is_month_start': future_dates.is_month_start.astype(int),\n",
    "    'is_month_end': future_dates.is_month_end.astype(int),\n",
    "    'day_of_year': future_dates.dayofyear,\n",
    "    'week_of_year': future_dates.isocalendar().week,\n",
    "    'day_of_week': future_dates.dayofweek + 1,\n",
    "    'year': future_dates.year,\n",
    "    'is_weekend': (future_dates.weekday >= 5).astype(int),\n",
    "    'is_spring': future_dates.month.isin([3, 4, 5]).astype(int),\n",
    "    'is_summer': future_dates.month.isin([6, 7, 8]).astype(int),\n",
    "    'is_fall': future_dates.month.isin([9, 10, 11]).astype(int),\n",
    "    'is_winter': future_dates.month.isin([12, 1, 2]).astype(int),\n",
    "    'sin_day': np.sin(2 * np.pi * future_dates.dayofweek / 7),\n",
    "    'cos_day': np.cos(2 * np.pi * future_dates.dayofweek / 7)\n",
    "})\n",
    "\n",
    "# Add public holidays for the future dates\n",
    "uk_holidays = holidays.UK()\n",
    "future_df['is_public_holiday'] = future_df['ds'].apply(lambda date: 1 if date in uk_holidays else 0)\n",
    "\n",
    "# Add lag features and rolling statistics for the future_df\n",
    "for lag in range(1, 8):\n",
    "    future_df[f'lag_{lag}'] = rml_data[target].shift(lag).iloc[-30:].values\n",
    "\n",
    "for window in [7, 30]:\n",
    "    future_df[f'rolling_mean_{window}'] = rml_data[target].rolling(window=window).mean().iloc[-30:].values\n",
    "    future_df[f'rolling_sum_{window}'] = rml_data[target].rolling(window=window).sum().iloc[-30:].values\n",
    "    future_df[f'rolling_std_{window}'] = rml_data[target].rolling(window=window).std().iloc[-30:].values\n",
    "\n",
    "future_df['expanding_sum'] = rml_data[target].expanding().sum().iloc[-30:].values\n",
    "\n",
    "# Predict future values using the best model\n",
    "future_predictions = best_model.predict(future_df[features])\n",
    "\n",
    "# Plot actual vs predicted values\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(test_df['ds'], test_df['y'], label='Actual', marker='o')\n",
    "plt.plot(future_df['ds'], future_predictions, label=f'Predicted - {best_model_name}', marker='x')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Actual vs Predicted Values')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ],
   "id": "22130f10b9311bf8",
   "execution_count": 13,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T19:37:20.633596Z",
     "start_time": "2024-07-29T19:36:38.150849Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "import holidays\n",
    "\n",
    "# Make a copy of the dataframe\n",
    "rml_data = b.copy()\n",
    "\n",
    "# Ensure 'ds' column is datetime\n",
    "rml_data['ds'] = pd.to_datetime(rml_data['ds'])\n",
    "rml_data['y'] = rml_data['quantity']\n",
    "\n",
    "# Define features and target\n",
    "features = ['month', 'day_of_month', 'is_month_start', 'is_month_end',\n",
    "            'day_of_year', 'week_of_year', 'day_of_week', 'year',\n",
    "            'is_weekend', 'is_spring', 'is_summer', 'is_fall', 'is_winter',\n",
    "            'sin_day', 'cos_day', 'is_public_holiday', 'lag_1', 'lag_2', \n",
    "            'lag_3', 'lag_4', 'lag_5', 'lag_6', 'lag_7', 'rolling_mean_7', \n",
    "            'rolling_sum_7', 'rolling_std_7', 'rolling_mean_30', \n",
    "            'rolling_sum_30', 'rolling_std_30', 'expanding_sum']\n",
    "\n",
    "target = 'y'\n",
    "\n",
    "# Split the data into train and test sets\n",
    "train_df = rml_data[:-30]\n",
    "test_df = rml_data[-30:]\n",
    "\n",
    "X_train, y_train = train_df[features], train_df[target]\n",
    "X_test, y_test = test_df[features], test_df[target]\n",
    "\n",
    "# Define the models and hyperparameters for tuning\n",
    "models = {\n",
    "    'LightGBM': (lgb.LGBMRegressor(), {\n",
    "        'n_estimators': [100, 200],\n",
    "        'learning_rate': [0.01, 0.1],\n",
    "        'num_leaves': [31, 127]\n",
    "    }),\n",
    "    'XGBoost': (XGBRegressor(objective='reg:squarederror'), {\n",
    "        'n_estimators': [100, 200],\n",
    "        'learning_rate': [0.01, 0.1],\n",
    "        'max_depth': [3, 6]\n",
    "    }),\n",
    "    'RandomForest': (RandomForestRegressor(), {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_features': ['auto', 'sqrt'],\n",
    "        'max_depth': [10, 20]\n",
    "    })\n",
    "}\n",
    "\n",
    "# Initialize lists to store the scores\n",
    "mae_scores = {}\n",
    "mape_scores = {}\n",
    "best_params = {}\n",
    "\n",
    "# Train and evaluate each model\n",
    "for model_name, (model, params) in models.items():\n",
    "    grid_search = GridSearchCV(model, params, cv=5, scoring='neg_mean_absolute_percentage_error', n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_params[model_name] = grid_search.best_params_\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    \n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "    \n",
    "    mae_scores[model_name] = mae\n",
    "    mape_scores[model_name] = mape\n",
    "    \n",
    "    print(f\"{model_name} - Best Params: {grid_search.best_params_}\")\n",
    "    print(f\"{model_name} - MAE: {mae:.4f}, MAPE: {mape:.4%}\")\n",
    "\n",
    "# Select the best model based on MAPE\n",
    "best_model_name = min(mape_scores, key=mape_scores.get)\n",
    "best_model = models[best_model_name][0]\n",
    "print(f\"\\nThe best model is {best_model_name} with a MAPE of {mape_scores[best_model_name]:.4%}\")\n",
    "print(f\"Best Hyperparameters: {best_params[best_model_name]}\")\n",
    "\n",
    "# Retrain the best model on the entire dataset\n",
    "best_model.set_params(**best_params[best_model_name])\n",
    "best_model.fit(rml_data[features], rml_data[target])\n",
    "\n",
    "# Predict the next 30 days\n",
    "future_dates = pd.date_range(start=rml_data['ds'].max() + pd.Timedelta(days=1), periods=30, freq='D')\n",
    "future_df = pd.DataFrame({\n",
    "    'ds': future_dates,\n",
    "    'month': future_dates.month,\n",
    "    'day_of_month': future_dates.day,\n",
    "    'is_month_start': future_dates.is_month_start.astype(int),\n",
    "    'is_month_end': future_dates.is_month_end.astype(int),\n",
    "    'day_of_year': future_dates.dayofyear,\n",
    "    'week_of_year': future_dates.isocalendar().week,\n",
    "    'day_of_week': future_dates.dayofweek + 1,\n",
    "    'year': future_dates.year,\n",
    "    'is_weekend': (future_dates.weekday >= 5).astype(int),\n",
    "    'is_spring': future_dates.month.isin([3, 4, 5]).astype(int),\n",
    "    'is_summer': future_dates.month.isin([6, 7, 8]).astype(int),\n",
    "    'is_fall': future_dates.month.isin([9, 10, 11]).astype(int),\n",
    "    'is_winter': future_dates.month.isin([12, 1, 2]).astype(int),\n",
    "    'sin_day': np.sin(2 * np.pi * future_dates.dayofweek / 7),\n",
    "    'cos_day': np.cos(2 * np.pi * future_dates.dayofweek / 7)\n",
    "})\n",
    "\n",
    "# Add public holidays for the future dates\n",
    "uk_holidays = holidays.UK()\n",
    "future_df['is_public_holiday'] = future_df['ds'].apply(lambda date: 1 if date in uk_holidays else 0)\n",
    "\n",
    "# Add lag features and rolling statistics for the future_df\n",
    "for lag in range(1, 8):\n",
    "    future_df[f'lag_{lag}'] = rml_data[target].shift(lag).iloc[-30:].values\n",
    "\n",
    "for window in [7, 30]:\n",
    "    future_df[f'rolling_mean_{window}'] = rml_data[target].rolling(window=window).mean().iloc[-30:].values\n",
    "    future_df[f'rolling_sum_{window}'] = rml_data[target].rolling(window=window).sum().iloc[-30:].values\n",
    "    future_df[f'rolling_std_{window}'] = rml_data[target].rolling(window=window).std().iloc[-30:].values\n",
    "\n",
    "future_df['expanding_sum'] = rml_data[target].expanding().sum().iloc[-30:].values\n",
    "\n",
    "# Predict future values using the best model\n",
    "future_predictions = best_model.predict(future_df[features])\n",
    "\n",
    "# Plot actual vs predicted values\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(rml_data['ds'], rml_data['y'], label='Actual', marker='o')\n",
    "plt.plot(future_df['ds'], future_predictions, label=f'Predicted - {best_model_name}', marker='x')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Actual vs Predicted Values')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ],
   "id": "ef6c5b64bb63bd27",
   "execution_count": 14,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T19:41:39.090760Z",
     "start_time": "2024-07-29T19:41:37.212078Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error\n",
    "import matplotlib.pyplot as plt\n",
    "import holidays\n",
    "\n",
    "# Make a copy of the dataframe\n",
    "rml_data = b.copy()\n",
    "\n",
    "# Ensure 'ds' column is datetime\n",
    "rml_data['ds'] = pd.to_datetime(rml_data['ds'])\n",
    "rml_data['y'] = rml_data['quantity']\n",
    "\n",
    "# Define features and target\n",
    "features = ['month', 'day_of_month', 'is_month_start', 'is_month_end',\n",
    "            'day_of_year', 'week_of_year', 'day_of_week', 'year',\n",
    "            'is_weekend', 'is_spring', 'is_summer', 'is_fall', 'is_winter',\n",
    "            'sin_day', 'cos_day', 'is_public_holiday', 'lag_1', 'lag_2', \n",
    "            'lag_3', 'lag_4', 'lag_5', 'lag_6', 'lag_7', 'rolling_mean_7', \n",
    "            'rolling_sum_7', 'rolling_std_7', 'rolling_mean_30', \n",
    "            'rolling_sum_30', 'rolling_std_30', 'expanding_sum']\n",
    "\n",
    "target = 'y'\n",
    "\n",
    "# Split the data into train and test sets\n",
    "train_df = rml_data[:-30]\n",
    "test_df = rml_data[-30:]\n",
    "\n",
    "X_train, y_train = train_df[features], train_df[target]\n",
    "X_test, y_test = test_df[features], test_df[target]\n",
    "\n",
    "# Define the best parameters for the XGBoost model\n",
    "best_params = {\n",
    "    'n_estimators': 200,\n",
    "    'learning_rate': 0.1,\n",
    "    'max_depth': 3\n",
    "}\n",
    "\n",
    "# Train the XGBoost model with the best parameters\n",
    "best_model = XGBRegressor(objective='reg:squarederror', **best_params)\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the entire historical period\n",
    "predictions = best_model.predict(rml_data[features])\n",
    "\n",
    "# Calculate MAPE for the predictions\n",
    "mape = mean_absolute_percentage_error(rml_data[target], predictions)\n",
    "print(f\"MAPE: {mape:.4%}\")\n",
    "\n",
    "# Plot actual vs predicted values\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Plot the actual values\n",
    "plt.plot(rml_data['ds'], rml_data['y'], label='Actual')\n",
    "\n",
    "# Plot the predicted values\n",
    "plt.plot(rml_data['ds'], predictions, label='Predicted')\n",
    "\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Quantity')\n",
    "plt.title('Actual vs Predicted Values')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(f\"Best Hyperparameters: {best_params}\")\n"
   ],
   "id": "5adec2b963eb7e11",
   "execution_count": 15,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "",
   "id": "87330f67b882ff0a",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-09T22:00:05.604865200Z",
     "start_time": "2024-05-09T22:00:05.484944200Z"
    }
   },
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from fpgrowth_py import fpgrowth\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gr"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "df = pd.read_csv(\"main.csv\")\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-06T04:42:34.856039400Z",
     "start_time": "2024-03-06T04:42:34.625161Z"
    }
   },
   "id": "ce47ef3dcac1c236",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "x1 = df[[\"CustomerID\",\"cluster\",\"Days_Since_Last_Purchase\",\"Average_Days_Between_Purchases\",\"Average_Transaction_Value\"]]\n",
    "x1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-06T04:42:34.966070100Z",
     "start_time": "2024-03-06T04:42:34.840396200Z"
    }
   },
   "id": "e350a5d572c3a535",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "x1['R_score'] = pd.qcut(x1['Days_Since_Last_Purchase'], q=3, labels=[1, 2, 3])  # High recency will have a score of 1\n",
    "x1['F_score'] = pd.qcut(x1['Average_Days_Between_Purchases'], q=3, labels=[1, 2, 3]) \n",
    "x1['M_score'] = pd.qcut(x1['Average_Transaction_Value'], q=3, labels=[1, 2, 3]) \n",
    "x1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-06T04:42:35.151054800Z",
     "start_time": "2024-03-06T04:42:34.903549600Z"
    }
   },
   "id": "f2a55f51ca03d7e",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "x1['RFM'] = x1[['R_score', 'F_score', 'M_score']].astype(str).agg(''.join, axis=1)\n",
    "x1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-06T04:42:35.451742200Z",
     "start_time": "2024-03-06T04:42:34.998132700Z"
    }
   },
   "id": "54e4b9f5b2391308",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def assign_cluster(score):\n",
    "    high_value_scores = ['111', '112', '113', '121', '122', '123']\n",
    "    nurture_scores = ['133', '213', '222', '223', '232', '233']\n",
    "    risk_scores = ['131', '132', '231', '311', '312', '313', '321', '322', '323', '331', '332', '333']\n",
    "\n",
    "    if score in high_value_scores:\n",
    "        return 'High Value'\n",
    "    elif score in nurture_scores:\n",
    "        return 'Nurture'\n",
    "    elif score in risk_scores:\n",
    "        return 'Risk'\n",
    "    else:\n",
    "        return 'Other'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-06T04:42:35.530342500Z",
     "start_time": "2024-03-06T04:42:35.189510100Z"
    }
   },
   "id": "ee02973f88f1e177",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "x1['Segment'] = x1['RFM'].apply(assign_cluster)\n",
    "x1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-06T04:42:35.530342500Z",
     "start_time": "2024-03-06T04:42:35.199295400Z"
    }
   },
   "id": "89f9db3a70353fbf",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "xci = x1[[\"CustomerID\",\"Segment\"]]\n",
    "\n",
    "xci"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-06T04:42:35.530342500Z",
     "start_time": "2024-03-06T04:42:35.277603400Z"
    }
   },
   "id": "ed173f8d1dd924c9",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "xci[\"Segment\"].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-06T04:42:35.530342500Z",
     "start_time": "2024-03-06T04:42:35.341626500Z"
    }
   },
   "id": "fe5dcf04310d9ad7",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "def map_rfm_labels(rfm_score):\n",
    "    \"\"\"Maps RFM scores to descriptive labels.\n",
    "\n",
    "    Args:\n",
    "        rfm_score: A string representing the RFM score (e.g., '111').\n",
    "\n",
    "    Returns:\n",
    "        The corresponding descriptive label (e.g., 'Champions').\n",
    "    \"\"\"\n",
    "\n",
    "    label_map = {\n",
    "        '111': 'Champions',\n",
    "        '112': 'Loyalists',\n",
    "        '113': 'High Potential',\n",
    "        '121': 'Big Spender',\n",
    "        '122': 'Regular Spenders',\n",
    "        '123': 'Emerging Loyalists',\n",
    "        '133': 'Needs a Spark',\n",
    "        '213': 'Upscale Focus',\n",
    "        '222': 'Consistent Spender',\n",
    "        '223': 'Potential Upscale',\n",
    "        '232': 'Win-Back Target',\n",
    "        '233': 'Casual Shopper',\n",
    "        '131': 'Wake-Up Call',\n",
    "        '132': 'Slipping Away',\n",
    "        '231': 'Dormant Upscale',\n",
    "        '311': 'One-offs', \n",
    "        '312': 'One-offs', \n",
    "        '313': 'One-offs',\n",
    "        '321': 'Sporadic',\n",
    "        '322': 'Sporadic',\n",
    "        '323': 'Sporadic',\n",
    "        '331': 'Lost Cause',\n",
    "        '332': 'Lost Cause',\n",
    "        '333': 'Lost Cause'\n",
    "    }\n",
    "\n",
    "    return label_map.get(rfm_score, 'Uncategorized')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-06T04:42:35.730244200Z",
     "start_time": "2024-03-06T04:42:35.420475600Z"
    }
   },
   "id": "f8c3ac76781f17b4",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "x1['Subsegment'] = x1['RFM'].apply(map_rfm_labels)\n",
    "x1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-06T04:42:35.808815400Z",
     "start_time": "2024-03-06T04:42:35.489534800Z"
    }
   },
   "id": "286d70fcc12a2997",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "x1.columns"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-06T04:42:35.808815400Z",
     "start_time": "2024-03-06T04:42:35.563031600Z"
    }
   },
   "id": "5da59f3d6bd94170",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "cdata = pd.read_csv(\"express.csv\")\n",
    "cdata\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-06T04:49:27.821227700Z",
     "start_time": "2024-03-06T04:49:21.362639Z"
    }
   },
   "id": "6f1d7587fa6e4410",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "cdata =cdata[[\"CustomerID\",\"InvoiceNo\",\"StockCode\"]]\n",
    "cdata\n",
    "\n",
    " "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-06T04:49:36.071718800Z",
     "start_time": "2024-03-06T04:49:35.977546200Z"
    }
   },
   "id": "cc678f58d728827d",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "basket = cdata.groupby([\"InvoiceNo\",\"CustomerID\"]).agg({\n",
    "    \"StockCode\": lambda s : list(set(s))\n",
    "})\n",
    "\n",
    "basket"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-06T04:49:48.069008300Z",
     "start_time": "2024-03-06T04:49:47.170865900Z"
    }
   },
   "id": "6fbd9c2da7b8e01d",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "unique = cdata.drop_duplicates(subset=\"CustomerID\")\n",
    "unique"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-06T04:42:44.784271300Z",
     "start_time": "2024-03-06T04:42:44.579598100Z"
    }
   },
   "id": "e5f570b77189473b",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "xci[\"CustomerID\"] = xci[\"CustomerID\"].astype(object)\n",
    "xci.dtypes"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-06T04:56:49.035805400Z",
     "start_time": "2024-03-06T04:56:49.004248400Z"
    }
   },
   "id": "e9b392f05eec8c50",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "trial = pd.merge(cdata,xci,on=\"CustomerID\",how=\"left\")\n",
    "trial"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-06T05:07:05.167281600Z",
     "start_time": "2024-03-06T05:07:04.793102200Z"
    }
   },
   "id": "6820bb889f4dd1f8",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "trial.sample(5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-06T05:07:42.025014200Z",
     "start_time": "2024-03-06T05:07:41.977737700Z"
    }
   },
   "id": "b3b54376b274a91d",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "n1 = cdata.loc[cdata[\"CustomerID\"]!=\"Unknown\"]\n",
    "n1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-06T05:12:07.205103500Z",
     "start_time": "2024-03-06T05:12:07.035968700Z"
    }
   },
   "id": "fdfbce3df3537475",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "\n",
    "trial = pd.merge(n1, xci, on=\"CustomerID\", how=\"left\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-06T05:13:00.237767200Z",
     "start_time": "2024-03-06T05:12:59.917877900Z"
    }
   },
   "id": "d87d776a816a9390",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "trial"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-06T05:13:10.334827800Z",
     "start_time": "2024-03-06T05:13:10.303360Z"
    }
   },
   "id": "c3cccf1d49da7224",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "n1['CustomerID'] = n1['CustomerID'].astype(str)\n",
    "xci['CustomerID'] = xci['CustomerID'].astype(str)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-06T06:14:06.852015400Z",
     "start_time": "2024-03-06T06:14:06.789013600Z"
    }
   },
   "id": "a6bce1564858221d",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "merged_df = pd.merge(n1, xci, on='CustomerID', how='left')\n",
    "merged_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-06T06:14:25.604520Z",
     "start_time": "2024-03-06T06:14:25.113130400Z"
    }
   },
   "id": "fa173f1fab01822",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "merged_df.dropna(inplace=True)\n",
    "merged_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-06T06:16:28.121107100Z",
     "start_time": "2024-03-06T06:16:25.956347900Z"
    }
   },
   "id": "c76316cafb4f8e57",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "merged_df.Segment.value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-06T06:17:03.778050400Z",
     "start_time": "2024-03-06T06:17:03.731437600Z"
    }
   },
   "id": "855d453ac53ccd04",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "high_value_df = merged_df[merged_df['Segment'] == 'High Value']\n",
    "risk_df = merged_df[merged_df['Segment'] == 'Risk']\n",
    "nurture_df = merged_df[merged_df['Segment'] == 'Nurture']\n",
    "other_df = merged_df[merged_df['Segment'].isin(['High Value', 'Risk', 'Nurture']) == False]\n",
    "\n",
    "high_value_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-06T06:20:03.135763100Z",
     "start_time": "2024-03-06T06:20:02.955742700Z"
    }
   },
   "id": "49b952626f8517f0",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "source": [
    "# Aggregate Transctions\n",
    "def aggregate_transactions(df):\n",
    "     transactions = df.groupby([\"InvoiceNo\",\"CustomerID\"]).agg({\"StockCode\": lambda s : list(set(s))})\n",
    "     return transactions"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-06T10:40:29.338844200Z",
     "start_time": "2024-03-06T10:40:29.292563200Z"
    }
   },
   "id": "6e16e413eb5531c2",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "source": [
    "def get_rules(df):\n",
    "    hbasket = aggregate_transactions(df)\n",
    "    freqItemSet, rules = fpgrowth(hbasket['StockCode'].values, minSupRatio=0.01, minConf=0.8)\n",
    "    print('Number of rules generated : ', len(rules))\n",
    "    association=pd.DataFrame(rules,columns =['basket','next_product','proba']) \n",
    "    association=association.sort_values(by='proba',ascending=False)\n",
    "    \n",
    "    return association\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-06T10:40:38.999115200Z",
     "start_time": "2024-03-06T10:40:38.986987100Z"
    }
   },
   "id": "8e829feae2bfd230",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "source": [
    "lb = get_rules(other_df)\n",
    "lb"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-06T07:50:52.666851900Z",
     "start_time": "2024-03-06T07:50:51.004868400Z"
    }
   },
   "id": "c6c08bbb0586273",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "source": [
    "#Get frequent item sets\n",
    "def get_most_frequent_itemsets(df):\n",
    "    # Group by InvoiceNo and CustomerID and aggregate unique StockCodes into lists\n",
    "    hbasket = aggregate_transactions(df)\n",
    "    \n",
    "    # Run FP-Growth algorithm to find frequent itemsets\n",
    "    freqItemSet, _ = fpgrowth(hbasket['StockCode'].values, minSupRatio=0.01, minConf=0.7)\n",
    "    \n",
    "    # Sort items within each itemset and convert frequent itemsets to DataFrame\n",
    "    frequent_itemsets_df = pd.DataFrame({'Frequent Itemset': [sorted(itemset) for itemset in freqItemSet]})\n",
    "    \n",
    "    # Remove duplicates (after sorting, duplicate itemsets will be identical)\n",
    "    frequent_itemsets_df = frequent_itemsets_df.drop_duplicates(subset='Frequent Itemset')\n",
    "    \n",
    "    # Filter out itemsets with less than 2 items\n",
    "    frequent_itemsets_df = frequent_itemsets_df[frequent_itemsets_df['Frequent Itemset'].apply(len) > 2]\n",
    "    \n",
    "    return frequent_itemsets_df\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-06T10:41:19.675235Z",
     "start_time": "2024-03-06T10:41:19.643160300Z"
    }
   },
   "id": "91197564da5ca379",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "source": [
    "xg = get_most_frequent_itemsets(nurture_df)\n",
    "xg"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-06T08:05:17.970069700Z",
     "start_time": "2024-03-06T08:04:59.834928500Z"
    }
   },
   "id": "9c9959590d1c9cd1",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "source": [
    "xg[\"Frequent Itemset\"][107]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-06T10:45:57.953986500Z",
     "start_time": "2024-03-06T10:45:57.907305900Z"
    }
   },
   "id": "b1aa3d55acacb3c6",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Promotional Strategies"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "53e224540340ed8d"
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "source": [
    "#Get Buy One,Get One Discounted Bundles\n",
    "def get_bogd_bundles(df):\n",
    "        hbasket = aggregate_transactions(df)\n",
    "        freqItemSet, rules = fpgrowth(hbasket['StockCode'].values, minSupRatio=0.01, minConf=0.9)\n",
    "        print('Number of rules generated : ', len(rules))\n",
    "        \n",
    "        association=pd.DataFrame(rules,columns =['basket','next_product','proba']) \n",
    "        association=association.sort_values(by='proba',ascending=False)\n",
    "    \n",
    "        return association\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-06T10:23:27.394012500Z",
     "start_time": "2024-03-06T10:23:27.374354700Z"
    }
   },
   "id": "851da77de8b97173",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "source": [
    "# Buy One,Get One Discounted\n",
    "def bogd(product_bundles, order):\n",
    "    # Check if any bundle matches the items in the order\n",
    "    matching_bundles = product_bundles[product_bundles['basket'].apply(lambda x: set(x).issubset(set(order)))]\n",
    "\n",
    "    if matching_bundles.empty:\n",
    "        print(\"Order Not Eligible For Discount\")\n",
    "        return []\n",
    "\n",
    "    # Extract and return consequent products from matching bundles\n",
    "    recommended_products = matching_bundles['next_product'].tolist()\n",
    "    recommended_item_codes = [item for sublist in recommended_products for item in sublist]\n",
    "    return recommended_item_codes"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-06T10:23:31.585904700Z",
     "start_time": "2024-03-06T10:23:31.580386900Z"
    }
   },
   "id": "b7d6247b21e829d6",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "source": [
    "gx = get_bogd_bundles(high_value_df)\n",
    "gx"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-06T10:24:49.263122800Z",
     "start_time": "2024-03-06T10:23:51.811267300Z"
    }
   },
   "id": "53482981c7764a47",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "source": [
    "order = gx[\"basket\"][2]  # Assuming you want to check this specific itemset\n",
    "bogd(gx, order)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-06T10:19:57.600746900Z",
     "start_time": "2024-03-06T10:19:57.571368900Z"
    }
   },
   "id": "73563af0d8b89e64",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "source": [
    "order = {'21080','45373', '21086','67262','63773'}\n",
    "bogd(gx, order)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-06T10:20:40.092787Z",
     "start_time": "2024-03-06T10:20:40.061242500Z"
    }
   },
   "id": "bc10b22a8a7773c8",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Fixed amount discount\n",
    "def apply_fixed_discount(total_amount, fixed_discount):\n",
    "    \"\"\"\n",
    "    Apply a fixed amount discount to the total purchase amount.\n",
    "\n",
    "    Parameters:\n",
    "    - total_amount (float): Total purchase amount before discount.\n",
    "    - fixed_discount (float): Fixed discount amount to be subtracted.\n",
    "\n",
    "    Returns:\n",
    "    - discounted_amount (float): Total purchase amount after applying the fixed discount.\n",
    "    \"\"\"\n",
    "    discounted_amount = max(total_amount - fixed_discount, 0)  # Ensure discounted amount doesn't go below zero\n",
    "    return discounted_amount\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3a11ef5833ca353e",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "source": [
    "#Bundled Discounts\n",
    "def apply_bundle_discount(bundle, discount_df):\n",
    "    \"\"\"\n",
    "    Apply discounts to each item in a product bundle and return the final price.\n",
    "\n",
    "    Parameters:\n",
    "    - bundle (list): List of product IDs in the bundle.\n",
    "    - discount_df (DataFrame): DataFrame containing product IDs and their discounts.\n",
    "\n",
    "    Returns:\n",
    "    - final_price (float): Final price of the bundle after applying discounts.\n",
    "    \"\"\"\n",
    "    final_price = 0\n",
    "    \n",
    "    # Iterate through each item in the bundle\n",
    "    for item in bundle:\n",
    "        # Look up the discount for the item in the discount DataFrame\n",
    "        item_discount = discount_df.loc[discount_df['ProductID'] == item, 'Discount'].values\n",
    "        \n",
    "        # If the item is found in the discount DataFrame, apply the discount\n",
    "        if len(item_discount) > 0:\n",
    "            item_discount = item_discount[0]  # Extract the discount value\n",
    "            # Assume original price of the item is 0 if not found in discount DataFrame\n",
    "            original_price = discount_df.loc[discount_df['ProductID'] == item, 'Price'].values[0]\n",
    "            # Apply the discount to the original price of the item\n",
    "            discounted_price = original_price * (1 - item_discount)\n",
    "            # Add the discounted price to the final price\n",
    "            final_price += discounted_price\n",
    "        else:\n",
    "            print(f\"Discount not found for item {item}. Assuming original price.\")\n",
    "\n",
    "    return final_price\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-06T11:04:47.362941500Z",
     "start_time": "2024-03-06T11:04:47.350900600Z"
    }
   },
   "id": "8676f5b4084bcfac",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "source": [
    "# Dummy Discount Data\n",
    "# Define the discount data\n",
    "discount_data = {\n",
    "    'ProductID': ['20725', '20727', '22383', '20728', '85099B', '23209', '23203','23170', '23171', '23172'],\n",
    "    'Price': [5.0, 8.0, 10.0, 7.0, 12.0, 9.0, 6.0,15.0, 20.0, 25.0],\n",
    "    'Discount': [0.1, 0.2, 0.15, 0.1, 0.25, 0.2, 0.15,0.1, 0.2, 0.15]  # Assuming sample discount percentages\n",
    "}\n",
    "\n",
    "# Create the discount DataFrame\n",
    "discount_df = pd.DataFrame(discount_data)\n",
    "discount_df\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-06T11:03:09.475074600Z",
     "start_time": "2024-03-06T11:03:09.427951400Z"
    }
   },
   "id": "a17c4a1af09108a1",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "source": [
    "# Example product bundle\n",
    "bundle = xg[\"Frequent Itemset\"][107]\n",
    "\n",
    "# Calculate final price of the bundle after applying discounts\n",
    "final_price = apply_bundle_discount(bundle, discount_df)\n",
    "print(\"Final price of the bundle after applying discounts:\", final_price)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-06T11:04:56.038043700Z",
     "start_time": "2024-03-06T11:04:56.006608300Z"
    }
   },
   "id": "2f1e4239b93ef4be",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# Get Tiered Discount\n",
    "def calculate_tiered_discount(total_price, tiers):\n",
    "    discount = 0\n",
    "    for tier in tiers:\n",
    "        if total_price >= tier['min_amount']:\n",
    "            discount = tier['discount']\n",
    "        else:\n",
    "            break\n",
    "    return total_price - (total_price * (discount / 100))\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-07T20:23:21.251524800Z",
     "start_time": "2024-03-07T20:23:21.231852200Z"
    }
   },
   "id": "13a1fd8095707a20",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# Example tiers\n",
    "tiers = [\n",
    "    {'min_amount': 0, 'discount': 0},   # Tier 1: $0 - $100 (0% discount)\n",
    "    {'min_amount': 101, 'discount': 5}, # Tier 2: $101 - $200 (5% discount)\n",
    "    {'min_amount': 201, 'discount': 10} # Tier 3: $201 and above (10% discount)\n",
    "]\n",
    "\n",
    "# Example usage\n",
    "total_price = 400\n",
    "discounted_amount = calculate_tiered_discount(total_price, tiers)\n",
    "print(f\"Discounted Amount: ${discounted_amount}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-07T20:23:54.732054600Z",
     "start_time": "2024-03-07T20:23:54.663496500Z"
    }
   },
   "id": "627ef267144f99a5",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# Loyalty Points\n",
    "def calculate_loyalty_points(total_price, points_per_dollar):\n",
    "    return total_price * points_per_dollar"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-07T20:32:22.036020200Z",
     "start_time": "2024-03-07T20:32:21.968485600Z"
    }
   },
   "id": "9cf79202d37d510a",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# Example usage\n",
    "total_price = 150\n",
    "points_per_dollar = 2  # Assume 2 points per dollar spent\n",
    "loyalty_points = calculate_loyalty_points(total_price, points_per_dollar)\n",
    "print(f\"Loyalty Points Earned: {loyalty_points}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-07T20:32:34.728067100Z",
     "start_time": "2024-03-07T20:32:34.647994600Z"
    }
   },
   "id": "42c5fa7cf00a6c0e",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Peak Selection"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4b314029139a98d2"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "def find_promotion_periods(sales_forecast, threshold_increase_pct, threshold_decrease_pct):\n",
    "    promotion_periods = []\n",
    "    current_period = []\n",
    "\n",
    "    for day, sales_volume in sales_forecast.items():\n",
    "        if current_period:\n",
    "            prev_day, prev_sales = current_period[-1]\n",
    "            increase_pct = (sales_volume - prev_sales) / prev_sales * 100\n",
    "            decrease_pct = (prev_sales - sales_volume) / prev_sales * 100\n",
    "            if increase_pct >= threshold_increase_pct or decrease_pct >= threshold_decrease_pct:\n",
    "                current_period.append((day, sales_volume))\n",
    "            else:\n",
    "                promotion_periods.append(current_period)\n",
    "                current_period = [(day, sales_volume)]\n",
    "        else:\n",
    "            current_period.append((day, sales_volume))\n",
    "\n",
    "    # Add the last period if it exists\n",
    "    if current_period:\n",
    "        promotion_periods.append(current_period)\n",
    "\n",
    "    # Merge consecutive periods if the next peak is within 2 days\n",
    "    merged_periods = []\n",
    "    i = 0\n",
    "    while i < len(promotion_periods):\n",
    "        period_start = promotion_periods[i][0][0]\n",
    "        period_end = promotion_periods[i][-1][0]\n",
    "        merged = False\n",
    "        for j in range(i + 1, min(i + 3, len(promotion_periods))):\n",
    "            next_period_start = promotion_periods[j][0][0]\n",
    "            if next_period_start - period_end <= 2:\n",
    "                period_end = promotion_periods[j][-1][0]\n",
    "                merged = True\n",
    "            else:\n",
    "                break\n",
    "        merged_periods.append((period_start, period_end))\n",
    "        if merged:\n",
    "            i = j + 1\n",
    "        else:\n",
    "            i += 1\n",
    "\n",
    "    # Separate peak and lull periods\n",
    "    peak_periods = []\n",
    "    lull_periods = []\n",
    "    for period in merged_periods:\n",
    "        period_start, period_end = period\n",
    "        period_sales = [sales_forecast[day] for day in range(period_start, period_end + 1)]\n",
    "        if all(sales > sales_forecast[period_start] for sales in period_sales):\n",
    "            peak_periods.append(period)\n",
    "        elif all(sales < sales_forecast[period_start] for sales in period_sales):\n",
    "            lull_periods.append(period)\n",
    "\n",
    "    # Ensure there are at least 2 lull periods and 1 peak period\n",
    "    while len(lull_periods) < 2:\n",
    "        if peak_periods:\n",
    "            lull_periods.append(peak_periods.pop(0))\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # Return both peak and lull periods\n",
    "    return lull_periods, peak_periods\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-07T20:49:36.035377100Z",
     "start_time": "2024-03-07T20:49:35.971433500Z"
    }
   },
   "id": "d1607fc2dd5e7441",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "# Example usage\n",
    "sales_forecast = {1: 100, 2: 120, 3: 130, 4: 150, 5: 160, 6: 170, 7: 175, 8: 180, 9: 175, 10: 170}\n",
    "threshold_increase_pct = 0  # Define threshold percentage increase\n",
    "threshold_decrease_pct = -10  # Define threshold percentage decrease\n",
    "\n",
    "lull_periods, peak_periods = find_promotion_periods(sales_forecast, threshold_increase_pct, threshold_decrease_pct)\n",
    "print(\"Lull Periods:\", lull_periods)\n",
    "print(\"Peak Periods:\", peak_periods)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-07T20:50:44.880987700Z",
     "start_time": "2024-03-07T20:50:44.864819500Z"
    }
   },
   "id": "742024d666b57bb7",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "import pandas as pd\n",
    "\n",
    "def find_promo_days(sales_data, peak_threshold=1.2, lull_threshold=0.8, num_promos=3):\n",
    "    \"\"\"Identifies peak and lull promotion days based on percentage thresholds of average sales volume.\n",
    "\n",
    "    Args:\n",
    "        sales_data (pd.DataFrame): DataFrame with at least 'Date' and 'Sales Volume' columns.\n",
    "        peak_threshold (float, optional): Multiplier for avg. sales volume to define a peak. Defaults to 1.2 (20% above average).\n",
    "        lull_threshold (float, optional): Multiplier for avg. sales volume to define a lull. Defaults to 0.8 (20% below average).\n",
    "        num_promos (int, optional): The maximum number of promotions per month. Defaults to 3.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing two lists:\n",
    "            - peak_days: A list of the top 'num_promos' sales peaks (dates).\n",
    "            - tull_days: A list of the top 'num_promos' sales tulls (dates). \n",
    "    \"\"\"\n",
    "\n",
    "    if not isinstance(sales_data, pd.DataFrame):\n",
    "        raise TypeError(\"sales_data must be a pandas DataFrame\")\n",
    "\n",
    "    # Calculate average sales volume\n",
    "    avg_sales = sales_data['Sales Volume'].mean()\n",
    "\n",
    "    # Apply thresholds\n",
    "    peak_condition = sales_data['Sales Volume'] >= avg_sales * peak_threshold\n",
    "    lull_condition = sales_data['Sales Volume'] <= avg_sales * lull_threshold\n",
    "\n",
    "    # Get top peaks and tulls\n",
    "    peak_days = sales_data[peak_condition]['Date'].iloc[:num_promos].tolist()\n",
    "    tull_days = sales_data[lull_condition]['Date'].iloc[:num_promos].tolist()\n",
    "\n",
    "    return peak_days, tull_days \n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-07T20:57:26.950088300Z",
     "start_time": "2024-03-07T20:57:26.886968800Z"
    }
   },
   "id": "dcbe0fc7026f3ca0",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# 1. Create date range (let's assume a month)\n",
    "dates = pd.date_range(start='2024-03-01', end='2024-03-31')\n",
    "\n",
    "# 2. Generate some base sales data with fluctuations\n",
    "base_sales = np.random.randint(50, 150, size=len(dates))\n",
    "\n",
    "# 3. Add some random peaks and dips \n",
    "for i in random.sample(range(len(dates)), k=5):  # Introduce 5 random spikes\n",
    "    base_sales[i] *= random.uniform(1.5, 2)  \n",
    "for i in random.sample(range(len(dates)), k=5):  # Introduce 5 random dips\n",
    "    base_sales[i] *= random.uniform(0.5, 0.8)  \n",
    "\n",
    "# 4. Create DataFrame\n",
    "sales_data = pd.DataFrame({'Date': dates, 'Sales Volume': base_sales})\n",
    "\n",
    "# 5. Test the function\n",
    "peak_days, tull_days = find_promo_days(sales_data)\n",
    "print(\"Peak Promotion Days:\", peak_days)\n",
    "print(\"Tull Promotion Days:\", tull_days)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-07T20:58:09.347703600Z",
     "start_time": "2024-03-07T20:58:09.088364200Z"
    }
   },
   "id": "12408cfe9cd13b2b",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "sales_data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-07T20:58:52.420043100Z",
     "start_time": "2024-03-07T20:58:52.356602800Z"
    }
   },
   "id": "284252d0d70b6042",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def find_promo_days(sales_data, peak_threshold=1.2, lull_threshold=0.8, num_promos=3, proximity_days=3):\n",
    "    \"\"\"Identifies peak and lull promotional periods based on percentage thresholds of average sales volume, \n",
    "    considering proximity to group close dates into extended periods.\n",
    "\n",
    "    Args:\n",
    "        sales_data (pd.DataFrame): DataFrame containing 'Date' and 'Sales Volume' columns.\n",
    "        peak_threshold (float, optional): Multiplier for avg. sales volume to define a peak. Defaults to 1.2 (20% above average).\n",
    "        lull_threshold (float, optional): Multiplier for avg. sales volume to define a lull. Defaults to 0.8 (20% below average).\n",
    "        num_promos (int, optional): The maximum number of promotions per month (ignored in this implementation). Defaults to 3.\n",
    "        proximity_days (int, optional): The maximum number of days between dates to consider them part of the same promotional period. Defaults to 3.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of lists, where each sub-list represents a promotional period (containing 'Date' objects).\n",
    "    \"\"\"\n",
    "\n",
    "    if not isinstance(sales_data, pd.DataFrame):\n",
    "        raise TypeError(\"sales_data must be a pandas DataFrame\")\n",
    "\n",
    "    # Calculate average sales volume\n",
    "    avg_sales = sales_data['Sales Volume'].mean()\n",
    "\n",
    "    # Apply thresholds to identify days within peak or lull zones\n",
    "    peak_condition = sales_data['Sales Volume'] >= avg_sales * peak_threshold\n",
    "    lull_condition = sales_data['Sales Volume'] <= avg_sales * lull_threshold\n",
    "\n",
    "    # Get top peaks and tulls (as dates) based on the conditions\n",
    "    peak_days = sales_data[peak_condition]['Date'].tolist()\n",
    "    tull_days = sales_data[lull_condition]['Date'].tolist()\n",
    "\n",
    "    # Process peak days for proximity and group into promotional periods\n",
    "    promo_periods = cluster_close_dates(peak_days, proximity_days) \n",
    "\n",
    "    # Process tull days for proximity and group into promotional periods\n",
    "    promo_periods.extend(cluster_close_dates(tull_days, proximity_days))\n",
    "\n",
    "    return promo_periods\n",
    "\n",
    "def cluster_close_dates(dates, proximity_days):\n",
    "    \"\"\"Groups closely spaced dates into promotional periods.\n",
    "\n",
    "    Args:\n",
    "        dates (list): A list of 'Date' objects.\n",
    "        proximity_days (int): The maximum number of days between dates to consider them part of the same promotional period.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of lists, where each sub-list represents a promotional period (containing 'Date' objects).\n",
    "    \"\"\"\n",
    "\n",
    "    promo_periods = []  # List to store promotional periods (groups of dates)\n",
    "    current_period = []  # Temporary list to store dates in a potential period\n",
    "\n",
    "    for date in dates:\n",
    "        # Check if a new period needs to be started (current period with a gap of more than 'proximity_days')\n",
    "        if current_period and (date - current_period[-1]).days > proximity_days:\n",
    "            promo_periods.append(current_period)  # Add the previous period to results\n",
    "            current_period = []  # Reset the current period list\n",
    "\n",
    "        current_period.append(date)  # Add the current date to the period list\n",
    "\n",
    "    if current_period:  # Add the last remaining period, if any\n",
    "        promo_periods.append(current_period)\n",
    "\n",
    "    return promo_periods\n",
    "\n",
    "# (Optional) Dummy data generation for testing purposes\n",
    "import datetime\n",
    "\n",
    "def generate_dummy_data(start_date='2024-03-01', end_date='2024-03-31'):\n",
    "  \"\"\"Generates dummy sales data with random fluctuations, peaks, and dips.\"\"\"\n",
    "  dates = pd.date_range(start=start_date, end=end_date)\n",
    "  base_sales = np.random.randint(50, 150, size=len(dates))\n",
    "  for i in random.sample(range(len(dates)), k=5):\n",
    "      base_sales[i] *= random.uniform(1.5, 2)  # Introduce random peaks\n",
    "  for i in random.sample(range(len(dates)), k=5):\n",
    "      base_sales[i] *= random.uniform(0.5, 0.8)  # Introduce random dips\n",
    "  return pd\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-07T21:02:40.915793800Z",
     "start_time": "2024-03-07T21:02:40.776138Z"
    }
   },
   "id": "f5f21dee53f8955",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "def generate_dummy_data(start_date='2024-03-01', end_date='2024-03-31'):\n",
    "    dates = pd.date_range(start=start_date, end=end_date)\n",
    "    base_sales = np.random.randint(50, 150, size=len(dates))\n",
    "\n",
    "    # Introduce clustered peaks and valleys\n",
    "    peak_clusters = [[2, 3, 5], [10, 11], [20, 22, 23]] \n",
    "    lull_clusters = [[8, 9], [16, 18], [27, 28, 29]]\n",
    "\n",
    "    for cluster in peak_clusters:\n",
    "        for i in cluster:\n",
    "            base_sales[i] *= random.uniform(1.5, 2)\n",
    "\n",
    "    for cluster in lull_clusters:\n",
    "        for i in cluster:\n",
    "            base_sales[i] *= random.uniform(0.5, 0.8)\n",
    "\n",
    "    return pd.DataFrame({'Date': dates, 'Sales Volume': base_sales})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-07T21:18:54.476740100Z",
     "start_time": "2024-03-07T21:18:54.439399400Z"
    }
   },
   "id": "35dc0b9df8509469",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "# Generate test data\n",
    "sales_data = generate_dummy_data()\n",
    "\n",
    "# Find promotional periods with a proximity of 2 days\n",
    "promo_periods = find_promo_days(sales_data, proximity_days=2) \n",
    "\n",
    "# Print results\n",
    "for period in promo_periods:\n",
    "    start_date = period[0].strftime('%Y-%m-%d')\n",
    "    end_date = period[-1].strftime('%Y-%m-%d')\n",
    "    print(f\"Promotional Period: {start_date} to {end_date}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-07T21:23:08.158109700Z",
     "start_time": "2024-03-07T21:23:08.016581700Z"
    }
   },
   "id": "e6e79317adeb93cf",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "sales_data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-07T21:24:15.890374700Z",
     "start_time": "2024-03-07T21:24:15.740117200Z"
    }
   },
   "id": "51dd107998e52126",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(sales_data[\"Date\"],sales_data[\"Sales Volume\"]);"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-07T21:26:46.398985900Z",
     "start_time": "2024-03-07T21:26:45.116586500Z"
    }
   },
   "id": "e0950bf1f28df6e7",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "6c4a26ee7eed5c50",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

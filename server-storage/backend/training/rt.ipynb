{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-24T07:25:47.818877Z",
     "start_time": "2024-07-24T07:25:35.446157Z"
    }
   },
   "source": [
    "#Import the neccesary libraries\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gr\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tabulate import tabulate"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T08:20:10.031597Z",
     "start_time": "2024-07-24T08:20:09.876738Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pd.read_csv(\"processed.csv\", parse_dates=['date'], index_col='date')\n",
    "df =df.asfreq('D')\n",
    "df =df[[\"quantity\"]]\n",
    "df.head()"
   ],
   "id": "aeb27527e71d59a7",
   "execution_count": 14,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T08:20:13.021669Z",
     "start_time": "2024-07-24T08:20:12.991302Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = df.copy()\n",
    "data[\"unique_id\"]=1.0\n",
    "data[\"ds\"] = data.index\n",
    "data.rename(columns={\"quantity\":\"y\"},inplace=True)\n",
    "data.head()"
   ],
   "id": "1c9e6d9f9eee9948",
   "execution_count": 15,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T08:20:17.632660Z",
     "start_time": "2024-07-24T08:20:17.618020Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Using Nxitla libraries\n",
    "#Data Split\n",
    "# Calculate the index for the split\n",
    "split_index = int(0.8 * len(data))\n",
    "\n",
    "# Split the data\n",
    "Y_train_df = data.iloc[:split_index]\n",
    "Y_test_df = data.iloc[split_index:]   # Test data for January 2012\n",
    "\n",
    "horizon = len(Y_test_df)"
   ],
   "id": "637d781538270449",
   "execution_count": 16,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# SARIMAX",
   "id": "440947762f6114a8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T07:27:30.398317Z",
     "start_time": "2024-07-24T07:27:05.427Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, r2_score\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming df is your DataFrame with the date as the index and 'quantity' as the column\n",
    "\n",
    "# Split data into train and test sets\n",
    "train_size = int(len(df) * 0.9)\n",
    "train, test = df.iloc[:train_size], df.iloc[train_size:]\n",
    "\n",
    "# Define and fit the SARIMAX model\n",
    "model = SARIMAX(train['quantity'], order=(1, 1, 1), seasonal_order=(1, 1, 1, 12))\n",
    "fit_model = model.fit(disp=False)\n",
    "\n",
    "# Forecast\n",
    "n_forecast = len(test)\n",
    "forecast = fit_model.get_forecast(steps=n_forecast)\n",
    "forecast_index = test.index\n",
    "forecast_values = forecast.predicted_mean\n",
    "\n",
    "# Calculate MAE and MAPE\n",
    "mae = mean_absolute_error(test['quantity'], forecast_values)\n",
    "mape = mean_absolute_percentage_error(test['quantity'], forecast_values)\n",
    "\n",
    "# Calculate sMAPE\n",
    "def smape(a, f):\n",
    "    return 100 * np.mean(2 * np.abs(f - a) / (np.abs(a) + np.abs(f)))\n",
    "\n",
    "smape_value = smape(test['quantity'].values, forecast_values.values)\n",
    "\n",
    "# Calculate R²\n",
    "r2 = r2_score(test['quantity'], forecast_values)\n",
    "\n",
    "# Print the results\n",
    "print(f'MAE: {mae}')\n",
    "print(f'MAPE: {mape}')\n",
    "print(f'sMAPE: {smape_value}')\n",
    "print(f'R²: {r2}')\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(train.index, train['quantity'], label='Train')\n",
    "plt.plot(test.index, test['quantity'], label='Test', color='green')\n",
    "plt.plot(forecast_index, forecast_values, label='Forecast', color='red')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Quantity')\n",
    "plt.title('SARIMAX Forecast')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ],
   "id": "920097c0de84c916",
   "execution_count": 5,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T08:21:22.626937Z",
     "start_time": "2024-07-24T08:21:22.617919Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ml_data = data.copy()\n",
    "train_df = ml_data [:-30]\n",
    "test_df = ml_data [-30:]"
   ],
   "id": "d7dbd4d2b18528ea",
   "execution_count": 17,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T07:37:38.807730Z",
     "start_time": "2024-07-24T07:33:57.121180Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsforecast.models import AutoARIMA, AutoETS, AutoCES, AutoTheta, SimpleExponentialSmoothingOptimized\n",
    "from sklearn.metrics import mean_absolute_percentage_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have defined train_df, test_df, and ml_data somewhere in your code\n",
    "\n",
    "# Extract the 'y' series from train and test dataframes\n",
    "train_series = train_df['y'].values\n",
    "test_series = test_df['y'].values\n",
    "\n",
    "# Define the models\n",
    "models = {\n",
    "    'AutoARIMA': AutoARIMA(season_length=12),\n",
    "    'AutoETS': AutoETS(model='ZZZ', season_length=12),\n",
    "    'AutoCES': AutoCES(model='Z', season_length=12),\n",
    "    'AutoTheta': AutoTheta(season_length=12),\n",
    "    'SESOpt': SimpleExponentialSmoothingOptimized()\n",
    "}\n",
    "\n",
    "# Initialize lists to store results\n",
    "mape_scores = []\n",
    "mae_scores = []\n",
    "smape_scores = []\n",
    "r2_scores = []\n",
    "forecasts_dict = {}\n",
    "\n",
    "def smape(a, f):\n",
    "    return 100 * np.mean(2 * np.abs(f - a) / (np.abs(a) + np.abs(f)))\n",
    "\n",
    "# Train and evaluate each model\n",
    "for name, model in models.items():\n",
    "    model = model.fit(y=train_series)\n",
    "    forecasts = model.predict(h=len(test_series))\n",
    "    forecasts_dict[name] = forecasts['mean']\n",
    "    \n",
    "    # Calculate MAE, MAPE, sMAPE, and R²\n",
    "    mae = mean_absolute_error(test_series, forecasts['mean'])\n",
    "    mape = mean_absolute_percentage_error(test_series, forecasts['mean'])\n",
    "    smape_value = smape(test_series, forecasts['mean'])\n",
    "    r2 = r2_score(test_series, forecasts['mean'])\n",
    "    \n",
    "    mae_scores.append((name, mae))\n",
    "    mape_scores.append((name, mape))\n",
    "    smape_scores.append((name, smape_value))\n",
    "    r2_scores.append((name, r2))\n",
    "    \n",
    "    print(f\"{name} - MAE: {mae:.4f}, MAPE: {mape:.4%}, sMAPE: {smape_value:.4%}, R²: {r2:.4f}\")\n",
    "\n",
    "# Select the best model based on MAPE\n",
    "best_model_name, best_mape = min(mape_scores, key=lambda x: x[1])\n",
    "best_model = models[best_model_name]\n",
    "\n",
    "print(f\"\\nThe best model is {best_model_name} with a MAPE of {best_mape:.4%}\")\n",
    "\n",
    "# Train the best model on the entire dataset\n",
    "best_model = best_model.fit(y=ml_data['y'].values)\n",
    "\n",
    "# Predict the next 60 days\n",
    "future_forecasts = best_model.predict(h=60)\n",
    "\n",
    "# Combine the historical data with the forecast data for plotting\n",
    "combined_df = ml_data.copy()\n",
    "future_df = pd.DataFrame({\n",
    "    'ds': pd.date_range(start=ml_data['ds'].max() + pd.Timedelta(days=1), periods=60, freq='D'),\n",
    "    'y': np.nan\n",
    "})\n",
    "combined_df = pd.concat([combined_df, future_df], ignore_index=True)\n",
    "combined_df['forecast'] = np.nan\n",
    "combined_df.loc[ml_data.shape[0]:, 'forecast'] = future_forecasts['mean']\n",
    "\n",
    "# Plot actual vs predicted values for the historical period and future forecast\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Plot the entire dataset\n",
    "plt.plot(combined_df['ds'], combined_df['y'], label='Actual', marker='o')\n",
    "\n",
    "# Plot the future forecasts\n",
    "plt.plot(\n",
    "    combined_df['ds'],\n",
    "    combined_df['forecast'],\n",
    "    label=f'Predicted - {best_model_name} (Next 60 Days)',\n",
    "    marker='x',\n",
    ")\n",
    "\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.title(f'Actual vs Predicted Values using {best_model_name}')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nThe best model is {best_model_name} with a MAPE of {best_mape:.4%}\")\n"
   ],
   "id": "f0589504788a1b3f",
   "execution_count": 7,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Machine learning methods",
   "id": "bfcdbf191c5d4e1c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T07:38:16.319927Z",
     "start_time": "2024-07-24T07:37:51.063442Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_absolute_percentage_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import holidays\n",
    "\n",
    "# Function to create date features\n",
    "def create_date_features(data):\n",
    "    data[\"month\"] = data.index.month\n",
    "    data[\"day_of_month\"] = data.index.day\n",
    "    data[\"is_month_start\"] = data.index.is_month_start.astype(int)\n",
    "    data[\"is_month_end\"] = data.index.is_month_end.astype(int)\n",
    "    data[\"day_of_year\"] = data.index.dayofyear\n",
    "    data[\"week_of_year\"] = data.index.isocalendar().week\n",
    "    data[\"day_of_week\"] = data.index.dayofweek + 1\n",
    "    data[\"year\"] = data.index.year\n",
    "    data[\"is_weekend\"] = (data.index.weekday >= 5).astype(int)\n",
    "    data['is_spring'] = data['month'].isin([3, 4, 5]).astype(int)\n",
    "    data['is_summer'] = data['month'].isin([6, 7, 8]).astype(int)\n",
    "    data['is_fall'] = data['month'].isin([9, 10, 11]).astype(int)\n",
    "    data['is_winter'] = data['month'].isin([12, 1, 2]).astype(int)\n",
    "    data['sin_day'] = np.sin(2 * np.pi * data.index.dayofweek / 7)\n",
    "    data['cos_day'] = np.cos(2 * np.pi * data.index.dayofweek / 7)\n",
    "    return data\n",
    "\n",
    "# Add holiday feature\n",
    "uk_holidays = holidays.CountryHoliday('UK')\n",
    "\n",
    "# Create date features\n",
    "rml_data = create_date_features(data.copy())\n",
    "rml_data['is_public_holiday'] = rml_data.index.to_series().apply(lambda date: 1 if date in uk_holidays else 0)\n",
    "\n",
    "# Define Features and Models\n",
    "models = {\n",
    "    'RandomForest': RandomForestRegressor(n_estimators=100),\n",
    "    'XGBoost': XGBRegressor(objective='reg:squarederror', n_estimators=100),\n",
    "    'LightGBM': lgb.LGBMRegressor(n_estimators=100)\n",
    "}\n",
    "\n",
    "# Specify the features to be used in the model\n",
    "date_features = ['month', 'day_of_month', 'is_month_start', 'is_month_end',\n",
    "                 'day_of_year', 'week_of_year', 'day_of_week', 'year',\n",
    "                 'is_weekend', 'is_spring', 'is_summer', 'is_fall', 'is_winter',\n",
    "                 'sin_day', 'cos_day', 'is_public_holiday']\n",
    "\n",
    "# Prepare training data\n",
    "def create_lagged_features(data, lags):\n",
    "    for lag in lags:\n",
    "        data[f'lag_{lag}'] = data['y'].shift(lag)\n",
    "    return data\n",
    "\n",
    "lags = [7, 14, 30]\n",
    "rml_data = create_lagged_features(rml_data, lags).dropna()\n",
    "\n",
    "X = rml_data[date_features + [f'lag_{lag}' for lag in lags]]\n",
    "y = rml_data['y']\n",
    "\n",
    "# Split data into train and test sets\n",
    "train_X = X[:-30]\n",
    "train_y = y[:-30]\n",
    "test_X = X[-30:]\n",
    "test_y = y[-30:]\n",
    "\n",
    "# Initialize lists to store results\n",
    "mape_scores = {}\n",
    "mae_scores = {}\n",
    "smape_scores = {}\n",
    "r2_scores = {}\n",
    "predictions = {}\n",
    "\n",
    "def smape(a, f):\n",
    "    return 100 * np.mean(2 * np.abs(f - a) / (np.abs(a) + np.abs(f)))\n",
    "\n",
    "# Train and evaluate each model\n",
    "for model_name, model in models.items():\n",
    "    model.fit(train_X, train_y)\n",
    "    predictions[model_name] = model.predict(test_X)\n",
    "    \n",
    "    # Calculate MAE, MAPE, sMAPE, and R²\n",
    "    mae = mean_absolute_error(test_y, predictions[model_name])\n",
    "    mape = mean_absolute_percentage_error(test_y, predictions[model_name])\n",
    "    smape_value = smape(test_y, predictions[model_name])\n",
    "    r2 = r2_score(test_y, predictions[model_name])\n",
    "    \n",
    "    mae_scores[model_name] = mae\n",
    "    mape_scores[model_name] = mape\n",
    "    smape_scores[model_name] = smape_value\n",
    "    r2_scores[model_name] = r2\n",
    "    \n",
    "    print(f\"{model_name} - MAE: {mae:.4f}, MAPE: {mape:.4%}, sMAPE: {smape_value:.4%}, R²: {r2:.4f}\")\n",
    "\n",
    "# Select the best model based on MAPE\n",
    "best_model_name = min(mape_scores, key=mape_scores.get)\n",
    "print(f\"\\nThe best model is {best_model_name} with a MAPE of {mape_scores[best_model_name]:.4%}\")\n",
    "\n",
    "# Retrain the best model on the entire dataset\n",
    "best_model = models[best_model_name]\n",
    "best_model.fit(X, y)\n",
    "\n",
    "# Predict the next 30 days\n",
    "future_X = X[-30:]  # Assuming that you want to predict the next 30 days using the same features\n",
    "future_predictions = best_model.predict(future_X)\n",
    "\n",
    "# Plot actual vs predicted values\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(test_y.index, test_y, label='Actual', marker='o')\n",
    "plt.plot(test_y.index, predictions[best_model_name], label=f'Predicted - {best_model_name}', marker='x')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Actual vs Predicted Values')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ],
   "id": "10f1f53221e14c44",
   "execution_count": 8,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T08:08:40.914545Z",
     "start_time": "2024-07-24T07:41:33.639093Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_percentage_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import holidays\n",
    "\n",
    "# Function to create date features\n",
    "def create_date_features(data):\n",
    "    data[\"month\"] = data.index.month\n",
    "    data[\"day_of_month\"] = data.index.day\n",
    "    data[\"is_month_start\"] = data.index.is_month_start.astype(int)\n",
    "    data[\"is_month_end\"] = data.index.is_month_end.astype(int)\n",
    "    data[\"day_of_year\"] = data.index.dayofyear\n",
    "    data[\"week_of_year\"] = data.index.isocalendar().week\n",
    "    data[\"day_of_week\"] = data.index.dayofweek + 1\n",
    "    data[\"year\"] = data.index.year\n",
    "    data[\"is_weekend\"] = (data.index.weekday >= 5).astype(int)\n",
    "    data['is_spring'] = data['month'].isin([3, 4, 5]).astype(int)\n",
    "    data['is_summer'] = data['month'].isin([6, 7, 8]).astype(int)\n",
    "    data['is_fall'] = data['month'].isin([9, 10, 11]).astype(int)\n",
    "    data['is_winter'] = data['month'].isin([12, 1, 2]).astype(int)\n",
    "    data['sin_day'] = np.sin(2 * np.pi * data.index.dayofweek / 7)\n",
    "    data['cos_day'] = np.cos(2 * np.pi * data.index.dayofweek / 7)\n",
    "    return data\n",
    "\n",
    "# Add holiday feature\n",
    "uk_holidays = holidays.CountryHoliday('UK')\n",
    "\n",
    "# Create date features\n",
    "rml_data = create_date_features(data.copy())\n",
    "rml_data['is_public_holiday'] = rml_data.index.to_series().apply(lambda date: 1 if date in uk_holidays else 0)\n",
    "\n",
    "# Define Features and Models\n",
    "date_features = ['month', 'day_of_month', 'is_month_start', 'is_month_end',\n",
    "                 'day_of_year', 'week_of_year', 'day_of_week', 'year',\n",
    "                 'is_weekend', 'is_spring', 'is_summer', 'is_fall', 'is_winter',\n",
    "                 'sin_day', 'cos_day', 'is_public_holiday']\n",
    "\n",
    "# Prepare training data\n",
    "def create_lagged_features(data, lags):\n",
    "    for lag in lags:\n",
    "        data[f'lag_{lag}'] = data['y'].shift(lag)\n",
    "    return data\n",
    "\n",
    "lags = [7, 14, 30]\n",
    "rml_data = create_lagged_features(rml_data, lags).dropna()\n",
    "\n",
    "X = rml_data[date_features + [f'lag_{lag}' for lag in lags]]\n",
    "y = rml_data['y']\n",
    "\n",
    "# Split data into train and test sets\n",
    "train_X = X[:-30]\n",
    "train_y = y[:-30]\n",
    "test_X = X[-30:]\n",
    "test_y = y[-30:]\n",
    "\n",
    "# Define parameter grids for each model\n",
    "param_grids = {\n",
    "    'RandomForest': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10]\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [3, 6, 9],\n",
    "        'learning_rate': [0.01, 0.1, 0.2]\n",
    "    },\n",
    "    'LightGBM': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'num_leaves': [31, 40, 50],\n",
    "        'learning_rate': [0.01, 0.1, 0.2]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initialize lists to store results\n",
    "mae_scores = {}\n",
    "mape_scores = {}\n",
    "smape_scores = {}\n",
    "r2_scores = {}\n",
    "predictions = {}\n",
    "\n",
    "def smape(a, f):\n",
    "    return 100 * np.mean(2 * np.abs(f - a) / (np.abs(a) + np.abs(f)))\n",
    "\n",
    "# Train and predict using each model with grid search\n",
    "for model_name, model in {'RandomForest': RandomForestRegressor(), \n",
    "                          'XGBoost': XGBRegressor(objective='reg:squarederror'), \n",
    "                          'LightGBM': lgb.LGBMRegressor()}.items():\n",
    "    grid_search = GridSearchCV(model, param_grids[model_name], cv=3, scoring='neg_mean_absolute_percentage_error')\n",
    "    grid_search.fit(train_X, train_y)\n",
    "    best_params = grid_search.best_params_\n",
    "    predictions[model_name] = grid_search.predict(test_X)\n",
    "    \n",
    "    # Calculate MAE, MAPE, sMAPE, and R²\n",
    "    mae = mean_absolute_error(test_y, predictions[model_name])\n",
    "    mape = mean_absolute_percentage_error(test_y, predictions[model_name])\n",
    "    smape_value = smape(test_y, predictions[model_name])\n",
    "    r2 = r2_score(test_y, predictions[model_name])\n",
    "    \n",
    "    mae_scores[model_name] = mae\n",
    "    mape_scores[model_name] = mape\n",
    "    smape_scores[model_name] = smape_value\n",
    "    r2_scores[model_name] = r2\n",
    "    \n",
    "    print(f\"{model_name} - Best Params: {best_params}, MAE: {mae:.4f}, MAPE: {mape:.4%}, sMAPE: {smape_value:.4%}, R²: {r2:.4f}\")\n",
    "\n",
    "# Select the Best Model based on MAPE\n",
    "best_model_name = min(mape_scores, key=mape_scores.get)\n",
    "print(f\"\\nThe best model is {best_model_name} with a MAPE of {mape_scores[best_model_name]:.4%}\")\n",
    "\n",
    "# Retrain the best model on the entire dataset with best parameters\n",
    "best_model = {'RandomForest': RandomForestRegressor(**param_grids['RandomForest']),\n",
    "              'XGBoost': XGBRegressor(objective='reg:squarederror', **param_grids['XGBoost']),\n",
    "              'LightGBM': lgb.LGBMRegressor(**param_grids['LightGBM'])}[best_model_name]\n",
    "best_model.fit(X, y)\n",
    "\n",
    "# Predict the next 30 days\n",
    "future_X = X[-30:]  # Assuming that you want to predict the next 30 days using the same features\n",
    "future_predictions = best_model.predict(future_X)\n",
    "\n",
    "# Plot actual vs predicted values\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(test_y.index, test_y, label='Actual', marker='o')\n",
    "plt.plot(test_y.index, predictions[best_model_name], label=f'Predicted - {best_model_name}', marker='x')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Actual vs Predicted Values')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ],
   "id": "fa344882d1d3fc0f",
   "execution_count": 9,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T09:17:59.572711Z",
     "start_time": "2024-07-24T09:17:46.585450Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_absolute_percentage_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import holidays\n",
    "\n",
    "# Function to create date features\n",
    "def create_date_features(data):\n",
    "    data[\"month\"] = data.index.month\n",
    "    data[\"day_of_month\"] = data.index.day\n",
    "    data[\"is_month_start\"] = data.index.is_month_start.astype(int)\n",
    "    data[\"is_month_end\"] = data.index.is_month_end.astype(int)\n",
    "    data[\"day_of_year\"] = data.index.dayofyear\n",
    "    data[\"week_of_year\"] = data.index.isocalendar().week\n",
    "    data[\"day_of_week\"] = data.index.dayofweek + 1\n",
    "    data[\"year\"] = data.index.year\n",
    "    data[\"is_weekend\"] = (data.index.weekday >= 5).astype(int)\n",
    "    data['is_spring'] = data['month'].isin([3, 4, 5]).astype(int)\n",
    "    data['is_summer'] = data['month'].isin([6, 7, 8]).astype(int)\n",
    "    data['is_fall'] = data['month'].isin([9, 10, 11]).astype(int)\n",
    "    data['is_winter'] = data['month'].isin([12, 1, 2]).astype(int)\n",
    "    data['sin_day'] = np.sin(2 * np.pi * data.index.dayofweek / 7)\n",
    "    data['cos_day'] = np.cos(2 * np.pi * data.index.dayofweek / 7)\n",
    "    return data\n",
    "\n",
    "# Add holiday feature\n",
    "uk_holidays = holidays.CountryHoliday('UK')\n",
    "\n",
    "# Create date features\n",
    "rml_data = create_date_features(data.copy())\n",
    "rml_data['is_public_holiday'] = rml_data.index.to_series().apply(lambda date: 1 if date in uk_holidays else 0)\n",
    "\n",
    "# Define Features and Models\n",
    "date_features = ['month', 'day_of_month', 'is_month_start', 'is_month_end',\n",
    "                 'day_of_year', 'week_of_year', 'day_of_week', 'year',\n",
    "                 'is_weekend', 'is_spring', 'is_summer', 'is_fall', 'is_winter',\n",
    "                 'sin_day', 'cos_day', 'is_public_holiday']\n",
    "\n",
    "# Prepare training data\n",
    "def create_lagged_features(data, lags):\n",
    "    for lag in lags:\n",
    "        data[f'lag_{lag}'] = data['y'].shift(lag)\n",
    "    return data\n",
    "\n",
    "lags = [7, 14, 30]\n",
    "rml_data = create_lagged_features(rml_data, lags).dropna()\n",
    "\n",
    "X = rml_data[date_features + [f'lag_{lag}' for lag in lags]]\n",
    "y = rml_data['y']\n",
    "\n",
    "# Split data into train and test sets\n",
    "train_X = X[:-30]\n",
    "train_y = y[:-30]\n",
    "test_X = X[-30:]\n",
    "test_y = y[-30:]\n",
    "\n",
    "# Initialize models with default parameters\n",
    "models = {\n",
    "    'RandomForest': RandomForestRegressor(),\n",
    "    'XGBoost': XGBRegressor(objective='reg:squarederror'),\n",
    "    'LightGBM': lgb.LGBMRegressor()\n",
    "}\n",
    "\n",
    "# Initialize lists to store results\n",
    "mae_scores = {}\n",
    "mape_scores = {}\n",
    "smape_scores = {}\n",
    "r2_scores = {}\n",
    "predictions = {}\n",
    "\n",
    "def smape(a, f):\n",
    "    return 100 * np.mean(2 * np.abs(f - a) / (np.abs(a) + np.abs(f)))\n",
    "\n",
    "# Train and predict using each model without grid search\n",
    "for model_name, model in models.items():\n",
    "    model.fit(train_X, train_y)\n",
    "    predictions[model_name] = model.predict(test_X)\n",
    "    \n",
    "    # Calculate MAE, MAPE, sMAPE, and R²\n",
    "    mae = mean_absolute_error(test_y, predictions[model_name])\n",
    "    mape = mean_absolute_percentage_error(test_y, predictions[model_name])\n",
    "    smape_value = smape(test_y, predictions[model_name])\n",
    "    r2 = r2_score(test_y, predictions[model_name])\n",
    "    \n",
    "    mae_scores[model_name] = mae\n",
    "    mape_scores[model_name] = mape\n",
    "    smape_scores[model_name] = smape_value\n",
    "    r2_scores[model_name] = r2\n",
    "    \n",
    "    print(f\"{model_name} - MAE: {mae:.4f}, MAPE: {mape:.4%}, sMAPE: {smape_value:.4%}, R²: {r2:.4f}\")\n",
    "\n",
    "# Select the Best Model based on MAPE\n",
    "best_model_name = min(mape_scores, key=mape_scores.get)\n",
    "print(f\"\\nThe best model is {best_model_name} with a MAPE of {mape_scores[best_model_name]:.4%}\")\n",
    "\n",
    "# Retrain the best model on the entire dataset with default parameters\n",
    "best_model = models[best_model_name]\n",
    "best_model.fit(X, y)\n",
    "\n",
    "# Predict the next 60 days\n",
    "future_dates = pd.date_range(start=X.index[-1] + pd.Timedelta(days=1), periods=60, freq='D')\n",
    "future_data = pd.DataFrame(index=future_dates)\n",
    "future_data = create_date_features(future_data)\n",
    "future_data['is_public_holiday'] = future_data.index.to_series().apply(lambda date: 1 if date in uk_holidays else 0)\n",
    "\n",
    "# Initialize the lagged features with previous values\n",
    "for lag in lags:\n",
    "    future_data[f'lag_{lag}'] = np.nan\n",
    "\n",
    "# Iterative prediction for the next 60 days\n",
    "for i, date in enumerate(future_dates):\n",
    "    if i == 0:\n",
    "        lagged_features = [np.nan] * len(lags)  # No lagged values for the first date\n",
    "    else:\n",
    "        lagged_features = future_data.iloc[i-1][[f'lag_{lag}' for lag in lags]].values\n",
    "    \n",
    "    # Create the feature vector for the current date\n",
    "    current_features = future_data.loc[date, date_features + [f'lag_{lag}' for lag in lags]].values.reshape(1, -1)\n",
    "    future_data.at[date, 'Predicted'] = best_model.predict(current_features)[0]\n",
    "    \n",
    "    # Update lagged features for future iterations\n",
    "    for j, lag in enumerate(lags):\n",
    "        if date - pd.Timedelta(days=lag) in future_data.index:\n",
    "            future_data.at[date, f'lag_{lag}'] = future_data.at[date, 'Predicted']\n",
    "\n",
    "# Combine actual and predicted values\n",
    "combined_df = pd.DataFrame({\n",
    "    'Date': future_dates,\n",
    "    'Predicted': future_data['Predicted']\n",
    "}).set_index('Date')\n",
    "\n",
    "# Plot actual vs predicted values\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(test_y.index, test_y, label='Actual', marker='o')\n",
    "plt.plot(test_y.index, predictions[best_model_name], label=f'Predicted - {best_model_name}', marker='x')\n",
    "plt.plot(combined_df.index, combined_df['Predicted'], label='Future Predictions', marker='x')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Actual vs Predicted Values')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ],
   "id": "a8e324e2673ebbd0",
   "execution_count": 28,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T09:14:15.554517Z",
     "start_time": "2024-07-24T09:11:14.309187Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_percentage_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import holidays\n",
    "\n",
    "# Function to create date features\n",
    "def create_date_features(data):\n",
    "    data[\"month\"] = data.index.month\n",
    "    data[\"day_of_month\"] = data.index.day\n",
    "    data[\"is_month_start\"] = data.index.is_month_start.astype(int)\n",
    "    data[\"is_month_end\"] = data.index.is_month_end.astype(int)\n",
    "    data[\"day_of_year\"] = data.index.dayofyear\n",
    "    data[\"week_of_year\"] = data.index.isocalendar().week\n",
    "    data[\"day_of_week\"] = data.index.dayofweek + 1\n",
    "    data[\"year\"] = data.index.year\n",
    "    data[\"is_weekend\"] = (data.index.weekday >= 5).astype(int)\n",
    "    data['is_spring'] = data['month'].isin([3, 4, 5]).astype(int)\n",
    "    data['is_summer'] = data['month'].isin([6, 7, 8]).astype(int)\n",
    "    data['is_fall'] = data['month'].isin([9, 10, 11]).astype(int)\n",
    "    data['is_winter'] = data['month'].isin([12, 1, 2]).astype(int)\n",
    "    data['sin_day'] = np.sin(2 * np.pi * data.index.dayofweek / 7)\n",
    "    data['cos_day'] = np.cos(2 * np.pi * data.index.dayofweek / 7)\n",
    "    return data\n",
    "\n",
    "# Add holiday feature\n",
    "uk_holidays = holidays.CountryHoliday('UK')\n",
    "\n",
    "# Create date features\n",
    "rml_data = create_date_features(data.copy())\n",
    "rml_data['is_public_holiday'] = rml_data.index.to_series().apply(lambda date: 1 if date in uk_holidays else 0)\n",
    "\n",
    "# Define Features and Models\n",
    "date_features = ['month', 'day_of_month', 'is_month_start', 'is_month_end',\n",
    "                 'day_of_year', 'week_of_year', 'day_of_week', 'year',\n",
    "                 'is_weekend', 'is_spring', 'is_summer', 'is_fall', 'is_winter',\n",
    "                 'sin_day', 'cos_day', 'is_public_holiday']\n",
    "\n",
    "# Prepare training data\n",
    "def create_lagged_features(data, lags):\n",
    "    for lag in lags:\n",
    "        data[f'lag_{lag}'] = data['y'].shift(lag)\n",
    "    return data\n",
    "\n",
    "lags = [7, 14, 30]\n",
    "rml_data = create_lagged_features(rml_data, lags).dropna()\n",
    "\n",
    "X = rml_data[date_features + [f'lag_{lag}' for lag in lags]]\n",
    "y = rml_data['y']\n",
    "\n",
    "# Split data into train and test sets\n",
    "train_X = X[:-30]\n",
    "train_y = y[:-30]\n",
    "test_X = X[-30:]\n",
    "test_y = y[-30:]\n",
    "\n",
    "# Define parameter grids for each model\n",
    "param_grids = {\n",
    "    'RandomForest': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10]\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [3, 6, 9],\n",
    "        'learning_rate': [0.01, 0.1, 0.2]\n",
    "    },\n",
    "    'LightGBM': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'num_leaves': [31, 40, 50],\n",
    "        'learning_rate': [0.01, 0.1, 0.2]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initialize lists to store results\n",
    "mae_scores = {}\n",
    "mape_scores = {}\n",
    "smape_scores = {}\n",
    "r2_scores = {}\n",
    "predictions = {}\n",
    "\n",
    "def smape(a, f):\n",
    "    return 100 * np.mean(2 * np.abs(f - a) / (np.abs(a) + np.abs(f)))\n",
    "\n",
    "# Train and predict using each model with grid search\n",
    "best_params = {}\n",
    "\n",
    "for model_name, model in {'RandomForest': RandomForestRegressor(), \n",
    "                          'XGBoost': XGBRegressor(objective='reg:squarederror'), \n",
    "                          'LightGBM': lgb.LGBMRegressor()}.items():\n",
    "    grid_search = GridSearchCV(model, param_grids[model_name], cv=3, scoring='neg_mean_absolute_percentage_error')\n",
    "    grid_search.fit(train_X, train_y)\n",
    "    best_params[model_name] = grid_search.best_params_\n",
    "    predictions[model_name] = grid_search.predict(test_X)\n",
    "    \n",
    "    # Calculate MAE, MAPE, sMAPE, and R²\n",
    "    mae = mean_absolute_error(test_y, predictions[model_name])\n",
    "    mape = mean_absolute_percentage_error(test_y, predictions[model_name])\n",
    "    smape_value = smape(test_y, predictions[model_name])\n",
    "    r2 = r2_score(test_y, predictions[model_name])\n",
    "    \n",
    "    mae_scores[model_name] = mae\n",
    "    mape_scores[model_name] = mape\n",
    "    smape_scores[model_name] = smape_value\n",
    "    r2_scores[model_name] = r2\n",
    "    \n",
    "    print(f\"{model_name} - Best Params: {best_params[model_name]}, MAE: {mae:.4f}, MAPE: {mape:.4%}, sMAPE: {smape_value:.4%}, R²: {r2:.4f}\")\n",
    "\n",
    "# Select the Best Model based on MAPE\n",
    "best_model_name = min(mape_scores, key=mape_scores.get)\n",
    "print(f\"\\nThe best model is {best_model_name} with a MAPE of {mape_scores[best_model_name]:.4%}\")\n",
    "\n",
    "# Retrain the best model on the entire dataset with best parameters\n",
    "best_model = {'RandomForest': RandomForestRegressor(**best_params['RandomForest']),\n",
    "              'XGBoost': XGBRegressor(objective='reg:squarederror', **best_params['XGBoost']),\n",
    "              'LightGBM': lgb.LGBMRegressor(**best_params['LightGBM'])}[best_model_name]\n",
    "\n",
    "best_model.fit(X, y)\n",
    "\n",
    "# Predict the next 60 days\n",
    "future_dates = pd.date_range(start=X.index[-1] + pd.Timedelta(days=1), periods=60, freq='D')\n",
    "future_data = pd.DataFrame(index=future_dates)\n",
    "future_data = create_date_features(future_data)\n",
    "future_data['is_public_holiday'] = future_data.index.to_series().apply(lambda date: 1 if date in uk_holidays else 0)\n",
    "\n",
    "# Initialize the lagged features with previous values\n",
    "for lag in lags:\n",
    "    future_data[f'lag_{lag}'] = np.nan\n",
    "\n",
    "# Iterative prediction for the next 60 days\n",
    "for i, date in enumerate(future_dates):\n",
    "    if i == 0:\n",
    "        lagged_features = [np.nan] * len(lags)  # No lagged values for the first date\n",
    "    else:\n",
    "        lagged_features = future_data.iloc[i-1][[f'lag_{lag}' for lag in lags]].values\n",
    "    \n",
    "    # Create the feature vector for the current date\n",
    "    current_features = future_data.loc[date, date_features + [f'lag_{lag}' for lag in lags]].values.reshape(1, -1)\n",
    "    future_data.at[date, 'Predicted'] = best_model.predict(current_features)[0]\n",
    "    \n",
    "    # Update lagged features for future iterations\n",
    "    for j, lag in enumerate(lags):\n",
    "        if date - pd.Timedelta(days=lag) in future_data.index:\n",
    "            future_data.at[date, f'lag_{lag}'] = future_data.at[date, 'Predicted']\n",
    "\n",
    "# Combine actual and predicted values\n",
    "combined_df = pd.DataFrame({\n",
    "    'Date': future_dates,\n",
    "    'Predicted': future_data['Predicted']\n",
    "}).set_index('Date')\n",
    "\n",
    "# Plot actual vs predicted values\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(test_y.index, test_y, label='Actual', marker='o')\n",
    "plt.plot(test_y.index, predictions[best_model_name], label=f'Predicted - {best_model_name}', marker='x')\n",
    "plt.plot(combined_df.index, combined_df['Predicted'], label='Future Predictions', marker='x')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Actual vs Predicted Values')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ],
   "id": "4652813bc9df4fce",
   "execution_count": 27,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T08:11:08.247244Z",
     "start_time": "2024-07-24T08:10:52.979830Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from mlforecast import MLForecast\n",
    "from mlforecast.lag_transforms import ExpandingMean, RollingMean\n",
    "from mlforecast.target_transforms import Differences\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to calculate sMAPE\n",
    "def smape(a, f):\n",
    "    return 100 * np.mean(2 * np.abs(f - a) / (np.abs(a) + np.abs(f)))\n",
    "\n",
    "# Assuming you have defined train_df, test_df, and ml_data somewhere in your code\n",
    "\n",
    "# Convert 'ds' column to datetime if necessary\n",
    "train_df['ds'] = pd.to_datetime(train_df['ds'])\n",
    "test_df['ds'] = pd.to_datetime(test_df['ds'])\n",
    "ml_data['ds'] = pd.to_datetime(ml_data['ds'])\n",
    "\n",
    "# Define the models\n",
    "models = {\n",
    "    'LightGBM': lgb.LGBMRegressor(verbosity=-1),\n",
    "    'XGBoost': xgb.XGBRegressor(verbosity=0),\n",
    "    'RandomForest': RandomForestRegressor(),\n",
    "    'LinearRegression': LinearRegression()\n",
    "}\n",
    "\n",
    "# Initialize lists to store results\n",
    "mae_scores = []\n",
    "mape_scores = []\n",
    "smape_scores = []\n",
    "r2_scores = []\n",
    "forecasts_dict = {}\n",
    "\n",
    "# Define MLForecast object\n",
    "def create_forecast_object(model):\n",
    "    return MLForecast(\n",
    "        models=[model],\n",
    "        freq='D',\n",
    "        lags=[7, 14],\n",
    "        lag_transforms={\n",
    "            1: [ExpandingMean()],\n",
    "            7: [RollingMean(window_size=28)]\n",
    "        },\n",
    "        date_features=['dayofweek'],\n",
    "        target_transforms=[Differences([1])]\n",
    "    )\n",
    "\n",
    "# Train and evaluate each model\n",
    "for name, model in models.items():\n",
    "    fcst = create_forecast_object(model)\n",
    "    fcst.fit(train_df)\n",
    "    \n",
    "    # Make predictions for the test set\n",
    "    predictions = fcst.predict(len(test_df))\n",
    "    predictions.columns = ['unique_id', 'ds', name]\n",
    "    forecasts_dict[name] = predictions\n",
    "    \n",
    "    # Align forecast with test_df to calculate MAE, MAPE, sMAPE, and R²\n",
    "    aligned_forecasts = predictions[name].values[:len(test_df)]\n",
    "    \n",
    "    mae = mean_absolute_error(test_df['y'], aligned_forecasts)\n",
    "    mape = mean_absolute_percentage_error(test_df['y'], aligned_forecasts)\n",
    "    smape_value = smape(test_df['y'], aligned_forecasts)\n",
    "    r2 = r2_score(test_df['y'], aligned_forecasts)\n",
    "    \n",
    "    mae_scores.append((name, mae))\n",
    "    mape_scores.append((name, mape))\n",
    "    smape_scores.append((name, smape_value))\n",
    "    r2_scores.append((name, r2))\n",
    "    \n",
    "    print(f\"{name} - MAE: {mae:.4f}, MAPE: {mape:.4%}, sMAPE: {smape_value:.4%}, R²: {r2:.4f}\")\n",
    "\n",
    "# Select the best model based on MAPE\n",
    "best_model_name, best_mape = min(mape_scores, key=lambda x: x[1])\n",
    "best_model = models[best_model_name]\n",
    "\n",
    "print(f\"\\nThe best model is {best_model_name} with a MAPE of {best_mape:.4%}\")\n",
    "\n",
    "# Train the best model on the entire dataset\n",
    "best_fcst = create_forecast_object(best_model)\n",
    "best_fcst.fit(ml_data)\n",
    "\n",
    "# Predict the next 60 days\n",
    "future_forecasts = best_fcst.predict(60)\n",
    "future_forecasts.columns = ['unique_id', 'ds', best_model_name]\n",
    "\n",
    "# Combine the historical data with the forecast data for plotting\n",
    "combined_df = ml_data.copy()\n",
    "future_df = pd.DataFrame({\n",
    "    'ds': pd.date_range(start=ml_data['ds'].max() + pd.Timedelta(days=1), periods=60, freq='D'),\n",
    "    'y': np.nan\n",
    "})\n",
    "combined_df = pd.concat([combined_df, future_df], ignore_index=True)\n",
    "combined_df['forecast'] = np.nan\n",
    "combined_df.loc[ml_data.shape[0]:, 'forecast'] = future_forecasts[best_model_name].values\n",
    "\n",
    "# Plot actual vs predicted values for the historical period and future forecast\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Plot the entire dataset\n",
    "plt.plot(combined_df['ds'], combined_df['y'], label='Actual', marker='o')\n",
    "\n",
    "# Plot the future forecasts\n",
    "plt.plot(\n",
    "    combined_df['ds'],\n",
    "    combined_df['forecast'],\n",
    "    label=f'Predicted - {best_model_name} (Next 60 Days)',\n",
    "    marker='x',\n",
    ")\n",
    "\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.title(f'Actual vs Predicted Values using {best_model_name}')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nThe best model is {best_model_name} with a MAPE of {best_mape:.4%}\")\n"
   ],
   "id": "83f3e283fe731e1b",
   "execution_count": 11,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Deep Learning",
   "id": "115d3a116f9ec8b2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T08:40:36.935255Z",
     "start_time": "2024-07-24T08:38:00.476792Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from neuralforecast import NeuralForecast\n",
    "from neuralforecast.models import NHITS, NBEATS, LSTM\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to calculate sMAPE\n",
    "def smape(a, f):\n",
    "    return 100 * np.mean(2 * np.abs(f - a) / (np.abs(a) + np.abs(f)))\n",
    "\n",
    "# Assuming you have defined train_df, test_df, and ml_data somewhere in your code\n",
    "\n",
    "# Define the models and their respective parameters\n",
    "nhits_params = {\n",
    "    'h': 60,\n",
    "    'input_size': 30,\n",
    "    'max_steps': 50\n",
    "}\n",
    "\n",
    "nbeats_params = {\n",
    "    'h': 60,\n",
    "    'input_size': 30,\n",
    "    'max_steps': 50\n",
    "}\n",
    "\n",
    "lstm_params = {\n",
    "    'h': 60,\n",
    "    'input_size': 30,\n",
    "    'max_steps': 50\n",
    "}\n",
    "\n",
    "# Initialize the models\n",
    "nhits_model = NHITS(**nhits_params)\n",
    "nbeats_model = NBEATS(**nbeats_params)\n",
    "lstm_model = LSTM(**lstm_params)\n",
    "\n",
    "# Train each model on the training data and evaluate on test data\n",
    "models = [nhits_model, nbeats_model, lstm_model]\n",
    "model_names = ['NHITS', 'NBEATS', 'LSTM']\n",
    "mae_scores = []\n",
    "mape_scores = []\n",
    "smape_scores = []\n",
    "r2_scores = []\n",
    "\n",
    "for model, name in zip(models, model_names):\n",
    "    nf = NeuralForecast(models=[model], freq='D')\n",
    "    nf.fit(train_df)\n",
    "    \n",
    "    # Create future dataframe for the test set to match expected combinations\n",
    "    future_test_df = nf.make_future_dataframe()\n",
    "    \n",
    "    # Make predictions for the test set\n",
    "    forecasts = nf.predict()\n",
    "    forecasts = forecasts.set_index('ds')\n",
    "    \n",
    "    # Align forecast with test_df to calculate MAE, MAPE, sMAPE, and R²\n",
    "    aligned_forecasts = forecasts[name].iloc[:len(test_df)].values\n",
    "    \n",
    "    mae = mean_absolute_error(test_df['y'], aligned_forecasts)\n",
    "    mape = mean_absolute_percentage_error(test_df['y'], aligned_forecasts)\n",
    "    smape_value = smape(test_df['y'], aligned_forecasts)\n",
    "    r2 = r2_score(test_df['y'], aligned_forecasts)\n",
    "    \n",
    "    mae_scores.append((name, mae))\n",
    "    mape_scores.append((name, mape))\n",
    "    smape_scores.append((name, smape_value))\n",
    "    r2_scores.append((name, r2))\n",
    "    \n",
    "    print(f\"{name} - MAE: {mae:.4f}, MAPE: {mape:.4%}, sMAPE: {smape_value:.4%}, R²: {r2:.4f}\")\n",
    "\n",
    "# Select the best model based on MAPE\n",
    "best_model_name, best_mape = min(mape_scores, key=lambda x: x[1])\n",
    "best_model = None\n",
    "\n",
    "if best_model_name == 'NHITS':\n",
    "    best_model = NHITS(**nhits_params)\n",
    "elif best_model_name == 'NBEATS':\n",
    "    best_model = NBEATS(**nbeats_params)\n",
    "elif best_model_name == 'LSTM':\n",
    "    best_model = LSTM(**lstm_params)\n",
    "    \n",
    "print(f\"\\nThe best model is {best_model_name} with a MAPE of {best_mape:.4%}\")\n",
    "\n",
    "# Train the best model on the entire dataset\n",
    "nf_best_model = NeuralForecast(models=[best_model], freq='D')\n",
    "nf_best_model.fit(ml_data)\n",
    "\n",
    "# Create a future dataframe for the next 60 days\n",
    "future_df = nf_best_model.make_future_dataframe()\n",
    "\n",
    "# Predict the next 60 days\n",
    "future_forecasts = nf_best_model.predict()\n",
    "future_forecasts = future_forecasts.set_index('ds')\n",
    "\n",
    "# Combine the historical data with the forecast data for plotting\n",
    "combined_df = pd.concat([ml_data, future_df], ignore_index=True)\n",
    "combined_df['forecast'] = np.nan\n",
    "combined_df.loc[ml_data.shape[0]:, 'forecast'] = future_forecasts[best_model_name].values\n",
    "\n",
    "# Plot actual vs predicted values for the historical period and future forecast\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Plot the entire dataset\n",
    "plt.plot(combined_df['ds'], combined_df['y'], label='Actual', marker='o')\n",
    "\n",
    "# Plot the future forecasts\n",
    "plt.plot(\n",
    "    combined_df['ds'],\n",
    "    combined_df['forecast'],\n",
    "    label=f'Predicted - {best_model_name} (Next 60 Days)',\n",
    "    marker='x',\n",
    ")\n",
    "\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.title(f'Actual vs Predicted Values using {best_model_name}')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"The best model is {best_model_name} with a MAPE of {best_mape:.4%}\")\n"
   ],
   "id": "12d21bbfdc7895ac",
   "execution_count": 20,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T08:34:12.619576Z",
     "start_time": "2024-07-24T08:25:07.509997Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from neuralforecast import NeuralForecast\n",
    "from neuralforecast.models import NHITS, NBEATS, LSTM\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to calculate sMAPE\n",
    "def smape(a, f):\n",
    "    return 100 * np.mean(2 * np.abs(f - a) / (np.abs(a) + np.abs(f)))\n",
    "\n",
    "# Assuming you have defined train_df, test_df, and ml_data somewhere in your code\n",
    "\n",
    "# Define the parameter grids for manual fine-tuning\n",
    "nhits_param_grid = [\n",
    "    {'h': 60, 'input_size': 30, 'max_steps': 50, 'learning_rate': 0.001},\n",
    "    {'h': 60, 'input_size': 60, 'max_steps': 100, 'learning_rate': 0.01},\n",
    "    {'h': 60, 'input_size': 90, 'max_steps': 150, 'learning_rate': 0.1},\n",
    "]\n",
    "\n",
    "nbeats_param_grid = [\n",
    "    {'h': 60, 'input_size': 30, 'max_steps': 50, 'learning_rate': 0.001},\n",
    "    {'h': 60, 'input_size': 60, 'max_steps': 100, 'learning_rate': 0.01},\n",
    "    {'h': 60, 'input_size': 90, 'max_steps': 150, 'learning_rate': 0.1},\n",
    "]\n",
    "\n",
    "lstm_param_grid = [\n",
    "    {'h': 60, 'input_size': 30, 'max_steps': 50, 'learning_rate': 0.001, 'encoder_n_layers': 2, 'encoder_hidden_size': 200},\n",
    "    {'h': 60, 'input_size': 60, 'max_steps': 100, 'learning_rate': 0.01, 'encoder_n_layers': 2, 'encoder_hidden_size': 200},\n",
    "    {'h': 60, 'input_size': 90, 'max_steps': 150, 'learning_rate': 0.1, 'encoder_n_layers': 3, 'encoder_hidden_size': 300},\n",
    "]\n",
    "\n",
    "# Initialize the models\n",
    "nhits_model = NHITS(h=60, input_size=30)\n",
    "nbeats_model = NBEATS(h=60, input_size=30)\n",
    "lstm_model = LSTM(h=60, input_size=30)\n",
    "\n",
    "# Train and evaluate each model with different hyperparameters\n",
    "models = [nhits_model, nbeats_model, lstm_model]\n",
    "param_grids = [nhits_param_grid, nbeats_param_grid, lstm_param_grid]\n",
    "model_names = ['NHITS', 'NBEATS', 'LSTM']\n",
    "mape_scores = []\n",
    "mae_scores = []\n",
    "smape_scores = []\n",
    "r2_scores = []\n",
    "\n",
    "for model, param_grid, name in zip(models, param_grids, model_names):\n",
    "    for params in param_grid:\n",
    "        if name == 'NHITS':\n",
    "            model = NHITS(**params)\n",
    "        elif name == 'NBEATS':\n",
    "            model = NBEATS(**params)\n",
    "        elif name == 'LSTM':\n",
    "            model = LSTM(**params)\n",
    "            \n",
    "        nf = NeuralForecast(models=[model], freq='D')\n",
    "        nf.fit(train_df)\n",
    "        \n",
    "        # Make predictions for the test set\n",
    "        forecasts = nf.predict()\n",
    "        forecasts = forecasts.set_index('ds')\n",
    "        \n",
    "        # Align forecast with test_df to calculate MAE, MAPE, sMAPE, and R²\n",
    "        aligned_forecasts = forecasts[name].iloc[:len(test_df)].values\n",
    "        \n",
    "        mae = mean_absolute_error(test_df['y'], aligned_forecasts)\n",
    "        mape = mean_absolute_percentage_error(test_df['y'], aligned_forecasts)\n",
    "        smape_value = smape(test_df['y'], aligned_forecasts)\n",
    "        r2 = r2_score(test_df['y'], aligned_forecasts)\n",
    "        \n",
    "        mae_scores.append((name, mae, params))\n",
    "        mape_scores.append((name, mape, params))\n",
    "        smape_scores.append((name, smape_value, params))\n",
    "        r2_scores.append((name, r2, params))\n",
    "        \n",
    "        print(f\"{name} - MAE: {mae:.4f}, MAPE: {mape:.4%}, sMAPE: {smape_value:.4%}, R²: {r2:.4f}\")\n",
    "\n",
    "# Select the best model based on MAPE\n",
    "best_model_name, best_mape, best_params = min(mape_scores, key=lambda x: x[1])\n",
    "\n",
    "if best_model_name == 'NHITS':\n",
    "    best_model = NHITS(**best_params)\n",
    "elif best_model_name == 'NBEATS':\n",
    "    best_model = NBEATS(**best_params)\n",
    "elif best_model_name == 'LSTM':\n",
    "    best_model = LSTM(**best_params)\n",
    "    \n",
    "print(f\"The best model is {best_model_name} with a MAPE of {best_mape:.4%} and best parameters: {best_params}\")\n",
    "\n",
    "# Train the best model on the entire dataset\n",
    "nf_best_model = NeuralForecast(models=[best_model], freq='D')\n",
    "nf_best_model.fit(ml_data)\n",
    "\n",
    "# Create a future dataframe for the next 60 days\n",
    "future_df = nf_best_model.make_future_dataframe()\n",
    "\n",
    "# Predict the next 60 days\n",
    "future_forecasts = nf_best_model.predict()\n",
    "future_forecasts = future_forecasts.set_index('ds')\n",
    "\n",
    "# Combine the historical data with the forecast data for plotting\n",
    "combined_df = pd.concat([ml_data, future_df], ignore_index=True)\n",
    "combined_df['forecast'] = np.nan\n",
    "combined_df.loc[ml_data.shape[0]:, 'forecast'] = future_forecasts[best_model_name].values\n",
    "\n",
    "# Plot actual vs predicted values for the historical period and future forecast\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Plot the entire dataset\n",
    "plt.plot(combined_df['ds'], combined_df['y'], label='Actual', marker='o')\n",
    "\n",
    "# Plot the future forecasts\n",
    "plt.plot(\n",
    "    combined_df['ds'],\n",
    "    combined_df['forecast'],\n",
    "    label=f'Predicted - {best_model_name} (Next 60 Days)',\n",
    "    marker='x',\n",
    ")\n",
    "\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.title(f'Actual vs Predicted Values using {best_model_name}')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"The best model is {best_model_name} with a MAPE of {best_mape:.4%} and best parameters: {best_params}\")\n"
   ],
   "id": "15305356b061b8a9",
   "execution_count": 19,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Prophet",
   "id": "9a5477fd874c5970"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T08:47:33.882917Z",
     "start_time": "2024-07-24T08:47:19.994870Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import itertools\n",
    "from sklearn.metrics import mean_absolute_percentage_error, mean_absolute_error, r2_score\n",
    "import pandas as pd\n",
    "from prophet import Prophet\n",
    "from prophet.diagnostics import cross_validation, performance_metrics\n",
    "from prophet.plot import plot_cross_validation_metric\n",
    "import holidays\n",
    "\n",
    "# Function to calculate sMAPE\n",
    "def smape(a, f):\n",
    "    return 100 * np.mean(2 * np.abs(f - a) / (np.abs(a) + np.abs(f)))\n",
    "\n",
    "# Add is_public_holiday column\n",
    "holiday = holidays.CountryHoliday('UK')\n",
    "data['is_public_holiday'] = data['ds'].apply(lambda date: 1 if date in holiday else 0)\n",
    "\n",
    "# Define model parameters\n",
    "model_params = {\n",
    "    'changepoint_prior_scale': 0.1,\n",
    "    'seasonality_prior_scale': 1.0,\n",
    "    'holidays_prior_scale': 1.0,\n",
    "    'seasonality_mode': 'additive'\n",
    "}\n",
    "\n",
    "# Train the model with defined parameters\n",
    "model = Prophet(**model_params)\n",
    "model.add_regressor('is_public_holiday')\n",
    "model.fit(data)\n",
    "\n",
    "# Cross-validate the model\n",
    "df_cv = cross_validation(model, initial='547 days', period='180 days', horizon='30 days')\n",
    "df_p = performance_metrics(df_cv)\n",
    "\n",
    "# Calculate metrics using yhat and y\n",
    "mape = mean_absolute_percentage_error(df_cv['y'], df_cv['yhat'])\n",
    "mae = mean_absolute_error(df_cv['y'], df_cv['yhat'])\n",
    "smape_value = smape(df_cv['y'], df_cv['yhat'])\n",
    "r2 = r2_score(df_cv['y'], df_cv['yhat'])\n",
    "\n",
    "# Output the metrics\n",
    "print(f\"MAPE: {round(mape, 2)}\")\n",
    "print(f\"MAE: {round(mae, 2)}\")\n",
    "print(f\"sMAPE: {round(smape_value, 2)}\")\n",
    "print(f\"R²: {round(r2, 2)}\")\n",
    "\n",
    "# Forecast future values\n",
    "future = model.make_future_dataframe(periods=365)\n",
    "future['is_public_holiday'] = future['ds'].apply(lambda date: 1 if date in holiday else 0)\n",
    "\n",
    "forecast = model.predict(future)\n",
    "\n",
    "# Visualize Results\n",
    "model.plot(forecast)\n",
    "model.plot_components(forecast)\n",
    "\n",
    "# Evaluate Accuracy with a longer horizon\n",
    "df_cv = cross_validation(model, initial='547 days', period='180 days', horizon='60 days')\n",
    "df_p = performance_metrics(df_cv)\n",
    "print(df_p.head().round(2))\n",
    "\n",
    "fig = plot_cross_validation_metric(df_cv, metric='rmse')\n",
    "\n",
    "# Calculate metrics using yhat and y for the longer horizon\n",
    "mape = mean_absolute_percentage_error(df_cv['y'], df_cv['yhat'])\n",
    "mae = mean_absolute_error(df_cv['y'], df_cv['yhat'])\n",
    "smape_value = smape(df_cv['y'], df_cv['yhat'])\n",
    "r2 = r2_score(df_cv['y'], df_cv['yhat'])\n",
    "\n",
    "print(f\"MAPE: {round(mape, 2)}\")\n",
    "print(f\"MAE: {round(mae, 2)}\")\n",
    "print(f\"sMAPE: {round(smape_value, 2)}\")\n",
    "print(f\"R²: {round(r2, 2)}\")\n"
   ],
   "id": "9f2a7b17d6b00b5f",
   "execution_count": 22,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T08:46:17.245505Z",
     "start_time": "2024-07-24T08:44:18.250060Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import itertools\n",
    "from sklearn.metrics import mean_absolute_percentage_error, mean_absolute_error, r2_score\n",
    "import pandas as pd\n",
    "from prophet import Prophet\n",
    "from prophet.diagnostics import cross_validation, performance_metrics\n",
    "from prophet.plot import plot_cross_validation_metric\n",
    "import holidays\n",
    "\n",
    "# Function to calculate sMAPE\n",
    "def smape(a, f):\n",
    "    return 100 * np.mean(2 * np.abs(f - a) / (np.abs(a) + np.abs(f)))\n",
    "\n",
    "# Add is_public_holiday column\n",
    "holiday = holidays.CountryHoliday('UK')\n",
    "data['is_public_holiday'] = data['ds'].apply(lambda date: 1 if date in holiday else 0)\n",
    "\n",
    "# Define hyperparameters to tune\n",
    "param_grid = {\n",
    "    'changepoint_prior_scale': [0.01, 0.1, 0.5],\n",
    "    'seasonality_prior_scale': [0.1, 1.0, 10.0],\n",
    "    'holidays_prior_scale': [0.1, 1.0, 10.0],\n",
    "    'seasonality_mode': ['additive', 'multiplicative']\n",
    "}\n",
    "\n",
    "# Generate all combinations of parameters\n",
    "all_params = [dict(zip(param_grid.keys(), v)) for v in itertools.product(*param_grid.values())]\n",
    "\n",
    "best_params = None\n",
    "best_mape = float('inf')\n",
    "best_mae = float('inf')\n",
    "best_smape = float('inf')\n",
    "best_r2 = float('-inf')\n",
    "\n",
    "# Grid search to find the best hyperparameters\n",
    "for params in all_params:\n",
    "    model = Prophet(**params)\n",
    "    model.add_regressor('is_public_holiday')\n",
    "    model.fit(data)\n",
    "\n",
    "    # Cross-validate the model\n",
    "    df_cv = cross_validation(model, initial='547 days', period='180 days', horizon='30 days')\n",
    "    df_p = performance_metrics(df_cv)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mape = mean_absolute_percentage_error(df_cv['y'], df_cv['yhat'])\n",
    "    mae = mean_absolute_error(df_cv['y'], df_cv['yhat'])\n",
    "    smape_value = smape(df_cv['y'], df_cv['yhat'])\n",
    "    r2 = r2_score(df_cv['y'], df_cv['yhat'])\n",
    "    \n",
    "    if mape < best_mape:\n",
    "        best_mape = mape\n",
    "        best_mae = mae\n",
    "        best_smape = smape_value\n",
    "        best_r2 = r2\n",
    "        best_params = params\n",
    "\n",
    "# Output the best parameters and metrics\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "print(f\"Best MAPE: {round(best_mape, 2)}\")\n",
    "print(f\"Best MAE: {round(best_mae, 2)}\")\n",
    "print(f\"Best sMAPE: {round(best_smape, 2)}\")\n",
    "print(f\"Best R²: {round(best_r2, 2)}\")\n",
    "\n",
    "# Train the final model with the best parameters\n",
    "model = Prophet(**best_params)\n",
    "model.add_regressor('is_public_holiday')\n",
    "model.fit(data)\n",
    "\n",
    "# Forecast future values\n",
    "future = model.make_future_dataframe(periods=365)\n",
    "future['is_public_holiday'] = future['ds'].apply(lambda date: 1 if date in holiday else 0)\n",
    "\n",
    "forecast = model.predict(future)\n",
    "\n",
    "# Visualize Results\n",
    "model.plot(forecast)\n",
    "model.plot_components(forecast)\n",
    "\n",
    "# Evaluate Accuracy\n",
    "df_cv = cross_validation(model, initial='547 days', period='180 days', horizon='60 days')\n",
    "df_p = performance_metrics(df_cv)\n",
    "print(df_p.head().round(2))\n",
    "\n",
    "fig = plot_cross_validation_metric(df_cv, metric='rmse')\n",
    "\n",
    "# Calculate metrics using yhat and y\n",
    "mape = mean_absolute_percentage_error(df_cv['y'], df_cv['yhat'])\n",
    "mae = mean_absolute_error(df_cv['y'], df_cv['yhat'])\n",
    "smape_value = smape(df_cv['y'], df_cv['yhat'])\n",
    "r2 = r2_score(df_cv['y'], df_cv['yhat'])\n",
    "\n",
    "print(f\"MAPE: {round(mape, 2)}\")\n",
    "print(f\"MAE: {round(mae, 2)}\")\n",
    "print(f\"sMAPE: {round(smape_value, 2)}\")\n",
    "print(f\"R²: {round(r2, 2)}\")\n"
   ],
   "id": "1f75ac0da8b785fa",
   "execution_count": 21,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "",
   "id": "8ecbcbad9bd5879d",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

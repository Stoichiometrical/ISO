{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 30 Forecast Horizon ",
   "id": "edcb4e14a661021b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### This notebook has the models forecasting only for a 30day forecast with fine tuning",
   "id": "55d36a061b0e5653"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T09:02:19.649462Z",
     "start_time": "2024-07-29T09:02:17.783982Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Import the neccesary libraries\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gr\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tabulate import tabulate\n",
    "import holidays"
   ],
   "id": "4c2c945e9879e1e7",
   "execution_count": 1,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T09:02:19.788207Z",
     "start_time": "2024-07-29T09:02:19.651473Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pd.read_csv(\"processed.csv\", parse_dates=['date'], index_col='date')\n",
    "df =df.asfreq('D')\n",
    "df =df[[\"quantity\"]]\n",
    "df.head()"
   ],
   "id": "54499b99e7e37c68",
   "execution_count": 2,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T09:02:19.810353Z",
     "start_time": "2024-07-29T09:02:19.788207Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = df.copy()\n",
    "data[\"unique_id\"]=1.0\n",
    "data[\"ds\"] = data.index\n",
    "data.rename(columns={\"quantity\":\"y\"},inplace=True)\n",
    "data.head()"
   ],
   "id": "137980cadb49155e",
   "execution_count": 3,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T09:02:19.823774Z",
     "start_time": "2024-07-29T09:02:19.813369Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Using Nxitla libraries\n",
    "#Data Split\n",
    "# Calculate the index for the split\n",
    "split_index = int(0.8 * len(data))\n",
    "\n",
    "# Split the data\n",
    "Y_train_df = data.iloc[:split_index]\n",
    "Y_test_df = data.iloc[split_index:]   # Test data for January 2012\n",
    "\n",
    "horizon = len(Y_test_df)"
   ],
   "id": "46b0a1432788dfed",
   "execution_count": 4,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T09:02:20.107837Z",
     "start_time": "2024-07-29T09:02:19.823774Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ml_data = data.copy()\n",
    "train_df = ml_data [:-30]\n",
    "test_df = ml_data [-30:]"
   ],
   "id": "7622e40b456b0f76",
   "execution_count": 5,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-25T05:26:19.209561Z",
     "start_time": "2024-07-25T05:25:56.420298Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import holidays\n",
    "# Function to create date features\n",
    "def create_date_features(data):\n",
    "    data[\"month\"] = data.index.month\n",
    "    data[\"day_of_month\"] = data.index.day\n",
    "    data[\"is_month_start\"] = data.index.is_month_start.astype(int)\n",
    "    data[\"is_month_end\"] = data.index.is_month_end.astype(int)\n",
    "    data[\"day_of_year\"] = data.index.dayofyear\n",
    "    data[\"week_of_year\"] = data.index.isocalendar().week\n",
    "    data[\"day_of_week\"] = data.index.dayofweek + 1\n",
    "    data[\"year\"] = data.index.year\n",
    "    data[\"is_weekend\"] = (data.index.weekday >= 5).astype(int)\n",
    "    data['is_spring'] = data['month'].isin([3, 4, 5]).astype(int)\n",
    "    data['is_summer'] = data['month'].isin([6, 7, 8]).astype(int)\n",
    "    data['is_fall'] = data['month'].isin([9, 10, 11]).astype(int)\n",
    "    data['is_winter'] = data['month'].isin([12, 1, 2]).astype(int)\n",
    "    data['sin_day'] = np.sin(2 * np.pi * data.index.dayofweek / 7)\n",
    "    data['cos_day'] = np.cos(2 * np.pi * data.index.dayofweek / 7)\n",
    "    return data\n",
    "\n",
    "# Add holiday feature\n",
    "uk_holidays = holidays.CountryHoliday('UK')\n"
   ],
   "id": "b06df5d0e8d79c55",
   "execution_count": 8,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-25T05:23:16.500752Z",
     "start_time": "2024-07-25T05:22:41.656678Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsforecast.models import AutoARIMA, AutoETS, AutoCES, AutoTheta, SimpleExponentialSmoothingOptimized\n",
    "from sklearn.metrics import mean_absolute_percentage_error, mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming train_df, test_df, and ml_data are already defined in your environment\n",
    "\n",
    "# Extract the 'y' series from train and test dataframes\n",
    "train_series = train_df['y'].values\n",
    "test_series = test_df['y'].values\n",
    "\n",
    "# Define the models\n",
    "models = {\n",
    "    'AutoARIMA': AutoARIMA(season_length=12),\n",
    "    'AutoETS': AutoETS(model='ZZZ', season_length=12),\n",
    "    'AutoCES': AutoCES(model='Z', season_length=12),\n",
    "    'AutoTheta': AutoTheta(season_length=12),\n",
    "    'SESOpt': SimpleExponentialSmoothingOptimized()\n",
    "}\n",
    "\n",
    "# Initialize lists to store results\n",
    "mape_scores = []\n",
    "mae_scores = []\n",
    "\n",
    "# Train and evaluate each model\n",
    "for name, model in models.items():\n",
    "    model = model.fit(y=train_series)\n",
    "    forecasts = model.predict(h=len(test_series))\n",
    "    \n",
    "    # Calculate MAE and MAPE\n",
    "    mae = mean_absolute_error(test_series, forecasts['mean'])\n",
    "    mape = mean_absolute_percentage_error(test_series, forecasts['mean'])\n",
    "    \n",
    "    mae_scores.append((name, mae))\n",
    "    mape_scores.append((name, mape))\n",
    "    \n",
    "    print(f\"{name} - MAE: {mae:.4f}, MAPE: {mape:.4%}\")\n",
    "\n",
    "# Create a DataFrame to display the results\n",
    "results_df = pd.DataFrame(mae_scores, columns=['Model', 'MAE']).merge(\n",
    "    pd.DataFrame(mape_scores, columns=['Model', 'MAPE']), on='Model'\n",
    ")\n",
    "\n",
    "# Display the results DataFrame\n",
    "print(results_df)\n",
    "\n",
    "# Select the best model based on MAPE\n",
    "best_model_name, best_mape = min(mape_scores, key=lambda x: x[1])\n",
    "best_model = models[best_model_name]\n",
    "\n",
    "print(f\"\\nThe best model is {best_model_name} with a MAPE of {best_mape:.4%}\")\n",
    "\n",
    "# Train the best model on the entire dataset\n",
    "best_model = best_model.fit(y=ml_data['y'].values)\n",
    "\n",
    "# Predict the next 30 days\n",
    "future_forecasts = best_model.predict(h=30)\n",
    "\n",
    "# Combine the historical data with the forecast data for plotting\n",
    "combined_df = ml_data.copy()\n",
    "future_df = pd.DataFrame({\n",
    "    'ds': pd.date_range(start=ml_data['ds'].max() + pd.Timedelta(days=1), periods=30, freq='D'),\n",
    "    'y': np.nan\n",
    "})\n",
    "combined_df = pd.concat([combined_df, future_df], ignore_index=True)\n",
    "combined_df['forecast'] = np.nan\n",
    "combined_df.loc[ml_data.shape[0]:, 'forecast'] = future_forecasts['mean']\n",
    "\n",
    "# Plot actual vs predicted values for the historical period and future forecast\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Plot the entire dataset\n",
    "plt.plot(combined_df['ds'], combined_df['y'], label='Actual', marker='o')\n",
    "\n",
    "# Plot the future forecasts\n",
    "plt.plot(\n",
    "    combined_df['ds'],\n",
    "    combined_df['forecast'],\n",
    "    label=f'Predicted - {best_model_name} (Next 30 Days)',\n",
    "    marker='x',\n",
    ")\n",
    "\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.title(f'Actual vs Predicted Values using {best_model_name}')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ],
   "id": "ef98a99123820862",
   "execution_count": 7,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Machine Learning",
   "id": "de5e9dce7f13a31a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-25T05:44:09.996724Z",
     "start_time": "2024-07-25T05:39:36.641469Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_percentage_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import holidays\n",
    "\n",
    "# Function to create date features\n",
    "def create_date_features(data):\n",
    "    data[\"month\"] = data.index.month\n",
    "    data[\"day_of_month\"] = data.index.day\n",
    "    data[\"is_month_start\"] = data.index.is_month_start.astype(int)\n",
    "    data[\"is_month_end\"] = data.index.is_month_end.astype(int)\n",
    "    data[\"day_of_year\"] = data.index.dayofyear\n",
    "    data[\"week_of_year\"] = data.index.isocalendar().week\n",
    "    data[\"day_of_week\"] = data.index.dayofweek + 1\n",
    "    data[\"year\"] = data.index.year\n",
    "    data[\"is_weekend\"] = (data.index.weekday >= 5).astype(int)\n",
    "    data['is_spring'] = data['month'].isin([3, 4, 5]).astype(int)\n",
    "    data['is_summer'] = data['month'].isin([6, 7, 8]).astype(int)\n",
    "    data['is_fall'] = data['month'].isin([9, 10, 11]).astype(int)\n",
    "    data['is_winter'] = data['month'].isin([12, 1, 2]).astype(int)\n",
    "    data['sin_day'] = np.sin(2 * np.pi * data.index.dayofweek / 7)\n",
    "    data['cos_day'] = np.cos(2 * np.pi * data.index.dayofweek / 7)\n",
    "    return data\n",
    "\n",
    "# Add holiday feature\n",
    "uk_holidays = holidays.CountryHoliday('UK')\n",
    "\n",
    "# Create date features\n",
    "rml_data = create_date_features(data.copy())\n",
    "rml_data['is_public_holiday'] = rml_data.index.to_series().apply(lambda date: 1 if date in uk_holidays else 0)\n",
    "\n",
    "# Define Features and Models\n",
    "date_features = ['month', 'day_of_month', 'is_month_start', 'is_month_end',\n",
    "                 'day_of_year', 'week_of_year', 'day_of_week', 'year',\n",
    "                 'is_weekend', 'is_spring', 'is_summer', 'is_fall', 'is_winter',\n",
    "                 'sin_day', 'cos_day', 'is_public_holiday']\n",
    "\n",
    "# Prepare training data\n",
    "def create_lagged_features(data, lags):\n",
    "    for lag in lags:\n",
    "        data[f'lag_{lag}'] = data['y'].shift(lag)\n",
    "    return data\n",
    "\n",
    "lags = [7, 14, 30]\n",
    "rml_data = create_lagged_features(rml_data, lags).dropna()\n",
    "\n",
    "X = rml_data[date_features + [f'lag_{lag}' for lag in lags]]\n",
    "y = rml_data['y']\n",
    "\n",
    "# Split data into train and test sets\n",
    "train_X = X[:-30]\n",
    "train_y = y[:-30]\n",
    "test_X = X[-30:]\n",
    "test_y = y[-30:]\n",
    "\n",
    "# Define parameter grids for each model\n",
    "param_grids = {\n",
    "    'RandomForest': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10]\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [3, 6, 9],\n",
    "        'learning_rate': [0.01, 0.1, 0.2]\n",
    "    },\n",
    "    'LightGBM': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'num_leaves': [31, 40, 50],\n",
    "        'learning_rate': [0.01, 0.1, 0.2]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initialize lists to store results\n",
    "mae_scores = {}\n",
    "mape_scores = {}\n",
    "predictions = {}\n",
    "best_params = {}\n",
    "\n",
    "def smape(a, f):\n",
    "    return 100 * np.mean(2 * np.abs(f - a) / (np.abs(a) + np.abs(f)))\n",
    "\n",
    "# Train and predict using each model with grid search\n",
    "for model_name, model in {'RandomForest': RandomForestRegressor(), \n",
    "                          'XGBoost': XGBRegressor(objective='reg:squarederror'), \n",
    "                          'LightGBM': lgb.LGBMRegressor()}.items():\n",
    "    grid_search = GridSearchCV(model, param_grids[model_name], cv=3, scoring='neg_mean_absolute_percentage_error')\n",
    "    grid_search.fit(train_X, train_y)\n",
    "    best_params[model_name] = grid_search.best_params_\n",
    "    predictions[model_name] = grid_search.predict(test_X)\n",
    "    \n",
    "    # Calculate MAE and MAPE\n",
    "    mae = mean_absolute_error(test_y, predictions[model_name])\n",
    "    mape = mean_absolute_percentage_error(test_y, predictions[model_name])\n",
    "    \n",
    "    mae_scores[model_name] = mae\n",
    "    mape_scores[model_name] = mape\n",
    "    \n",
    "    print(f\"{model_name} - Best Params: {best_params[model_name]}, MAE: {mae:.4f}, MAPE: {mape:.4%}\")\n",
    "\n",
    "# Create a DataFrame to display the results\n",
    "results_df = pd.DataFrame(mae_scores.items(), columns=['Model', 'MAE']).merge(\n",
    "    pd.DataFrame(mape_scores.items(), columns=['Model', 'MAPE']), on='Model'\n",
    ")\n",
    "\n",
    "# Display the results DataFrame\n",
    "print(results_df)\n",
    "\n",
    "# Select the best model based on MAPE\n",
    "best_model_name = min(mape_scores, key=mape_scores.get)\n",
    "print(f\"\\nThe best model is {best_model_name} with a MAPE of {mape_scores[best_model_name]:.4%}\")\n",
    "\n",
    "# Retrain the best model on the entire dataset with best parameters\n",
    "best_model = {'RandomForest': RandomForestRegressor(**best_params['RandomForest']),\n",
    "              'XGBoost': XGBRegressor(objective='reg:squarederror', **best_params['XGBoost']),\n",
    "              'LightGBM': lgb.LGBMRegressor(**best_params['LightGBM'])}[best_model_name]\n",
    "\n",
    "best_model.fit(X, y)\n",
    "\n",
    "# Predict the next 30 days\n",
    "future_dates = pd.date_range(start=X.index[-1] + pd.Timedelta(days=1), periods=30, freq='D')\n",
    "future_data = pd.DataFrame(index=future_dates)\n",
    "future_data = create_date_features(future_data)\n",
    "future_data['is_public_holiday'] = future_data.index.to_series().apply(lambda date: 1 if date in uk_holidays else 0)\n",
    "\n",
    "# Initialize the lagged features with previous values\n",
    "for lag in lags:\n",
    "    future_data[f'lag_{lag}'] = np.nan\n",
    "\n",
    "# Iterative prediction for the next 30 days\n",
    "for i, date in enumerate(future_dates):\n",
    "    if i == 0:\n",
    "        lagged_features = [np.nan] * len(lags)  # No lagged values for the first date\n",
    "    else:\n",
    "        lagged_features = future_data.iloc[i-1][[f'lag_{lag}' for lag in lags]].values\n",
    "    \n",
    "    # Create the feature vector for the current date\n",
    "    current_features = future_data.loc[date, date_features + [f'lag_{lag}' for lag in lags]].values.reshape(1, -1)\n",
    "    future_data.at[date, 'Predicted'] = best_model.predict(current_features)[0]\n",
    "    \n",
    "    # Update lagged features for future iterations\n",
    "    for j, lag in enumerate(lags):\n",
    "        if date - pd.Timedelta(days=lag) in future_data.index:\n",
    "            future_data.at[date, f'lag_{lag}'] = future_data.at[date, 'Predicted']\n",
    "\n",
    "# Combine actual and predicted values\n",
    "combined_df = pd.DataFrame({\n",
    "    'Date': future_dates,\n",
    "    'Predicted': future_data['Predicted']\n",
    "}).set_index('Date')\n",
    "\n",
    "# Plot actual vs predicted values\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(test_y.index, test_y, label='Actual', marker='o')\n",
    "plt.plot(test_y.index, predictions[best_model_name], label=f'Predicted - {best_model_name}', marker='x')\n",
    "plt.plot(combined_df.index, combined_df['Predicted'], label='Future Predictions', marker='x')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Actual vs Predicted Values')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ],
   "id": "dc5db30c14f448ad",
   "execution_count": 10,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Deep learning",
   "id": "623395a6930e7d14"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-25T06:51:31.736546Z",
     "start_time": "2024-07-25T06:39:25.893439Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from neuralforecast import NeuralForecast\n",
    "from neuralforecast.models import NHITS, NBEATS, LSTM\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have defined train_df, test_df, and ml_data somewhere in your code\n",
    "\n",
    "# Define the parameter grids for manual fine-tuning\n",
    "nhits_param_grid = [\n",
    "    {'h': 30, 'input_size': 30, 'max_steps': 50, 'learning_rate': 0.001},\n",
    "    {'h': 30, 'input_size': 60, 'max_steps': 100, 'learning_rate': 0.01},\n",
    "    {'h': 30, 'input_size': 90, 'max_steps': 150, 'learning_rate': 0.1},\n",
    "]\n",
    "\n",
    "nbeats_param_grid = [\n",
    "    {'h': 30, 'input_size': 30, 'max_steps': 50, 'learning_rate': 0.001},\n",
    "    {'h': 30, 'input_size': 60, 'max_steps': 100, 'learning_rate': 0.01},\n",
    "    {'h': 30, 'input_size': 90, 'max_steps': 150, 'learning_rate': 0.1},\n",
    "]\n",
    "\n",
    "lstm_param_grid = [\n",
    "    {'h': 30, 'input_size': 30, 'max_steps': 50, 'learning_rate': 0.001, 'encoder_n_layers': 2, 'encoder_hidden_size': 200},\n",
    "    {'h': 30, 'input_size': 60, 'max_steps': 100, 'learning_rate': 0.01, 'encoder_n_layers': 2, 'encoder_hidden_size': 200},\n",
    "    {'h': 30, 'input_size': 90, 'max_steps': 150, 'learning_rate': 0.1, 'encoder_n_layers': 3, 'encoder_hidden_size': 300},\n",
    "]\n",
    "\n",
    "# Initialize the models\n",
    "nhits_model = NHITS(h=30, input_size=30)\n",
    "nbeats_model = NBEATS(h=30, input_size=30)\n",
    "lstm_model = LSTM(h=30, input_size=30)\n",
    "\n",
    "# Train and evaluate each model with different hyperparameters\n",
    "models = [nhits_model, nbeats_model, lstm_model]\n",
    "param_grids = [nhits_param_grid, nbeats_param_grid, lstm_param_grid]\n",
    "model_names = ['NHITS', 'NBEATS', 'LSTM']\n",
    "mape_scores = []\n",
    "mae_scores = []\n",
    "\n",
    "for model, param_grid, name in zip(models, param_grids, model_names):\n",
    "    for params in param_grid:\n",
    "        if name == 'NHITS':\n",
    "            model = NHITS(**params)\n",
    "        elif name == 'NBEATS':\n",
    "            model = NBEATS(**params)\n",
    "        elif name == 'LSTM':\n",
    "            model = LSTM(**params)\n",
    "            \n",
    "        nf = NeuralForecast(models=[model], freq='D')\n",
    "        nf.fit(train_df)\n",
    "        \n",
    "        # Make predictions for the test set\n",
    "        forecasts = nf.predict()\n",
    "        forecasts = forecasts.set_index('ds')\n",
    "        \n",
    "        # Align forecast with test_df to calculate MAE and MAPE\n",
    "        aligned_forecasts = forecasts[name].iloc[:len(test_df)].values\n",
    "        \n",
    "        mae = mean_absolute_error(test_df['y'], aligned_forecasts)\n",
    "        mape = mean_absolute_percentage_error(test_df['y'], aligned_forecasts)\n",
    "        \n",
    "        mae_scores.append((name, mae, str(params)))\n",
    "        mape_scores.append((name, mape, str(params)))\n",
    "        \n",
    "        print(f\"{name} - MAE: {mae:.4f}, MAPE: {mape:.4%}\")\n",
    "\n",
    "# Create a DataFrame to display the results\n",
    "results_df = pd.DataFrame(mae_scores, columns=['Model', 'MAE', 'Parameters']).merge(\n",
    "    pd.DataFrame(mape_scores, columns=['Model', 'MAPE', 'Parameters']), on=['Model', 'Parameters']\n",
    ")\n",
    "\n",
    "# Display the results DataFrame\n",
    "print(results_df)\n",
    "\n",
    "# Select the best model based on MAPE\n",
    "best_model_name, best_mape, best_params = min(mape_scores, key=lambda x: x[1])\n",
    "\n",
    "if best_model_name == 'NHITS':\n",
    "    best_model = NHITS(**eval(best_params))\n",
    "elif best_model_name == 'NBEATS':\n",
    "    best_model = NBEATS(**eval(best_params))\n",
    "elif best_model_name == 'LSTM':\n",
    "    best_model = LSTM(**eval(best_params))\n",
    "    \n",
    "print(f\"The best model is {best_model_name} with a MAPE of {best_mape:.4%} and best parameters: {best_params}\")\n",
    "\n",
    "# Train the best model on the entire dataset\n",
    "nf_best_model = NeuralForecast(models=[best_model], freq='D')\n",
    "nf_best_model.fit(ml_data)\n",
    "\n",
    "# Create a future dataframe for the next 30 days\n",
    "future_df = nf_best_model.make_future_dataframe()\n",
    "\n",
    "# Predict the next 30 days\n",
    "future_forecasts = nf_best_model.predict()\n",
    "future_forecasts = future_forecasts.set_index('ds')\n",
    "\n",
    "# Combine the historical data with the forecast data for plotting\n",
    "combined_df = pd.concat([ml_data, future_df], ignore_index=True)\n",
    "combined_df['forecast'] = np.nan\n",
    "combined_df.loc[ml_data.shape[0]:, 'forecast'] = future_forecasts[best_model_name].values\n",
    "\n",
    "# Plot actual vs predicted values for the historical period and future forecast\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Plot the entire dataset\n",
    "plt.plot(combined_df['ds'], combined_df['y'], label='Actual', marker='o')\n",
    "\n",
    "# Plot the future forecasts\n",
    "plt.plot(\n",
    "    combined_df['ds'],\n",
    "    combined_df['forecast'],\n",
    "    label=f'Predicted - {best_model_name} (Next 30 Days)',\n",
    "    marker='x',\n",
    ")\n",
    "\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.title(f'Actual vs Predicted Values using {best_model_name}')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"The best model is {best_model_name} with a MAPE of {best_mape:.4%} and best parameters: {best_params}\")\n"
   ],
   "id": "ccc44e7de7d3e891",
   "execution_count": 14,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Prophet",
   "id": "290c13cd01db8e25"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-25T07:03:03.747583Z",
     "start_time": "2024-07-25T07:01:04.245914Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import itertools\n",
    "from sklearn.metrics import mean_absolute_percentage_error, mean_absolute_error\n",
    "import pandas as pd\n",
    "from prophet import Prophet\n",
    "from prophet.diagnostics import cross_validation, performance_metrics\n",
    "import holidays\n",
    "\n",
    "# Function to calculate sMAPE\n",
    "def smape(a, f):\n",
    "    return 100 * np.mean(2 * np.abs(f - a) / (np.abs(a) + np.abs(f)))\n",
    "\n",
    "# Assuming data is already defined with 'ds' and 'y' columns\n",
    "# Add is_public_holiday column\n",
    "holiday = holidays.CountryHoliday('UK')\n",
    "data['is_public_holiday'] = data['ds'].apply(lambda date: 1 if date in holiday else 0)\n",
    "\n",
    "# Define hyperparameters to tune\n",
    "param_grid = {\n",
    "    'changepoint_prior_scale': [0.01, 0.1, 0.5],\n",
    "    'seasonality_prior_scale': [0.1, 1.0, 10.0],\n",
    "    'holidays_prior_scale': [0.1, 1.0, 10.0],\n",
    "    'seasonality_mode': ['additive', 'multiplicative']\n",
    "}\n",
    "\n",
    "# Generate all combinations of parameters\n",
    "all_params = [dict(zip(param_grid.keys(), v)) for v in itertools.product(*param_grid.values())]\n",
    "\n",
    "best_params = None\n",
    "best_mape = float('inf')\n",
    "best_mae = float('inf')\n",
    "\n",
    "# Grid search to find the best hyperparameters\n",
    "for params in all_params:\n",
    "    model = Prophet(**params)\n",
    "    model.add_regressor('is_public_holiday')\n",
    "    model.fit(data)\n",
    "\n",
    "    # Cross-validate the model\n",
    "    df_cv = cross_validation(model, initial='547 days', period='180 days', horizon='30 days')\n",
    "    df_p = performance_metrics(df_cv)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mape = mean_absolute_percentage_error(df_cv['y'], df_cv['yhat'])\n",
    "    mae = mean_absolute_error(df_cv['y'], df_cv['yhat'])\n",
    "    \n",
    "    if mape < best_mape:\n",
    "        best_mape = mape\n",
    "        best_mae = mae\n",
    "        best_params = params\n",
    "\n",
    "# Output the best parameters and metrics\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "print(f\"Best MAPE: {round(best_mape, 2)}\")\n",
    "print(f\"Best MAE: {round(best_mae, 2)}\")\n",
    "\n",
    "# Train the final model with the best parameters\n",
    "model = Prophet(**best_params)\n",
    "model.add_regressor('is_public_holiday')\n",
    "model.fit(data)\n",
    "\n",
    "# Forecast future values for the next 30 days\n",
    "future = model.make_future_dataframe(periods=30)\n",
    "future['is_public_holiday'] = future['ds'].apply(lambda date: 1 if date in holiday else 0)\n",
    "\n",
    "forecast = model.predict(future)\n",
    "\n",
    "# Visualize Results\n",
    "model.plot(forecast)\n",
    "model.plot_components(forecast)\n"
   ],
   "id": "ffd6ab92304e2716",
   "execution_count": 15,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-25T07:25:49.643672Z",
     "start_time": "2024-07-25T07:22:35.419487Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import itertools\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "import pandas as pd\n",
    "from prophet import Prophet\n",
    "from prophet.diagnostics import cross_validation, performance_metrics\n",
    "from prophet.plot import plot_cross_validation_metric\n",
    "import holidays\n",
    "\n",
    "\n",
    "# Define hyperparameters to tune\n",
    "param_grid = {  \n",
    "    'changepoint_prior_scale': [0.01, 0.1, 0.5],\n",
    "    'seasonality_prior_scale': [0.1, 1.0, 10.0],\n",
    "    'holidays_prior_scale': [0.1, 1.0, 10.0],\n",
    "    'seasonality_mode': ['additive', 'multiplicative']\n",
    "}\n",
    "\n",
    "# Generate all combinations of parameters\n",
    "all_params = [dict(zip(param_grid.keys(), v)) for v in itertools.product(*param_grid.values())]\n",
    "\n",
    "best_params = None\n",
    "best_mape = float('inf')\n",
    "\n",
    "# Grid search to find the best hyperparameters\n",
    "for params in all_params:\n",
    "    model = Prophet(**params)\n",
    "    model.add_regressor('is_public_holiday')\n",
    "    model.fit(data)\n",
    "\n",
    "    # Cross-validate the model\n",
    "    df_cv = cross_validation(model, initial='547 days', period='180 days', horizon='30 days')\n",
    "    df_p = performance_metrics(df_cv)\n",
    "    \n",
    "    # Calculate MAPE\n",
    "    mape = mean_absolute_percentage_error(df_cv['y'], df_cv['yhat'])\n",
    "    \n",
    "    if mape < best_mape:\n",
    "        best_mape = mape\n",
    "        best_params = params\n",
    "\n",
    "# Output the best parameters and MAPE\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "print(f\"Best MAPE: {round(best_mape, 2)}\")\n",
    "\n",
    "# Train the final model with the best parameters\n",
    "model = Prophet(**best_params)\n",
    "model.add_regressor('is_public_holiday')\n",
    "model.fit(data)\n",
    "\n",
    "# Forecast future values\n",
    "future = model.make_future_dataframe(periods=365)\n",
    "future['is_public_holiday'] = future['ds'].apply(\n",
    "    lambda date: 1 if date in holiday else 0\n",
    ")\n",
    "\n",
    "forecast = model.predict(future)\n",
    "\n",
    "# Visualize Results\n",
    "model.plot(forecast)\n",
    "model.plot_components(forecast)\n",
    "\n",
    "# Evaluate Accuracy\n",
    "df_cv = cross_validation(model, initial='547 days', period='180 days', horizon='30 days')\n",
    "df_p = performance_metrics(df_cv)\n",
    "print(df_p.head().round(2))\n",
    "\n",
    "fig = plot_cross_validation_metric(df_cv, metric='rmse')\n",
    "\n",
    "# Calculate MAPE using yhat and y\n",
    "mape = mean_absolute_percentage_error(df_cv['y'], df_cv['yhat'])\n",
    "print(f\"MAPE: {round(mape, 2)}\")\n"
   ],
   "id": "af05a353042112da",
   "execution_count": 17,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "",
   "id": "2ca823d100c8bf3d",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

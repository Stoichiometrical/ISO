{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Model Training",
   "id": "60a718cf6189e1c0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T22:37:06.702467Z",
     "start_time": "2024-08-01T22:37:05.417200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import itertools\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "import pandas as pd\n",
    "from prophet import Prophet\n",
    "from prophet.diagnostics import cross_validation, performance_metrics\n",
    "from prophet.plot import plot_cross_validation_metric\n",
    "import holidays\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gr\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import holidays"
   ],
   "id": "7c410ca6a3f91352",
   "execution_count": 1,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T22:37:06.795574Z",
     "start_time": "2024-08-01T22:37:06.705563Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df=pd.read_csv(\"processed_full.csv\")\n",
    "df.head()"
   ],
   "id": "24951986fd06d4eb",
   "execution_count": 2,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T22:37:06.886356Z",
     "start_time": "2024-08-01T22:37:06.829999Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df.index =df[\"date\"]\n",
    "df.head(2)"
   ],
   "id": "b5ae272f8fc83f7e",
   "execution_count": 3,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T22:37:06.935350Z",
     "start_time": "2024-08-01T22:37:06.900553Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = df.copy()\n",
    "data[\"ds\"] = df.index\n",
    "data[\"y\"] = df[\"quantity_winsorized\"]\n",
    "data['ds'] = pd.to_datetime(data['ds'])\n",
    "data.drop(columns=['date'],inplace=True)"
   ],
   "id": "92e99f1d81f1ec74",
   "execution_count": 4,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Machine Learning Models",
   "id": "79ecdeb6497faf06"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T20:57:30.658055Z",
     "start_time": "2024-08-01T20:57:25.338448Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error\n",
    "import matplotlib.pyplot as plt\n",
    "import holidays\n",
    "\n",
    "# Make a copy of the dataframe\n",
    "rml_data = data.copy()\n",
    "\n",
    "# Ensure 'ds' column is datetime\n",
    "rml_data['ds'] = pd.to_datetime(rml_data['ds'])\n",
    "\n",
    "# Define features and target\n",
    "features = ['month', 'day_of_month', 'is_month_start', 'is_month_end',\n",
    "            'day_of_year', 'week_of_year', 'day_of_week', 'year',\n",
    "            'is_weekend', 'is_spring', 'is_summer', 'is_fall', 'is_winter',\n",
    "            'sin_day', 'cos_day', 'is_public_holiday', 'lag_1', 'lag_2', \n",
    "            'lag_3', 'lag_4', 'lag_5', 'lag_6', 'lag_7', 'rolling_mean_7', \n",
    "            'rolling_sum_7', 'rolling_std_7', 'rolling_mean_30', \n",
    "            'rolling_sum_30', 'rolling_std_30', 'expanding_sum']\n",
    "\n",
    "target = 'y'\n",
    "\n",
    "# Split the data into train and test sets\n",
    "train_df = rml_data[:-60]\n",
    "test_df = rml_data[-60:]\n",
    "\n",
    "X_train, y_train = train_df[features], train_df[target]\n",
    "X_test, y_test = test_df[features], test_df[target]\n",
    "\n",
    "# Define the models\n",
    "models = {\n",
    "    'LightGBM': lgb.LGBMRegressor(n_estimators=100, verbosity=-1),\n",
    "    'XGBoost': XGBRegressor(n_estimators=100, verbosity=0),\n",
    "    'RandomForest': RandomForestRegressor(n_estimators=100)\n",
    "}\n",
    "\n",
    "# Initialize lists to store the scores\n",
    "mae_scores = {}\n",
    "mape_scores = {}\n",
    "\n",
    "# Train and evaluate each model\n",
    "for model_name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "    \n",
    "    mae_scores[model_name] = mae\n",
    "    mape_scores[model_name] = mape\n",
    "    \n",
    "    print(f\"{model_name} - MAE: {mae:.4f}, MAPE: {mape:.4%}\")\n",
    "\n",
    "# Select the best model based on MAPE\n",
    "best_model_name = min(mape_scores, key=mape_scores.get)\n",
    "best_model = models[best_model_name]\n",
    "print(f\"\\nThe best model is {best_model_name} with a MAPE of {mape_scores[best_model_name]:.4%}\")\n",
    "\n",
    "# Retrain the best model on the entire dataset\n",
    "best_model.fit(rml_data[features], rml_data[target])\n",
    "\n",
    "# Predict the next 60 days\n",
    "future_dates = pd.date_range(start=rml_data['ds'].max() + pd.Timedelta(days=1), periods=60, freq='D')\n",
    "future_df = pd.DataFrame({\n",
    "    'ds': future_dates,\n",
    "    'month': future_dates.month,\n",
    "    'day_of_month': future_dates.day,\n",
    "    'is_month_start': future_dates.is_month_start.astype(int),\n",
    "    'is_month_end': future_dates.is_month_end.astype(int),\n",
    "    'day_of_year': future_dates.dayofyear,\n",
    "    'week_of_year': future_dates.isocalendar().week,\n",
    "    'day_of_week': future_dates.dayofweek + 1,\n",
    "    'year': future_dates.year,\n",
    "    'is_weekend': (future_dates.weekday >= 5).astype(int),\n",
    "    'is_spring': future_dates.month.isin([3, 4, 5]).astype(int),\n",
    "    'is_summer': future_dates.month.isin([6, 7, 8]).astype(int),\n",
    "    'is_fall': future_dates.month.isin([9, 10, 11]).astype(int),\n",
    "    'is_winter': future_dates.month.isin([12, 1, 2]).astype(int),\n",
    "    'sin_day': np.sin(2 * np.pi * future_dates.dayofweek / 7),\n",
    "    'cos_day': np.cos(2 * np.pi * future_dates.dayofweek / 7)\n",
    "})\n",
    "\n",
    "# Add public holidays for the future dates\n",
    "uk_holidays = holidays.UK()\n",
    "future_df['is_public_holiday'] = future_df['ds'].apply(lambda date: 1 if date in uk_holidays else 0)\n",
    "\n",
    "# Add lag features and rolling statistics for the future_df\n",
    "for lag in range(1, 8):\n",
    "    future_df[f'lag_{lag}'] = rml_data[target].shift(lag).iloc[-60:].values\n",
    "\n",
    "for window in [7, 30]:\n",
    "    future_df[f'rolling_mean_{window}'] = rml_data[target].rolling(window=window).mean().iloc[-60:].values\n",
    "    future_df[f'rolling_sum_{window}'] = rml_data[target].rolling(window=window).sum().iloc[-60:].values\n",
    "    future_df[f'rolling_std_{window}'] = rml_data[target].rolling(window=window).std().iloc[-60:].values\n",
    "\n",
    "future_df['expanding_sum'] = rml_data[target].expanding().sum().iloc[-60:].values\n",
    "\n",
    "# Predict future values using the best model\n",
    "future_predictions = best_model.predict(future_df[features])\n",
    "\n",
    "# Plot actual vs predicted values\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(test_df['ds'], test_df['y'], label='Actual', marker='o')\n",
    "plt.plot(future_df['ds'], future_predictions, label=f'Predicted - {best_model_name}', marker='x')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Actual vs Predicted Values')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ],
   "id": "5a551823b2bc8b76",
   "execution_count": 7,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Reduce forecast horizon to 30 days",
   "id": "50f8dc19f4d95aad"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T20:58:29.187600Z",
     "start_time": "2024-08-01T20:58:23.280459Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error\n",
    "import matplotlib.pyplot as plt\n",
    "import holidays\n",
    "\n",
    "# Make a copy of the dataframe\n",
    "rml_data = data.copy()\n",
    "\n",
    "# Ensure 'ds' column is datetime\n",
    "rml_data['ds'] = pd.to_datetime(rml_data['ds'])\n",
    "\n",
    "# Define features and target\n",
    "features = ['month', 'day_of_month', 'is_month_start', 'is_month_end',\n",
    "            'day_of_year', 'week_of_year', 'day_of_week', 'year',\n",
    "            'is_weekend', 'is_spring', 'is_summer', 'is_fall', 'is_winter',\n",
    "            'sin_day', 'cos_day', 'is_public_holiday', 'lag_1', 'lag_2', \n",
    "            'lag_3', 'lag_4', 'lag_5', 'lag_6', 'lag_7', 'rolling_mean_7', \n",
    "            'rolling_sum_7', 'rolling_std_7', 'rolling_mean_30', \n",
    "            'rolling_sum_30', 'rolling_std_30', 'expanding_sum']\n",
    "\n",
    "target = 'y'\n",
    "\n",
    "# Split the data into train and test sets\n",
    "train_df = rml_data[:-30]\n",
    "test_df = rml_data[-30:]\n",
    "\n",
    "X_train, y_train = train_df[features], train_df[target]\n",
    "X_test, y_test = test_df[features], test_df[target]\n",
    "\n",
    "# Define the models\n",
    "models = {\n",
    "    'LightGBM': lgb.LGBMRegressor(n_estimators=100, verbosity=-1),\n",
    "    'XGBoost': XGBRegressor(n_estimators=100, verbosity=0),\n",
    "    'RandomForest': RandomForestRegressor(n_estimators=100)\n",
    "}\n",
    "\n",
    "# Initialize lists to store the scores\n",
    "mae_scores = {}\n",
    "mape_scores = {}\n",
    "\n",
    "# Train and evaluate each model\n",
    "for model_name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "    \n",
    "    mae_scores[model_name] = mae\n",
    "    mape_scores[model_name] = mape\n",
    "    \n",
    "    print(f\"{model_name} - MAE: {mae:.4f}, MAPE: {mape:.4%}\")\n",
    "\n",
    "# Select the best model based on MAPE\n",
    "best_model_name = min(mape_scores, key=mape_scores.get)\n",
    "best_model = models[best_model_name]\n",
    "print(f\"\\nThe best model is {best_model_name} with a MAPE of {mape_scores[best_model_name]:.4%}\")\n",
    "\n",
    "# Retrain the best model on the entire dataset\n",
    "best_model.fit(rml_data[features], rml_data[target])\n",
    "\n",
    "# Predict the next 30 days\n",
    "future_dates = pd.date_range(start=rml_data['ds'].max() + pd.Timedelta(days=1), periods=30, freq='D')\n",
    "future_df = pd.DataFrame({\n",
    "    'ds': future_dates,\n",
    "    'month': future_dates.month,\n",
    "    'day_of_month': future_dates.day,\n",
    "    'is_month_start': future_dates.is_month_start.astype(int),\n",
    "    'is_month_end': future_dates.is_month_end.astype(int),\n",
    "    'day_of_year': future_dates.dayofyear,\n",
    "    'week_of_year': future_dates.isocalendar().week,\n",
    "    'day_of_week': future_dates.dayofweek + 1,\n",
    "    'year': future_dates.year,\n",
    "    'is_weekend': (future_dates.weekday >= 5).astype(int),\n",
    "    'is_spring': future_dates.month.isin([3, 4, 5]).astype(int),\n",
    "    'is_summer': future_dates.month.isin([6, 7, 8]).astype(int),\n",
    "    'is_fall': future_dates.month.isin([9, 10, 11]).astype(int),\n",
    "    'is_winter': future_dates.month.isin([12, 1, 2]).astype(int),\n",
    "    'sin_day': np.sin(2 * np.pi * future_dates.dayofweek / 7),\n",
    "    'cos_day': np.cos(2 * np.pi * future_dates.dayofweek / 7)\n",
    "})\n",
    "\n",
    "# Add public holidays for the future dates\n",
    "uk_holidays = holidays.UK()\n",
    "future_df['is_public_holiday'] = future_df['ds'].apply(lambda date: 1 if date in uk_holidays else 0)\n",
    "\n",
    "# Add lag features and rolling statistics for the future_df\n",
    "for lag in range(1, 8):\n",
    "    future_df[f'lag_{lag}'] = rml_data[target].shift(lag).iloc[-30:].values\n",
    "\n",
    "for window in [7, 30]:\n",
    "    future_df[f'rolling_mean_{window}'] = rml_data[target].rolling(window=window).mean().iloc[-30:].values\n",
    "    future_df[f'rolling_sum_{window}'] = rml_data[target].rolling(window=window).sum().iloc[-30:].values\n",
    "    future_df[f'rolling_std_{window}'] = rml_data[target].rolling(window=window).std().iloc[-30:].values\n",
    "\n",
    "future_df['expanding_sum'] = rml_data[target].expanding().sum().iloc[-30:].values\n",
    "\n",
    "# Predict future values using the best model\n",
    "future_predictions = best_model.predict(future_df[features])\n",
    "\n",
    "# Plot actual vs predicted values\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(test_df['ds'], test_df['y'], label='Actual', marker='o')\n",
    "plt.plot(future_df['ds'], future_predictions, label=f'Predicted - {best_model_name}', marker='x')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Actual vs Predicted Values')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ],
   "id": "388e5f12fc21c3a7",
   "execution_count": 8,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Add fine tuning",
   "id": "77a35a9005dadd0f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T20:59:59.210191Z",
     "start_time": "2024-08-01T20:59:27.908723Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "import holidays\n",
    "\n",
    "# Make a copy of the dataframe\n",
    "rml_data = data.copy()\n",
    "\n",
    "# Ensure 'ds' column is datetime\n",
    "rml_data['ds'] = pd.to_datetime(rml_data['ds'])\n",
    "\n",
    "# Define features and target\n",
    "features = ['month', 'day_of_month', 'is_month_start', 'is_month_end',\n",
    "            'day_of_year', 'week_of_year', 'day_of_week', 'year',\n",
    "            'is_weekend', 'is_spring', 'is_summer', 'is_fall', 'is_winter',\n",
    "            'sin_day', 'cos_day', 'is_public_holiday', 'lag_1', 'lag_2', \n",
    "            'lag_3', 'lag_4', 'lag_5', 'lag_6', 'lag_7', 'rolling_mean_7', \n",
    "            'rolling_sum_7', 'rolling_std_7', 'rolling_mean_30', \n",
    "            'rolling_sum_30', 'rolling_std_30', 'expanding_sum']\n",
    "\n",
    "target = 'y'\n",
    "\n",
    "# Split the data into train and test sets\n",
    "train_df = rml_data[:-30]\n",
    "test_df = rml_data[-30:]\n",
    "\n",
    "X_train, y_train = train_df[features], train_df[target]\n",
    "X_test, y_test = test_df[features], test_df[target]\n",
    "\n",
    "# Define the models and hyperparameters for tuning\n",
    "models = {\n",
    "    'LightGBM': (lgb.LGBMRegressor(), {\n",
    "        'n_estimators': [100, 200],\n",
    "        'learning_rate': [0.01, 0.1],\n",
    "        'num_leaves': [31, 127]\n",
    "    }),\n",
    "    'XGBoost': (XGBRegressor(objective='reg:squarederror'), {\n",
    "        'n_estimators': [100, 200],\n",
    "        'learning_rate': [0.01, 0.1],\n",
    "        'max_depth': [3, 6]\n",
    "    }),\n",
    "    'RandomForest': (RandomForestRegressor(), {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_features': ['auto', 'sqrt'],\n",
    "        'max_depth': [10, 20]\n",
    "    })\n",
    "}\n",
    "\n",
    "# Initialize lists to store the scores\n",
    "mae_scores = {}\n",
    "mape_scores = {}\n",
    "\n",
    "# Train and evaluate each model\n",
    "for model_name, (model, params) in models.items():\n",
    "    grid_search = GridSearchCV(model, params, cv=5, scoring='neg_mean_absolute_percentage_error', n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    best_model = grid_search.best_estimator_\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    \n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "    \n",
    "    mae_scores[model_name] = mae\n",
    "    mape_scores[model_name] = mape\n",
    "    \n",
    "    print(f\"{model_name} - MAE: {mae:.4f}, MAPE: {mape:.4%}\")\n",
    "\n",
    "# Select the best model based on MAPE\n",
    "best_model_name = min(mape_scores, key=mape_scores.get)\n",
    "best_model = models[best_model_name][0]\n",
    "print(f\"\\nThe best model is {best_model_name} with a MAPE of {mape_scores[best_model_name]:.4%}\")\n",
    "\n",
    "# Retrain the best model on the entire dataset\n",
    "best_model.fit(rml_data[features], rml_data[target])\n",
    "\n",
    "# Predict the next 30 days\n",
    "future_dates = pd.date_range(start=rml_data['ds'].max() + pd.Timedelta(days=1), periods=30, freq='D')\n",
    "future_df = pd.DataFrame({\n",
    "    'ds': future_dates,\n",
    "    'month': future_dates.month,\n",
    "    'day_of_month': future_dates.day,\n",
    "    'is_month_start': future_dates.is_month_start.astype(int),\n",
    "    'is_month_end': future_dates.is_month_end.astype(int),\n",
    "    'day_of_year': future_dates.dayofyear,\n",
    "    'week_of_year': future_dates.isocalendar().week,\n",
    "    'day_of_week': future_dates.dayofweek + 1,\n",
    "    'year': future_dates.year,\n",
    "    'is_weekend': (future_dates.weekday >= 5).astype(int),\n",
    "    'is_spring': future_dates.month.isin([3, 4, 5]).astype(int),\n",
    "    'is_summer': future_dates.month.isin([6, 7, 8]).astype(int),\n",
    "    'is_fall': future_dates.month.isin([9, 10, 11]).astype(int),\n",
    "    'is_winter': future_dates.month.isin([12, 1, 2]).astype(int),\n",
    "    'sin_day': np.sin(2 * np.pi * future_dates.dayofweek / 7),\n",
    "    'cos_day': np.cos(2 * np.pi * future_dates.dayofweek / 7)\n",
    "})\n",
    "\n",
    "# Add public holidays for the future dates\n",
    "uk_holidays = holidays.UK()\n",
    "future_df['is_public_holiday'] = future_df['ds'].apply(lambda date: 1 if date in uk_holidays else 0)\n",
    "\n",
    "# Add lag features and rolling statistics for the future_df\n",
    "for lag in range(1, 8):\n",
    "    future_df[f'lag_{lag}'] = rml_data[target].shift(lag).iloc[-30:].values\n",
    "\n",
    "for window in [7, 30]:\n",
    "    future_df[f'rolling_mean_{window}'] = rml_data[target].rolling(window=window).mean().iloc[-30:].values\n",
    "    future_df[f'rolling_sum_{window}'] = rml_data[target].rolling(window=window).sum().iloc[-30:].values\n",
    "    future_df[f'rolling_std_{window}'] = rml_data[target].rolling(window=window).std().iloc[-30:].values\n",
    "\n",
    "future_df['expanding_sum'] = rml_data[target].expanding().sum().iloc[-30:].values\n",
    "\n",
    "# Predict future values using the best model\n",
    "future_predictions = best_model.predict(future_df[features])\n",
    "\n",
    "# Plot actual vs predicted values\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(test_df['ds'], test_df['y'], label='Actual', marker='o')\n",
    "plt.plot(future_df['ds'], future_predictions, label=f'Predicted - {best_model_name}', marker='x')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Actual vs Predicted Values')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ],
   "id": "c6c2f5cbd5634615",
   "execution_count": 9,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T22:44:05.672840Z",
     "start_time": "2024-08-01T22:43:38.549462Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "import holidays\n",
    "\n",
    "# Make a copy of the dataframe\n",
    "rml_data = data.copy()\n",
    "\n",
    "# Ensure 'ds' column is datetime\n",
    "rml_data['ds'] = pd.to_datetime(rml_data['ds'])\n",
    "\n",
    "# Define features and target\n",
    "features = ['month', 'day_of_month', 'is_month_start', 'is_month_end',\n",
    "            'day_of_year', 'week_of_year', 'day_of_week', 'year',\n",
    "            'is_weekend', 'is_spring', 'is_summer', 'is_fall', 'is_winter',\n",
    "            'sin_day', 'cos_day', 'is_public_holiday', 'lag_1', 'lag_2', \n",
    "            'lag_3', 'lag_4', 'lag_5', 'lag_6', 'lag_7', 'rolling_mean_7', \n",
    "            'rolling_sum_7', 'rolling_std_7', 'rolling_mean_30', \n",
    "            'rolling_sum_30', 'rolling_std_30', 'expanding_sum']\n",
    "\n",
    "target = 'y'\n",
    "\n",
    "# Split the data into train and test sets\n",
    "train_df = rml_data[:-30]\n",
    "test_df = rml_data[-30:]\n",
    "\n",
    "X_train, y_train = train_df[features], train_df[target]\n",
    "X_test, y_test = test_df[features], test_df[target]\n",
    "\n",
    "# Define the models and hyperparameters for tuning\n",
    "models = {\n",
    "    'LightGBM': (lgb.LGBMRegressor(), {\n",
    "        'n_estimators': [100, 200],\n",
    "        'learning_rate': [0.01, 0.1],\n",
    "        'num_leaves': [31, 127]\n",
    "    }),\n",
    "    'XGBoost': (XGBRegressor(objective='reg:squarederror'), {\n",
    "        'n_estimators': [100, 200],\n",
    "        'learning_rate': [0.01, 0.1],\n",
    "        'max_depth': [3, 6]\n",
    "    }),\n",
    "    'RandomForest': (RandomForestRegressor(), {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_features': ['auto', 'sqrt'],\n",
    "        'max_depth': [10, 20]\n",
    "    })\n",
    "}\n",
    "\n",
    "# Initialize lists to store the scores and best parameters\n",
    "mae_scores = {}\n",
    "mape_scores = {}\n",
    "best_params_dict = {}\n",
    "\n",
    "# Train and evaluate each model\n",
    "for model_name, (model, params) in models.items():\n",
    "    grid_search = GridSearchCV(model, params, cv=5, scoring='neg_mean_absolute_percentage_error', n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    best_model = grid_search.best_estimator_\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    \n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "    \n",
    "    mae_scores[model_name] = mae\n",
    "    mape_scores[model_name] = mape\n",
    "    best_params_dict[model_name] = grid_search.best_params_\n",
    "    \n",
    "    print(f\"{model_name} - MAE: {mae:.4f}, MAPE: {mape:.4%}\")\n",
    "\n",
    "# Select the best model based on MAPE\n",
    "best_model_name = min(mape_scores, key=mape_scores.get)\n",
    "best_model = models[best_model_name][0]\n",
    "best_params = best_params_dict[best_model_name]\n",
    "\n",
    "print(f\"\\nThe best model is {best_model_name} with a MAPE of {mape_scores[best_model_name]:.4%}\")\n",
    "print(f\"Best parameters for {best_model_name}: {best_params}\")\n",
    "\n",
    "# Retrain the best model on the entire dataset\n",
    "best_model.set_params(**best_params)\n",
    "best_model.fit(rml_data[features], rml_data[target])\n",
    "\n",
    "# Predict for the entire historical period\n",
    "historical_predictions = best_model.predict(rml_data[features])\n",
    "\n",
    "# Predict the next 30 days\n",
    "future_dates = pd.date_range(start=rml_data['ds'].max() + pd.Timedelta(days=1), periods=30, freq='D')\n",
    "future_df = pd.DataFrame({\n",
    "    'ds': future_dates,\n",
    "    'month': future_dates.month,\n",
    "    'day_of_month': future_dates.day,\n",
    "    'is_month_start': future_dates.is_month_start.astype(int),\n",
    "    'is_month_end': future_dates.is_month_end.astype(int),\n",
    "    'day_of_year': future_dates.dayofyear,\n",
    "    'week_of_year': future_dates.isocalendar().week,\n",
    "    'day_of_week': future_dates.dayofweek + 1,\n",
    "    'year': future_dates.year,\n",
    "    'is_weekend': (future_dates.weekday >= 5).astype(int),\n",
    "    'is_spring': future_dates.month.isin([3, 4, 5]).astype(int),\n",
    "    'is_summer': future_dates.month.isin([6, 7, 8]).astype(int),\n",
    "    'is_fall': future_dates.month.isin([9, 10, 11]).astype(int),\n",
    "    'is_winter': future_dates.month.isin([12, 1, 2]).astype(int),\n",
    "    'sin_day': np.sin(2 * np.pi * future_dates.dayofweek / 7),\n",
    "    'cos_day': np.cos(2 * np.pi * future_dates.dayofweek / 7)\n",
    "})\n",
    "\n",
    "# Add public holidays for the future dates\n",
    "uk_holidays = holidays.UK()\n",
    "future_df['is_public_holiday'] = future_df['ds'].apply(lambda date: 1 if date in uk_holidays else 0)\n",
    "\n",
    "# Add lag features and rolling statistics for the future_df\n",
    "for lag in range(1, 8):\n",
    "    future_df[f'lag_{lag}'] = rml_data[target].shift(lag).iloc[-30:].values\n",
    "\n",
    "for window in [7, 30]:\n",
    "    future_df[f'rolling_mean_{window}'] = rml_data[target].rolling(window=window).mean().iloc[-30:].values\n",
    "    future_df[f'rolling_sum_{window}'] = rml_data[target].rolling(window=window).sum().iloc[-30:].values\n",
    "    future_df[f'rolling_std_{window}'] = rml_data[target].rolling(window=window).std().iloc[-30:].values\n",
    "\n",
    "future_df['expanding_sum'] = rml_data[target].expanding().sum().iloc[-30:].values\n",
    "\n",
    "# Predict future values using the best model\n",
    "future_predictions = best_model.predict(future_df[features])\n",
    "\n",
    "# Combine the historical and future data for plotting\n",
    "combined_df = pd.concat([rml_data, future_df], ignore_index=True)\n",
    "combined_df['forecast'] = np.nan\n",
    "combined_df.loc[:len(historical_predictions)-1, 'forecast'] = historical_predictions\n",
    "combined_df.loc[len(historical_predictions):, 'forecast'] = future_predictions\n",
    "\n",
    "# Plot actual vs predicted values for the entire period\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(combined_df['ds'], combined_df['y'], label='Actual')\n",
    "plt.plot(combined_df['ds'], combined_df['forecast'], label=f'Predicted - {best_model_name}')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Actual vs Predicted Values')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ],
   "id": "d214a8b504611d20",
   "execution_count": 8,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Deep Learning Models",
   "id": "cec4fde5c25d7b58"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T21:07:25.690639Z",
     "start_time": "2024-08-01T21:06:04.722019Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from neuralforecast import NeuralForecast\n",
    "from neuralforecast.models import NHITS, NBEATS, LSTM\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Make a copy of the dataframe\n",
    "rml_data = data.copy()\n",
    "\n",
    "# Ensure 'ds' column is datetime\n",
    "rml_data['ds'] = pd.to_datetime(rml_data['ds'])\n",
    "\n",
    "# Add a unique_id column\n",
    "rml_data['unique_id'] = 'series_1'\n",
    "\n",
    "# Split the data into train and test sets\n",
    "train_df = rml_data[:-60]\n",
    "test_df = rml_data[-60:]\n",
    "\n",
    "# Define the models and their respective parameters\n",
    "nhits_params = {\n",
    "    'h': 60,\n",
    "    'input_size': 30,\n",
    "    'max_steps': 50\n",
    "}\n",
    "\n",
    "nbeats_params = {\n",
    "    'h': 60,\n",
    "    'input_size': 30,\n",
    "    'max_steps': 50\n",
    "}\n",
    "\n",
    "lstm_params = {\n",
    "    'h': 60,\n",
    "    'input_size': 30,\n",
    "    'max_steps': 50\n",
    "}\n",
    "\n",
    "# Initialize the models\n",
    "nhits_model = NHITS(**nhits_params)\n",
    "nbeats_model = NBEATS(**nbeats_params)\n",
    "lstm_model = LSTM(**lstm_params)\n",
    "\n",
    "# Train each model on the training data and evaluate on test data\n",
    "models = [nhits_model, nbeats_model, lstm_model]\n",
    "model_names = ['NHITS', 'NBEATS', 'LSTM']\n",
    "mape_scores = []\n",
    "\n",
    "for model, name in zip(models, model_names):\n",
    "    nf = NeuralForecast(models=[model], freq='D')\n",
    "    nf.fit(df=train_df, id_col='unique_id', time_col='ds', target_col='y')\n",
    "    forecasts = nf.predict(futr_df=test_df)\n",
    "    print(forecasts.head())\n",
    "    mape = mean_absolute_percentage_error(test_df['y'], forecasts[name])\n",
    "    mape_scores.append((name, mape))\n",
    "\n",
    "# Select the best model based on MAPE\n",
    "best_model_name, best_mape = min(mape_scores, key=lambda x: x[1])\n",
    "best_model = None\n",
    "\n",
    "if best_model_name == 'NHITS':\n",
    "    best_model = NHITS(**nhits_params)\n",
    "elif best_model_name == 'NBEATS':\n",
    "    best_model = NBEATS(**nbeats_params)\n",
    "elif best_model_name == 'LSTM':\n",
    "    best_model = LSTM(**lstm_params)\n",
    "\n",
    "# Train the best model on the entire dataset\n",
    "nf_best_model = NeuralForecast(models=[best_model], freq='D')\n",
    "nf_best_model.fit(df=rml_data, id_col='unique_id', time_col='ds', target_col='y')\n",
    "\n",
    "# Predict the next 60 days with the best model\n",
    "future_dates = pd.date_range(rml_data['ds'].max() + pd.Timedelta(days=1), periods=60, freq='D')\n",
    "future_df = pd.DataFrame({'ds': future_dates})\n",
    "future_df['unique_id'] = 'series_1'\n",
    "\n",
    "final_forecasts = nf_best_model.predict(futr_df=future_df)\n",
    "\n",
    "# Plot actual vs predicted values for the best model\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Plot the test data\n",
    "plt.plot(test_df['ds'], test_df['y'], label='Actual', marker='o')\n",
    "\n",
    "# Plot the forecasts\n",
    "plt.plot(\n",
    "    future_dates,\n",
    "    final_forecasts[best_model_name][:60],  # Ensure we take only 60 predictions\n",
    "    label=f'Predicted - {best_model_name}',\n",
    "    marker='x',\n",
    ")\n",
    "\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.title(f'Actual vs Predicted Values using {best_model_name}')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"The best model is {best_model_name} with a MAPE of {best_mape}\")\n"
   ],
   "id": "48cab7f0e70fe7a2",
   "execution_count": 13,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Reduce forecast horizon to 30 days",
   "id": "981631b69e4179e1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T21:09:39.413460Z",
     "start_time": "2024-08-01T21:08:37.181438Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from neuralforecast import NeuralForecast\n",
    "from neuralforecast.models import NHITS, NBEATS, LSTM\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Make a copy of the dataframe\n",
    "rml_data = data.copy()\n",
    "\n",
    "# Ensure 'ds' column is datetime\n",
    "rml_data['ds'] = pd.to_datetime(rml_data['ds'])\n",
    "\n",
    "# Add a unique_id column\n",
    "rml_data['unique_id'] = 'series_1'\n",
    "\n",
    "# Split the data into train and test sets\n",
    "train_df = rml_data[:-30]\n",
    "test_df = rml_data[-30:]\n",
    "\n",
    "# Define the models and their respective parameters\n",
    "nhits_params = {\n",
    "    'h': 30,\n",
    "    'input_size': 30,\n",
    "    'max_steps': 50\n",
    "}\n",
    "\n",
    "nbeats_params = {\n",
    "    'h': 30,\n",
    "    'input_size': 30,\n",
    "    'max_steps': 50\n",
    "}\n",
    "\n",
    "lstm_params = {\n",
    "    'h': 30,\n",
    "    'input_size': 30,\n",
    "    'max_steps': 50\n",
    "}\n",
    "\n",
    "# Initialize the models\n",
    "nhits_model = NHITS(**nhits_params)\n",
    "nbeats_model = NBEATS(**nbeats_params)\n",
    "lstm_model = LSTM(**lstm_params)\n",
    "\n",
    "# Train each model on the training data and evaluate on test data\n",
    "models = [nhits_model, nbeats_model, lstm_model]\n",
    "model_names = ['NHITS', 'NBEATS', 'LSTM']\n",
    "mape_scores = []\n",
    "\n",
    "for model, name in zip(models, model_names):\n",
    "    nf = NeuralForecast(models=[model], freq='D')\n",
    "    nf.fit(df=train_df, id_col='unique_id', time_col='ds', target_col='y')\n",
    "    forecasts = nf.predict(futr_df=test_df)\n",
    "    print(forecasts.head())\n",
    "    mape = mean_absolute_percentage_error(test_df['y'], forecasts[name])\n",
    "    mape_scores.append((name, mape))\n",
    "\n",
    "# Select the best model based on MAPE\n",
    "best_model_name, best_mape = min(mape_scores, key=lambda x: x[1])\n",
    "best_model = None\n",
    "\n",
    "if best_model_name == 'NHITS':\n",
    "    best_model = NHITS(**nhits_params)\n",
    "elif best_model_name == 'NBEATS':\n",
    "    best_model = NBEATS(**nbeats_params)\n",
    "elif best_model_name == 'LSTM':\n",
    "    best_model = LSTM(**lstm_params)\n",
    "\n",
    "# Train the best model on the entire dataset\n",
    "nf_best_model = NeuralForecast(models=[best_model], freq='D')\n",
    "nf_best_model.fit(df=rml_data, id_col='unique_id', time_col='ds', target_col='y')\n",
    "\n",
    "# Predict the next 30 days with the best model\n",
    "future_dates = pd.date_range(rml_data['ds'].max() + pd.Timedelta(days=1), periods=30, freq='D')\n",
    "future_df = pd.DataFrame({'ds': future_dates})\n",
    "future_df['unique_id'] = 'series_1'\n",
    "\n",
    "final_forecasts = nf_best_model.predict(futr_df=future_df)\n",
    "\n",
    "# Plot actual vs predicted values for the best model\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Plot the test data\n",
    "plt.plot(test_df['ds'], test_df['y'], label='Actual', marker='o')\n",
    "\n",
    "# Plot the forecasts\n",
    "plt.plot(\n",
    "    future_dates,\n",
    "    final_forecasts[best_model_name][:30],  # Ensure we take only 30 predictions\n",
    "    label=f'Predicted - {best_model_name}',\n",
    "    marker='x',\n",
    ")\n",
    "\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.title(f'Actual vs Predicted Values using {best_model_name}')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"The best model is {best_model_name} with a MAPE of {best_mape}\")\n"
   ],
   "id": "326b51e89a6c8a22",
   "execution_count": 14,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T21:05:57.959572Z",
     "start_time": "2024-08-01T21:05:57.919593Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data.drop(columns=['date'],inplace=True)\n",
    "data.head()"
   ],
   "id": "de08933e346cb33d",
   "execution_count": 12,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Fine tuning",
   "id": "79c3c52f12fcd546"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T21:26:56.101345Z",
     "start_time": "2024-08-01T21:13:02.139994Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from neuralforecast import NeuralForecast\n",
    "from neuralforecast.models import NHITS, NBEATS, LSTM\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Make a copy of the dataframe\n",
    "rml_data = data.copy()\n",
    "\n",
    "# Ensure 'ds' column is datetime\n",
    "rml_data['ds'] = pd.to_datetime(rml_data['ds'])\n",
    "\n",
    "# Add a unique_id column\n",
    "rml_data['unique_id'] = 'series_1'\n",
    "\n",
    "# Split the data into train and test sets\n",
    "train_df = rml_data[:-30]\n",
    "test_df = rml_data[-30:]\n",
    "\n",
    "# Define the parameter grids for each model\n",
    "nhits_param_grid = {\n",
    "    'h': [30],\n",
    "    'input_size': [30, 60, 90],\n",
    "    'max_steps': [50, 100, 150]\n",
    "}\n",
    "\n",
    "nbeats_param_grid = {\n",
    "    'h': [30],\n",
    "    'input_size': [30, 60, 90],\n",
    "    'max_steps': [50, 100, 150]\n",
    "}\n",
    "\n",
    "lstm_param_grid = {\n",
    "    'h': [30],\n",
    "    'input_size': [30, 60, 90],\n",
    "    'max_steps': [50, 100, 150]\n",
    "}\n",
    "\n",
    "# Initialize the parameter grids\n",
    "nhits_params_list = list(ParameterGrid(nhits_param_grid))\n",
    "nbeats_params_list = list(ParameterGrid(nbeats_param_grid))\n",
    "lstm_params_list = list(ParameterGrid(lstm_param_grid))\n",
    "\n",
    "# Function to train and evaluate a model with given parameters\n",
    "def train_evaluate_model(model_class, param_list, train_df, test_df, model_name):\n",
    "    best_params = None\n",
    "    best_mape = float('inf')\n",
    "    best_forecasts = None\n",
    "\n",
    "    for params in param_list:\n",
    "        model = model_class(**params)\n",
    "        nf = NeuralForecast(models=[model], freq='D')\n",
    "        nf.fit(df=train_df, id_col='unique_id', time_col='ds', target_col='y')\n",
    "        forecasts = nf.predict(futr_df=test_df)\n",
    "        \n",
    "        mape = mean_absolute_percentage_error(test_df['y'], forecasts[model_name])\n",
    "        \n",
    "        if mape < best_mape:\n",
    "            best_mape = mape\n",
    "            best_params = params\n",
    "            best_forecasts = forecasts\n",
    "    \n",
    "    return best_params, best_mape, best_forecasts\n",
    "\n",
    "# Fine-tune and evaluate each model\n",
    "nhits_best_params, nhits_best_mape, nhits_best_forecasts = train_evaluate_model(\n",
    "    NHITS, nhits_params_list, train_df, test_df, 'NHITS'\n",
    ")\n",
    "\n",
    "nbeats_best_params, nbeats_best_mape, nbeats_best_forecasts = train_evaluate_model(\n",
    "    NBEATS, nbeats_params_list, train_df, test_df, 'NBEATS'\n",
    ")\n",
    "\n",
    "lstm_best_params, lstm_best_mape, lstm_best_forecasts = train_evaluate_model(\n",
    "    LSTM, lstm_params_list, train_df, test_df, 'LSTM'\n",
    ")\n",
    "\n",
    "# Store the best models and their MAPE scores\n",
    "best_models = [\n",
    "    ('NHITS', nhits_best_mape, NHITS, nhits_best_params, nhits_best_forecasts),\n",
    "    ('NBEATS', nbeats_best_mape, NBEATS, nbeats_best_params, nbeats_best_forecasts),\n",
    "    ('LSTM', lstm_best_mape, LSTM, lstm_best_params, lstm_best_forecasts)\n",
    "]\n",
    "\n",
    "# Select the best model based on MAPE\n",
    "best_model_name, best_mape, best_model_class, best_model_params, best_forecasts = min(\n",
    "    best_models, key=lambda x: x[1]\n",
    ")\n",
    "\n",
    "# Train the best model on the entire dataset with the best parameters\n",
    "best_model = best_model_class(**best_model_params)\n",
    "nf_best_model = NeuralForecast(models=[best_model], freq='D')\n",
    "nf_best_model.fit(df=rml_data, id_col='unique_id', time_col='ds', target_col='y')\n",
    "\n",
    "# Predict the next 30 days with the best model\n",
    "future_dates = pd.date_range(rml_data['ds'].max() + pd.Timedelta(days=1), periods=30, freq='D')\n",
    "future_df = pd.DataFrame({'ds': future_dates})\n",
    "future_df['unique_id'] = 'series_1'\n",
    "\n",
    "final_forecasts = nf_best_model.predict(futr_df=future_df)\n",
    "\n",
    "# Plot actual vs predicted values for the best model\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Plot the test data\n",
    "plt.plot(test_df['ds'], test_df['y'], label='Actual', marker='o')\n",
    "\n",
    "# Plot the forecasts\n",
    "plt.plot(\n",
    "    future_dates,\n",
    "    final_forecasts[best_model_name][:30],  # Ensure we take only 30 predictions\n",
    "    label=f'Predicted - {best_model_name}',\n",
    "    marker='x',\n",
    ")\n",
    "\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.title(f'Actual vs Predicted Values using {best_model_name}')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"The best model is {best_model_name} with a MAPE of {best_mape}\")\n",
    "print(f\"Best parameters: {best_model_params}\")\n"
   ],
   "id": "aff3f274a3cbb958",
   "execution_count": 15,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Prophet",
   "id": "e72b37b67a425e62"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T21:50:03.080669Z",
     "start_time": "2024-08-01T21:49:54.636289Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from prophet import Prophet\n",
    "from prophet.diagnostics import cross_validation, performance_metrics\n",
    "from prophet.plot import plot_cross_validation_metric\n",
    "import holidays\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "# 1. Prepare Data\n",
    "# data = df.copy()\n",
    "# data[\"ds\"] = df.index\n",
    "# data[\"y\"] = df[\"quantity_winsorized\"]\n",
    "# data['ds'] = pd.to_datetime(data['ds'])\n",
    "\n",
    "# Add is_public_holiday column\n",
    "holiday = holidays.CountryHoliday('UK')\n",
    "data['is_public_holiday'] = data['ds'].apply(\n",
    "    lambda date: 1 if date in holiday else 0\n",
    ")\n",
    "\n",
    "# Define specific hyperparameters (based on previous knowledge or initial testing)\n",
    "params = {\n",
    "    'changepoint_prior_scale': 0.1,\n",
    "    'seasonality_prior_scale': 1.0,\n",
    "    'holidays_prior_scale': 1.0,\n",
    "    'seasonality_mode': 'additive'\n",
    "}\n",
    "\n",
    "# Train the model with the defined hyperparameters\n",
    "model = Prophet(**params)\n",
    "model.add_regressor('is_public_holiday')\n",
    "model.fit(data)\n",
    "\n",
    "# Forecast future values\n",
    "future = model.make_future_dataframe(periods=365)\n",
    "future['is_public_holiday'] = future['ds'].apply(\n",
    "    lambda date: 1 if date in holiday else 0\n",
    ")\n",
    "\n",
    "forecast = model.predict(future)\n",
    "\n",
    "# Visualize Results\n",
    "model.plot(forecast)\n",
    "model.plot_components(forecast)\n",
    "\n",
    "# Evaluate Accuracy\n",
    "df_cv = cross_validation(model, initial='547 days', period='180 days', horizon='60 days')\n",
    "df_p = performance_metrics(df_cv)\n",
    "print(df_p.head().round(2))\n",
    "\n",
    "fig = plot_cross_validation_metric(df_cv, metric='mae')\n",
    "\n",
    "# Calculate MAPE using yhat and y\n",
    "mape = mean_absolute_percentage_error(df_cv['y'], df_cv['yhat'])\n",
    "print(f\"MAPE: {round(mape, 3)}\")\n"
   ],
   "id": "801fbc11a76179e2",
   "execution_count": 18,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T21:49:21.955817Z",
     "start_time": "2024-08-01T21:49:16.728888Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from prophet import Prophet\n",
    "from prophet.diagnostics import cross_validation, performance_metrics\n",
    "from prophet.plot import plot_cross_validation_metric\n",
    "import holidays\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "\n",
    "\n",
    "# Add is_public_holiday column\n",
    "holiday = holidays.CountryHoliday('UK')\n",
    "data['is_public_holiday'] = data['ds'].apply(\n",
    "    lambda date: 1 if date in holiday else 0\n",
    ")\n",
    "\n",
    "# Define specific hyperparameters (based on previous knowledge or initial testing)\n",
    "params = {\n",
    "    'changepoint_prior_scale': 0.1,\n",
    "    'seasonality_prior_scale': 1.0,\n",
    "    'holidays_prior_scale': 1.0,\n",
    "    'seasonality_mode': 'additive'\n",
    "}\n",
    "\n",
    "# Train the model with the defined hyperparameters\n",
    "model = Prophet(**params)\n",
    "model.add_regressor('is_public_holiday')\n",
    "model.fit(data)\n",
    "\n",
    "# Forecast future values\n",
    "future = model.make_future_dataframe(periods=365)\n",
    "future['is_public_holiday'] = future['ds'].apply(\n",
    "    lambda date: 1 if date in holiday else 0\n",
    ")\n",
    "\n",
    "forecast = model.predict(future)\n",
    "\n",
    "# Visualize Results\n",
    "model.plot(forecast)\n",
    "model.plot_components(forecast)\n",
    "\n",
    "# Evaluate Accuracy\n",
    "df_cv = cross_validation(model, initial='547 days', period='180 days', horizon='30 days')\n",
    "df_p = performance_metrics(df_cv)\n",
    "print(df_p.head().round(2))\n",
    "\n",
    "fig = plot_cross_validation_metric(df_cv, metric='mae')\n",
    "\n",
    "# Calculate MAPE using yhat and y\n",
    "mape = mean_absolute_percentage_error(df_cv['y'], df_cv['yhat'])\n",
    "print(f\"MAPE: {round(mape, 3)}\")\n"
   ],
   "id": "cac85ab3b2e34739",
   "execution_count": 17,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T21:45:29.762090Z",
     "start_time": "2024-08-01T21:44:12.514887Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import itertools\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "import pandas as pd\n",
    "from prophet import Prophet\n",
    "from prophet.diagnostics import cross_validation, performance_metrics\n",
    "from prophet.plot import plot_cross_validation_metric\n",
    "import holidays\n",
    "\n",
    "# 1. Prepare Data\n",
    "# data = df.copy()\n",
    "# data[\"ds\"] = df.index\n",
    "# data[\"y\"] = df[\"quantity_winsorized\"]\n",
    "# data['ds'] = pd.to_datetime(data['ds'])\n",
    "\n",
    "# Add is_public_holiday column\n",
    "holiday = holidays.CountryHoliday('UK')\n",
    "data['is_public_holiday'] = data['ds'].apply(\n",
    "    lambda date: 1 if date in holiday else 0\n",
    ")\n",
    "\n",
    "# Define hyperparameters to tune\n",
    "param_grid = {  \n",
    "    'changepoint_prior_scale': [0.01, 0.1, 0.5],\n",
    "    'seasonality_prior_scale': [0.1, 1.0, 10.0],\n",
    "    'holidays_prior_scale': [0.1, 1.0, 10.0],\n",
    "    'seasonality_mode': ['additive', 'multiplicative']\n",
    "}\n",
    "\n",
    "# Generate all combinations of parameters\n",
    "all_params = [dict(zip(param_grid.keys(), v)) for v in itertools.product(*param_grid.values())]\n",
    "\n",
    "best_params = None\n",
    "best_mape = float('inf')\n",
    "\n",
    "# Grid search to find the best hyperparameters\n",
    "for params in all_params:\n",
    "    model = Prophet(**params)\n",
    "    model.add_regressor('is_public_holiday')\n",
    "    model.fit(data)\n",
    "\n",
    "    # Cross-validate the model\n",
    "    df_cv = cross_validation(model, initial='547 days', period='180 days', horizon='30 days')\n",
    "    df_p = performance_metrics(df_cv)\n",
    "    \n",
    "    # Calculate MAPE\n",
    "    mape = mean_absolute_percentage_error(df_cv['y'], df_cv['yhat'])\n",
    "    \n",
    "    if mape < best_mape:\n",
    "        best_mape = mape\n",
    "        best_params = params\n",
    "\n",
    "# Output the best parameters and MAPE\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "print(f\"Best MAPE: {round(best_mape, 2)}\")\n",
    "\n",
    "# Train the final model with the best parameters\n",
    "model = Prophet(**best_params)\n",
    "model.add_regressor('is_public_holiday')\n",
    "model.fit(data)\n",
    "\n",
    "# Forecast future values\n",
    "future = model.make_future_dataframe(periods=365)\n",
    "future['is_public_holiday'] = future['ds'].apply(\n",
    "    lambda date: 1 if date in holiday else 0\n",
    ")\n",
    "\n",
    "forecast = model.predict(future)\n",
    "\n",
    "# Visualize Results\n",
    "model.plot(forecast)\n",
    "model.plot_components(forecast)\n",
    "\n",
    "# Evaluate Accuracy\n",
    "df_cv = cross_validation(model, initial='547 days', period='180 days', horizon='30 days')\n",
    "df_p = performance_metrics(df_cv)\n",
    "print(df_p.head().round(2))\n",
    "\n",
    "fig = plot_cross_validation_metric(df_cv, metric='mae')\n",
    "\n",
    "# Calculate MAPE using yhat and y\n",
    "mape = mean_absolute_percentage_error(df_cv['y'], df_cv['yhat'])\n",
    "print(f\"MAPE: {round(mape, 3)}\")\n"
   ],
   "id": "744014460a054e93",
   "execution_count": 16,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Statistical Models",
   "id": "57239388357d1e95"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T22:00:52.405299Z",
     "start_time": "2024-08-01T22:00:33.144025Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error\n",
    "\n",
    "## Model 1 : SARIMAX\n",
    "# Split data into train and test sets\n",
    "train_size = int(len(df) * 0.8)\n",
    "train, test = df.iloc[:train_size], df.iloc[train_size:]\n",
    "\n",
    "# Define and fit the SARIMAX model\n",
    "model = SARIMAX(train['quantity'], order=(1, 1, 1), seasonal_order=(1, 1, 1, 12))\n",
    "fit_model = model.fit(disp=False)\n",
    "\n",
    "# Forecast\n",
    "n_forecast = len(test)\n",
    "forecast = fit_model.get_forecast(steps=n_forecast)\n",
    "forecast_index = test.index\n",
    "forecast_values = forecast.predicted_mean\n",
    "\n",
    "# Calculate MAE and MAPE\n",
    "mae = mean_absolute_error(test['quantity'], forecast_values)\n",
    "mape = mean_absolute_percentage_error(test['quantity'], forecast_values)\n",
    "\n",
    "# Print the results\n",
    "print(f'MAE: {mae}')\n",
    "print(f'MAPE: {mape}')\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(train.index, train['quantity'], label='Train')\n",
    "plt.plot(test.index, test['quantity'], label='Test', color='green')\n",
    "plt.plot(forecast_index, forecast_values, label='Forecast', color='red')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Quantity')\n",
    "plt.title('SARIMAX Forecast')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "db73e544b87948f6",
   "execution_count": 19,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### ARIMA & SES",
   "id": "66ffb75e64d4fb9e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T22:02:31.029813Z",
     "start_time": "2024-08-01T22:02:12.672659Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.holtwinters import SimpleExpSmoothing\n",
    "\n",
    "\n",
    "# ARIMA Model\n",
    "def fit_arima(train_data, order):\n",
    "    model = ARIMA(train_data, order=order)\n",
    "    fit_model = model.fit()\n",
    "    return fit_model\n",
    "\n",
    "# Exponential Smoothing (Simple Exponential Smoothing)\n",
    "def fit_exponential_smoothing(train_data):\n",
    "    model = SimpleExpSmoothing(train_data)\n",
    "    fit_model = model.fit()\n",
    "    return fit_model\n",
    "\n",
    "# Forecasting and Evaluation Function\n",
    "def forecast_and_evaluate(model, test_data):\n",
    "    forecast_values = model.forecast(len(test_data))\n",
    "    mae = mean_absolute_error(test_data, forecast_values)\n",
    "    mape = mean_absolute_percentage_error(test_data, forecast_values)\n",
    "    return forecast_values, mae, mape\n",
    "\n",
    "# Fit ARIMA model\n",
    "arima_model = fit_arima(train['quantity'], order=(1, 1, 1))\n",
    "\n",
    "# Fit Exponential Smoothing model\n",
    "exp_smoothing_model = fit_exponential_smoothing(train['quantity'])\n",
    "\n",
    "# Forecast and evaluate ARIMA\n",
    "arima_forecast, arima_mae, arima_mape = forecast_and_evaluate(arima_model, test['quantity'])\n",
    "\n",
    "# Forecast and evaluate Exponential Smoothing\n",
    "exp_smoothing_forecast, exp_smoothing_mae, exp_smoothing_mape = forecast_and_evaluate(exp_smoothing_model, test['quantity'])\n",
    "\n",
    "# Print results\n",
    "print(f'ARIMA MAE: {arima_mae}, ARIMA MAPE: {arima_mape}')\n",
    "print(f'Exponential Smoothing MAE: {exp_smoothing_mae}, Exponential Smoothing MAPE: {exp_smoothing_mape}')\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(train.index, train['quantity'], label='Train')\n",
    "plt.plot(test.index, test['quantity'], label='Test', color='green')\n",
    "\n",
    "# Plot ARIMA forecast\n",
    "plt.plot(test.index, arima_forecast, label='ARIMA Forecast', color='blue')\n",
    "\n",
    "# Plot Exponential Smoothing forecast\n",
    "plt.plot(test.index, exp_smoothing_forecast, label='Exponential Smoothing Forecast', color='red')\n",
    "\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Quantity')\n",
    "plt.title('ARIMA vs Exponential Smoothing Forecast Comparison')\n",
    "plt.legend()\n",
    "plt.show();"
   ],
   "id": "eaff096c424fbd23",
   "execution_count": 20,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T22:18:15.634999Z",
     "start_time": "2024-08-01T22:17:45.003389Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsforecast.models import AutoARIMA, AutoETS, AutoCES, AutoTheta, SimpleExponentialSmoothingOptimized\n",
    "from sklearn.metrics import mean_absolute_percentage_error, mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have defined train_df, test_df, and ml_data somewhere in your code\n",
    "\n",
    "# Extract the 'y' series from train and test dataframes\n",
    "train_series = train_df['y'].values\n",
    "test_series = test_df['y'].values\n",
    "\n",
    "# Define the models\n",
    "models = {\n",
    "    'AutoARIMA': AutoARIMA(season_length=12),\n",
    "    'AutoETS': AutoETS(model='ZZZ', season_length=12),\n",
    "    'AutoCES': AutoCES(model='Z', season_length=12),\n",
    "    'AutoTheta': AutoTheta(season_length=12),\n",
    "    'SESOpt': SimpleExponentialSmoothingOptimized()\n",
    "}\n",
    "\n",
    "# Initialize lists to store results\n",
    "mape_scores = []\n",
    "mae_scores = []\n",
    "forecasts_dict = {}\n",
    "\n",
    "# Train and evaluate each model\n",
    "for name, model in models.items():\n",
    "    model = model.fit(y=train_series)\n",
    "    forecasts = model.predict(h=len(test_series))\n",
    "    forecasts_dict[name] = forecasts['mean']\n",
    "    \n",
    "    # Calculate MAPE and MAE\n",
    "    mape = mean_absolute_percentage_error(test_series, forecasts['mean'])\n",
    "    mae = mean_absolute_error(test_series, forecasts['mean'])\n",
    "    \n",
    "    mape_scores.append((name, mape))\n",
    "    mae_scores.append((name, mae))\n",
    "    \n",
    "    print(f\"{name} - MAE: {mae:.4f}, MAPE: {mape:.4%}\")\n",
    "\n",
    "# Select the best model based on MAPE\n",
    "best_model_name, best_mape = min(mape_scores, key=lambda x: x[1])\n",
    "best_model = models[best_model_name]\n",
    "\n",
    "print(f\"\\nThe best model is {best_model_name} with a MAPE of {best_mape:.4%}\")\n",
    "\n",
    "# Train the best model on the entire dataset\n",
    "best_model = best_model.fit(y=data['y'].values)\n",
    "\n",
    "# Predict the next 60 days\n",
    "future_forecasts = best_model.predict(h=60)\n",
    "\n",
    "# Combine the historical data with the forecast data for plotting\n",
    "combined_df = data.copy()\n",
    "future_df = pd.DataFrame({\n",
    "    'ds': pd.date_range(start=data['ds'].max() + pd.Timedelta(days=1), periods=60, freq='D'),\n",
    "    'y': np.nan\n",
    "})\n",
    "combined_df = pd.concat([combined_df, future_df], ignore_index=True)\n",
    "combined_df['forecast'] = np.nan\n",
    "combined_df.loc[data.shape[0]:, 'forecast'] = future_forecasts['mean']\n",
    "\n",
    "# Plot actual vs predicted values for the historical period and future forecast\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Plot the entire dataset\n",
    "plt.plot(combined_df['ds'], combined_df['y'], label='Actual', marker='o')\n",
    "\n",
    "# Plot the future forecasts\n",
    "plt.plot(\n",
    "    combined_df['ds'],\n",
    "    combined_df['forecast'],\n",
    "    label=f'Predicted - {best_model_name} (Next 60 Days)',\n",
    "    marker='x',\n",
    ")\n",
    "\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.title(f'Actual vs Predicted Values using {best_model_name}')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nThe best model is {best_model_name} with a MAPE of {best_mape:.4%}\")\n"
   ],
   "id": "dd48717fdb9ee50b",
   "execution_count": 22,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T22:22:07.754208Z",
     "start_time": "2024-08-01T22:21:34.250771Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsforecast.models import AutoARIMA, AutoETS, AutoCES, AutoTheta, SimpleExponentialSmoothingOptimized\n",
    "from sklearn.metrics import mean_absolute_percentage_error, mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have defined train_df, test_df, and ml_data somewhere in your code\n",
    "\n",
    "# Extract the 'y' series from train and test dataframes\n",
    "train_series = train_df['y'].values\n",
    "test_series = test_df['y'].values\n",
    "\n",
    "# Define the models\n",
    "models = {\n",
    "    'AutoARIMA': AutoARIMA(season_length=12),\n",
    "    'AutoETS': AutoETS(model='ZZZ', season_length=12),\n",
    "    'AutoCES': AutoCES(model='Z', season_length=12),\n",
    "    'AutoTheta': AutoTheta(season_length=12),\n",
    "    'SESOpt': SimpleExponentialSmoothingOptimized()\n",
    "}\n",
    "\n",
    "# Initialize lists to store results\n",
    "mape_scores = []\n",
    "mae_scores = []\n",
    "forecasts_dict = {}\n",
    "\n",
    "# Train and evaluate each model\n",
    "for name, model in models.items():\n",
    "    model = model.fit(y=train_series)\n",
    "    forecasts = model.predict(h=len(test_series))\n",
    "    forecasts_dict[name] = forecasts['mean']\n",
    "    \n",
    "    # Calculate MAPE and MAE\n",
    "    mape = mean_absolute_percentage_error(test_series, forecasts['mean'])\n",
    "    mae = mean_absolute_error(test_series, forecasts['mean'])\n",
    "    \n",
    "    mape_scores.append((name, mape))\n",
    "    mae_scores.append((name, mae))\n",
    "    \n",
    "    print(f\"{name} - MAE: {mae:.4f}, MAPE: {mape:.4%}\")\n",
    "\n",
    "# Select the best model based on MAPE\n",
    "best_model_name, best_mape = min(mape_scores, key=lambda x: x[1])\n",
    "best_model = models[best_model_name]\n",
    "\n",
    "print(f\"\\nThe best model is {best_model_name} with a MAPE of {best_mape:.4%}\")\n",
    "\n",
    "# Train the best model on the entire dataset\n",
    "best_model = best_model.fit(y=data['y'].values)\n",
    "\n",
    "# Predict the next 30 days instead of 60\n",
    "future_forecasts = best_model.predict(h=30)\n",
    "\n",
    "# Combine the historical data with the forecast data for plotting\n",
    "combined_df = data.copy()\n",
    "future_df = pd.DataFrame({\n",
    "    'ds': pd.date_range(start=data['ds'].max() + pd.Timedelta(days=1), periods=30, freq='D'),\n",
    "    'y': np.nan\n",
    "})\n",
    "combined_df = pd.concat([combined_df, future_df], ignore_index=True)\n",
    "combined_df['forecast'] = np.nan\n",
    "combined_df.loc[data.shape[0]:, 'forecast'] = future_forecasts['mean']\n",
    "\n",
    "# Plot actual vs predicted values for the historical period and future forecast\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Plot the entire dataset\n",
    "plt.plot(combined_df['ds'], combined_df['y'], label='Actual', marker='o')\n",
    "\n",
    "# Plot the future forecasts\n",
    "plt.plot(\n",
    "    combined_df['ds'],\n",
    "    combined_df['forecast'],\n",
    "    label=f'Predicted - {best_model_name} (Next 30 Days)',\n",
    "    marker='x',\n",
    ")\n",
    "\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.title(f'Actual vs Predicted Values using {best_model_name}')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nThe best model is {best_model_name} with a MAPE of {best_mape:.4%}\")\n"
   ],
   "id": "6965d0c08193a1",
   "execution_count": 24,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "",
   "id": "314242369da68f88",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
